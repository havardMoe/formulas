{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac5c068",
   "metadata": {
    "id": "kr1KHoZizYvc"
   },
   "source": [
    "# Term vector similarity\n",
    "\n",
    "In this exercise you'll need to complete the code for computing the similarity between two documents that are represented by their term vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78e5d2",
   "metadata": {
    "id": "snq6kz6dzvgI"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6876f",
   "metadata": {
    "id": "rYvn9QR2zYvo"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import math\n",
    "import pytest\n",
    "from typing import List\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b6923",
   "metadata": {
    "id": "WYH9kjdkzYvq"
   },
   "source": [
    "## Jaccard similarity\n",
    "\n",
    "This metric is a set similarity; that is, it only captures the presence and absence of terms with no regard to their frequency. Put simply, it captures the ratio of shared terms and total terms in the two documents.\n",
    "\n",
    "$$sim_{Jaccard} = \\frac{|X \\cap Y|}{|X \\cup Y|}$$\n",
    "\n",
    "where $X$ and $Y$ denote the set of terms in documents $x$ and $y$, respectively.\n",
    "\n",
    "If the two documents are given as term vectors, Jaccard similarity may be computed as:\n",
    "\n",
    "$$sim_{\\mathrm{Jaccard}}(\\mathbf{x},\\mathbf{y}) = \\frac{\\sum_{i} \\mathbb{1}(x_i) \\times \\mathbb{1}(y_i)}{\\sum_{i} \\mathbb{1}(x_i+y_i)}$$\n",
    "\n",
    "where $\\mathbb{1}(x)$ is an indicator function ($1$ if $x>0$ and $0$ otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2d480",
   "metadata": {
    "id": "XBdM52LNzYvr"
   },
   "outputs": [],
   "source": [
    "def jaccard(x: List[int], y: List[int]) -> float:\n",
    "    \"\"\"Computes the Jaccard similarity between two term vectors.\"\"\"\n",
    "    intersection = sum([x_i > 0 and y_i > 0 for x_i, y_i in zip(x, y)])\n",
    "    union = sum([x_i > 0 or y_i > 0 for x_i, y_i in zip(x, y)])\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ebe0e",
   "metadata": {
    "id": "m5yKbHJczYvs"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2569942",
   "metadata": {
    "id": "5Ir5nWjVzYvs"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_no_common_terms():\n",
    "    x = [0, 0, 0, 1, 2, 1]\n",
    "    y = [1, 5, 3, 0, 0, 0]\n",
    "    assert jaccard(x, y) == 0\n",
    "\n",
    "def test_only_common_terms():\n",
    "    x = [0, 1, 2, 1, 0, 1]\n",
    "    y = [0, 5, 3, 7, 0, 1]\n",
    "    assert jaccard(x, y) == 1\n",
    "\n",
    "def test_some_common_terms():\n",
    "    x = [0, 1, 1, 0, 1, 1]\n",
    "    y = [5, 0, 3, 0, 7, 0]\n",
    "    assert jaccard(x, y) == 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84e5f8",
   "metadata": {
    "id": "snfrRppezYvt"
   },
   "source": [
    "## Cosine similarity\n",
    "\n",
    "$$sim_{cos}(x,y) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{||\\mathbf{x}||~||\\mathbf{y}||} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sqrt{\\sum_{i=1}^n x_i^2} \\sqrt{\\sum_{i=1}^n y_i^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5116b84",
   "metadata": {
    "id": "JBrPFYr-zYvu"
   },
   "outputs": [],
   "source": [
    "def cosine(x: List[float], y: List[float]) -> float:\n",
    "    \"\"\"Computes the Cosine similarity between two term vectors.\"\"\"\n",
    "    dot_product = sum([x_i * y_i for x_i, y_i in zip(x, y)])\n",
    "    norm_x = math.sqrt(sum(x_i**2 for x_i in x))\n",
    "    norm_y = math.sqrt(sum(y_i**2 for y_i in y))\n",
    "    return dot_product / (norm_x * norm_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521fa94",
   "metadata": {
    "id": "CV-SbZvPzYvu"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16abcbf",
   "metadata": {
    "id": "6svWK8pQzYvv"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_no_common_terms():\n",
    "    x = [0, 0, 0, 1, 2, 1]\n",
    "    y = [1, 5, 3, 0, 0, 0]\n",
    "    assert cosine(x, y) == 0\n",
    "\n",
    "def test_identical_docs():\n",
    "    x = [0, 0, 0, 1, 2, 1]\n",
    "    assert cosine(x, x) == pytest.approx(1, rel=1e-3)\n",
    "\n",
    "def test_short_docs():\n",
    "    x = [4, 2]\n",
    "    y = [1, 3]\n",
    "    assert cosine(x, y) == pytest.approx(math.sqrt(2) / 2, rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a22ee",
   "metadata": {
    "id": "2jDta06Q5zkD"
   },
   "source": [
    "# Text preprocessing\n",
    "\n",
    "In this exercise, you'll need to implement basic text preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fe3ea",
   "metadata": {
    "id": "VQXIuPwd55GC"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f505b5c",
   "metadata": {
    "id": "NPzbGg6A5zkE"
   },
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "import ipytest\n",
    "import string\n",
    "import re\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8a8e4",
   "metadata": {
    "id": "1atNr1xm5zkF"
   },
   "source": [
    "## Task 1: Tokenization\n",
    "\n",
    "Split an input text into tokens based on whitespaces, punctuation, hyphens, and HTML markup. Additionally, lowercase all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb7f17",
   "metadata": {
    "id": "pEVqJ0a-5zkG"
   },
   "outputs": [],
   "source": [
    "def tokenize(text: str)-> List[str]:    \n",
    "    \"\"\"Returns a sequence of terms given an input text.\"\"\"\n",
    "    # Remove HTML markup using a regular expression.\n",
    "    re_html = re.compile(\"<[^>]+>\")\n",
    "    text = re_html.sub(\" \", text)\n",
    "    # Replace punctuation marks (including hyphens) with spaces.\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c, \" \")\n",
    "    # Lowercase and split on whitespaces.\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a963730",
   "metadata": {
    "id": "5GQK55Rr5zkG"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c702d6",
   "metadata": {
    "id": "W3C8B-PJ5zkH"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_whitespace():\n",
    "    assert tokenize(\"aaa bbb ccc\") == [\"aaa\", \"bbb\", \"ccc\"]\n",
    "    \n",
    "def test_punctuation():\n",
    "    assert tokenize(\"aaa! bbb.ccc,ddd:eee ff\\\"f\") == [\"aaa\", \"bbb\", \"ccc\", \"ddd\", \"eee\", \"ff\", \"f\"]\n",
    "    \n",
    "def test_hyphens():\n",
    "    assert tokenize(\"aaa bbb-Ccc\") == [\"aaa\", \"bbb\", \"ccc\"]\n",
    "    \n",
    "def test_html():\n",
    "    assert tokenize(\"aaa <bbb>ccc <ddd>eee</ddd></bbb>fff <ggg />\") == [\"aaa\", \"ccc\", \"eee\", \"fff\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7608f9c",
   "metadata": {
    "id": "-DeJ8IXl5zkH"
   },
   "source": [
    "## Task 2: Stopwords removal\n",
    "\n",
    "Remove stopwords from a sequence of tokens, given a set of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d5c28",
   "metadata": {
    "id": "hBXz41375zkI"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens: List[str], stopwords: Set[str]) -> List[str]:\n",
    "    \"\"\"Removes stopwords from a sequence of tokens.\"\"\"\n",
    "    return [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1b914",
   "metadata": {
    "id": "PeqR2gNY5zkI"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8110f4",
   "metadata": {
    "id": "k7a-Ideu5zkJ"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_no_stopwords():\n",
    "    assert remove_stopwords([\"this\", \"is\", \"some\", \"text\"], {}) == [\"this\", \"is\", \"some\", \"text\"]\n",
    "    \n",
    "def test_stopwords():\n",
    "    assert remove_stopwords([\"this\", \"is\", \"some\", \"text\"], {\"is\", \"this\"}) == [\"some\", \"text\"]\n",
    "    \n",
    "def test_stopwords2():\n",
    "    assert remove_stopwords([\"this\", \"isolate\", \"otto\"], {\"is\", \"this\", \"to\"}) == [\"isolate\", \"otto\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360ffe2",
   "metadata": {
    "id": "GoQTh3_25zkK"
   },
   "source": [
    "## Task 3: Suffix-s stemming\n",
    "\n",
    "Remove the s-suffix from all terms in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b23e5",
   "metadata": {
    "id": "wy9zy8y55zkK"
   },
   "outputs": [],
   "source": [
    "def suffix_s_stemmer(terms: List[str]) -> List[str]:\n",
    "    \"\"\"Removes the s-suffix from all terms in a sequence.\"\"\"\n",
    "    stemmed_terms = []\n",
    "    for term in terms:        \n",
    "        stemmed_term = term[:-1] if term[-1] == \"s\" else term\n",
    "        stemmed_terms.append(stemmed_term)\n",
    "    return stemmed_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b89deb",
   "metadata": {
    "id": "coaaPaHT5zkK"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35942b2",
   "metadata": {
    "id": "Y0A-_Iwc5zkK"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_stemming():\n",
    "    assert suffix_s_stemmer([\"dogs\", \"better\", \"cats\"]) == [\"dog\", \"better\", \"cat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef40db",
   "metadata": {
    "id": "ceYNFACNzYzX"
   },
   "source": [
    "# Naive Bayes text classifier\n",
    "\n",
    "In this exercise, you'll implement a Naive Bayes classifier for text from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb8bd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DlB0nYezxyG",
    "outputId": "077eb1b7-9cb4-40d7-aba5-d38ae1d69520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ipytest\n",
      "  Downloading ipytest-0.13.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
      "Collecting pytest>=5.4\n",
      "  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
      "\u001b[K     |████████████████████████████████| 316 kB 8.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.13.0)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Downloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 48.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n",
      "Installing collected packages: pluggy, jedi, iniconfig, exceptiongroup, pytest, ipytest\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 0.7.1\n",
      "    Uninstalling pluggy-0.7.1:\n",
      "      Successfully uninstalled pluggy-0.7.1\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 3.6.4\n",
      "    Uninstalling pytest-3.6.4:\n",
      "      Successfully uninstalled pytest-3.6.4\n",
      "Successfully installed exceptiongroup-1.0.4 iniconfig-1.1.1 ipytest-0.13.0 jedi-0.18.1 pluggy-1.0.0 pytest-7.2.0\n"
     ]
    }
   ],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25787742",
   "metadata": {
    "id": "EsGmC71KzYzg"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "from typing import List\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53d932",
   "metadata": {
    "id": "Fbn-FoRpzYzl"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "  - Calculate $P(y)$ for each class label in the training data\n",
    "  - Calculate $P(x_i|y)$ for each feature (term) for each class label in the training data using Laplace (add-one) smoothing\n",
    "  \n",
    "$$P(x_i|y)=\\frac{c_{i,y} + 1}{c_i + m}$$\n",
    "\n",
    "where \n",
    "  - $c_{i,y}$ is the number of times term $x_i$ appears in class $y$\n",
    "  - $c_i$ is the total number of times term $x_i$ appears in the collection\n",
    "  - $m$ is the number of classes\n",
    "\n",
    "\n",
    "### Applying the model\n",
    "\n",
    "Return the class $y \\in Y$ that maximizes $P(y) \\prod_{x_i} P(x_i|y)$.\n",
    "\n",
    "Note that we need to consider $x_i$ at each *word position* in the document. Thus, we need to multiply with $P(x_i|y)$ as many times as $x_i$ appears in the document.\n",
    "We can rewrite it as: $$P(y|x) \\propto P(y) \\prod_{i \\in d} P(x_i|y)^{c_{i,d}}$$ where $c_{i,d}$ is the number of times term $i$ appears in document $d$.\n",
    "\n",
    "Finally, we perform the computations in the log domain, that is, $$\\log P(y) +  \\sum_{i=1}^n (c_{i,d} \\log P(x_i|y))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed17c4f",
   "metadata": {
    "id": "gESAyM8czYzl"
   },
   "source": [
    "## 1) Probability estimations\n",
    "\n",
    "The estimation of probabilities $P(x_i|y)$ and $P(y)$ are refactored to a separate class to make them testable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89e120",
   "metadata": {
    "id": "DOuxB-qYzYzm"
   },
   "outputs": [],
   "source": [
    "class NBProbabilityEstimator:\n",
    "    \n",
    "    def get_prior_prob(self, y: int, training_labels: List[int]) -> float:\n",
    "        \"\"\"Computes the class prior probability, P(y).\n",
    "        \n",
    "        Args:\n",
    "            y: Class ID.\n",
    "            training_labels: Class labels in training data.\n",
    "            \n",
    "        Returns:\n",
    "            The probability P(y).\n",
    "        \"\"\"\n",
    "        return training_labels.count(y) / len(training_labels)\n",
    "    \n",
    "    def get_term_prob(self, count_inclass: int, count_total: int, num_classes: int) -> float:\n",
    "        \"\"\"Computes the smoothed term probability for a given class, P(x_i|y).\n",
    "        \n",
    "        Args:\n",
    "          count_inclass: Number of times the term appears in the given class.\n",
    "          count_total: Number of times the term appears in the collection.\n",
    "          num_classes: Number of classes.\n",
    "          \n",
    "        Returns:\n",
    "          The probability P(x_i|y).\n",
    "        \"\"\"\n",
    "        return (count_inclass + 1) / (count_total + num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46311611",
   "metadata": {
    "id": "gMOzJYpszYzp"
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca472e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUQLB1wezYzq",
    "outputId": "428317da-f44b-4c64-8c22-1ea4e899b169"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n",
      "ipytest.clean_tests is deprecated in favor of ipytest.clean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_prior_prob():\n",
    "    nbpe = NBProbabilityEstimator()\n",
    "    assert nbpe.get_prior_prob(1, [0, 1, 2, 3]) == 0.25\n",
    "    assert nbpe.get_prior_prob(1, [1, 1, 2, 3]) == 0.5\n",
    "\n",
    "def test_term_prob():\n",
    "    nbpe = NBProbabilityEstimator()\n",
    "    assert nbpe.get_term_prob(5, 20, 10) == 0.2\n",
    "    assert nbpe.get_term_prob(74, 90, 10) == 0.75\n",
    "    assert nbpe.get_term_prob(0, 6, 10) == 0.0625"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c386eb9",
   "metadata": {
    "id": "Y1swdMcezYz2"
   },
   "source": [
    "## 2) Naive Bayes classifier\n",
    "\n",
    "Implement training and prediction for a Naive Bayes classifier.  We are operating with dense matrices for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10e0b1",
   "metadata": {
    "id": "z5hK0VMFzYz3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class NBClassifier:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._nbprob = NBProbabilityEstimator()\n",
    "        self._num_classes = 0\n",
    "        self._prior_prob = None  # Holds P(y) values\n",
    "        self._term_prob = None  # Holds P(x_i|y) values\n",
    "        \n",
    "    \n",
    "    def fit(self, X_train: List[List[int]], y_train: List[int]) -> None:\n",
    "        \"\"\"Fits the model.\n",
    "        \n",
    "        Args:\n",
    "          X_train: Document-term matrix for training data. \n",
    "              Rows correspond to documents and columns correspond to terms.\n",
    "          y_train: Class labels corresponding to training documents.\n",
    "        \"\"\"        \n",
    "        self._num_classes = len(np.unique(y_train))\n",
    "        num_docs = len(X_train)\n",
    "        num_terms = len(X_train[0])        \n",
    "        self._term_prob = np.zeros((num_terms, self._num_classes))\n",
    "        \n",
    "        # Iterating through the vocabulary\n",
    "        for i in range(num_terms):\n",
    "            # Holds c_{i,j} values, i.e., the number of times term i appears with class j.\n",
    "            class_count = [0] * self._num_classes\n",
    "            for d in range(num_docs):\n",
    "                class_count[y_train[d]] += X_train[d][i]\n",
    "                        \n",
    "            # Calculate P(x_i|y)\n",
    "            total_count = sum(class_count)\n",
    "            for j in range(self._num_classes):\n",
    "                self._term_prob[i, j] = self._nbprob.get_term_prob(class_count[j], total_count, num_terms)\n",
    "                \n",
    "        # Pre-compute class prior probabilities\n",
    "        self._prior_prob = []\n",
    "        for y in range(self._num_classes):\n",
    "            self._prior_prob.append(self._nbprob.get_prior_prob(y, y_train))\n",
    "\n",
    "                \n",
    "    def _predict_instance(self, x: List[int]) -> int:\n",
    "        \"\"\"Predict class for a single instance (document).\n",
    "        \n",
    "        Args:\n",
    "          x: Document term vector.\n",
    "          \n",
    "        Returns:\n",
    "          The predicted class label (0-indexed).\n",
    "        \"\"\"\n",
    "        probs = []\n",
    "        \n",
    "        for y in range(self._num_classes):\n",
    "            p = math.log(self._prior_prob[y])\n",
    "            for i in range(len(x)):\n",
    "                if x[i] > 0:\n",
    "                    p += x[i] * math.log(self._term_prob[i][y])\n",
    "            probs.append(p)\n",
    "            \n",
    "        # Get the class with the highest probability.\n",
    "        return probs.index(max(probs))\n",
    "        \n",
    "    \n",
    "    def predict(self, X_test: List[List[int]]) -> List[float]:\n",
    "        \"\"\"Make predictions for a set of documents.\n",
    "        \n",
    "        Args:\n",
    "          X_test: Document-term matrix for test data.\n",
    "          \n",
    "        Returns:\n",
    "          List with predictions.\n",
    "        \"\"\"\n",
    "        return [self._predict_instance(x) for x in X_test]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508e673",
   "metadata": {
    "id": "nA1xiVXizYz6"
   },
   "source": [
    "## 3) Testing on real data\n",
    "\n",
    "We will be using a subset of the 20Newsgroups collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358f157",
   "metadata": {
    "id": "sN3Rb5XdzYz8"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"soc.religion.christian\", \n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"comp.sys.mac.hardware\"\n",
    "]\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=123)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3e3d2",
   "metadata": {
    "id": "g5_16ZTWzYz8"
   },
   "source": [
    "### Feature extraction\n",
    "\n",
    "Get term frequencies using `CountVectorizer`. (We ignore terms that appear in less than 10 documents to speed up computation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318a525",
   "metadata": {
    "id": "ZDv9IBm4zYz8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(min_df=10)\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "X_test_counts = count_vect.transform(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60135d5e",
   "metadata": {
    "id": "Mal71SvQzYz9"
   },
   "source": [
    "### Train and apply model\n",
    "\n",
    "Note that we convert sparse matrices to dense ones. This is not efficient and should be avoided when working with large datasets. Nevertheless, this simplifies the implementation for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7284d",
   "metadata": {
    "id": "f8OW0YIGzYz-"
   },
   "outputs": [],
   "source": [
    "nb = NBClassifier()\n",
    "nb.fit(X_train_counts.toarray(), train.target.tolist())\n",
    "predicted = nb.predict(X_test_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e4d86",
   "metadata": {
    "id": "bY6t-QhIzY0A"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d605d77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxahB86tzY0A",
    "outputId": "3b6eb0a3-049e-47bc-d119-ee80cf573a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.595\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "print(f\"{metrics.accuracy_score(test.target, np.asarray(predicted)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5282292",
   "metadata": {
    "id": "xBqHqYb_zY0A"
   },
   "source": [
    "**TODO** Once you completed the exercise E2-3, check back here to see if the performance you got with the implementation from scratch is comparable to that of sklearn. Most likely, you'll see quite a bit difference. Can you find out the reason for that? You can share the solution at the next class session (for a bonus point)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d978e94",
   "metadata": {
    "id": "-sMrq5qFzY0A"
   },
   "source": [
    "## Optional exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a1915",
   "metadata": {
    "id": "O9n9Ai-JzY0B"
   },
   "source": [
    "If you're done, try to implement it without making a conversion to dense matrices.\n",
    "\n",
    "Also, do we really need to precompute and store all term probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6302c15",
   "metadata": {
    "id": "tDFkxwf9zaOE"
   },
   "source": [
    "# Cross-validation\n",
    "\n",
    "In this exercise, we want to split a dataset into train-test splits for k-fold cross-validation. Part of the excercise involves designing appropriate tests for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1908b13",
   "metadata": {
    "id": "SZ_VvhnIzwna"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a9d4e",
   "metadata": {
    "id": "hSQ-qfU1zaOK"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178959b9",
   "metadata": {
    "id": "lGmV3Y3MzaON"
   },
   "source": [
    "## Task\n",
    "\n",
    "The dataset is given as a list of instances (by their IDs). Your task is divide it into k folds to perform cross-validation.\n",
    "\n",
    "Each fold should enumerate the instances for the train and test splits.\n",
    "\n",
    "For examples, given `instances = [1, 2, 3]` and `k=3`, the method should return\n",
    "\n",
    "```\n",
    "folds = [\n",
    "    {'train': [1, 2], 'test': [3]},\n",
    "    {'train': [1, 3], 'test': [2]},\n",
    "    {'train': [2, 3], 'test': [1]},\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f82fc",
   "metadata": {
    "id": "ma0vuTHGzaOO"
   },
   "outputs": [],
   "source": [
    "def create_folds(instances: List[int], k: int = 5) -> List[Dict[str, List[int]]]:\n",
    "    \"\"\"Given a set of instances, it returns k splits of train and test.\"\"\"\n",
    "    # Shuffle instances (by first making a copy of them).\n",
    "    instances_shuffled = list(instances)\n",
    "    # random.seed(10)  # Uncomment to check if the last test fails.\n",
    "    random.shuffle(instances_shuffled)\n",
    "\n",
    "    folds = []\n",
    "    for fold_id in range(k):\n",
    "        train, test = [], []\n",
    "        for i in range(len(instances_shuffled)):\n",
    "            if i % k == fold_id:\n",
    "                test.append(instances_shuffled[i])\n",
    "            else:\n",
    "                train.append(instances_shuffled[i])\n",
    "        \n",
    "        folds.append({\n",
    "            'train': train, \n",
    "            'test': test\n",
    "        })\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b214d2",
   "metadata": {
    "id": "fe11lejPzaOO"
   },
   "source": [
    "### Tests\n",
    "\n",
    "One simple test is provided, which merely checks if the required number of folds is generated and that each contains the correct number of train and test instances.\n",
    "\n",
    "Part of the exercise is to create some more advanced tests. \n",
    "\n",
    "  - One test should test converage, that is, check that all instances are part of exactly one test fold and k-1 train folds.\n",
    "  - Another test should checks that the folds are sufficiently random, i.e., that you're not always returning the exact same partitioning of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131504bd",
   "metadata": {
    "id": "gnl68KZKzaOP"
   },
   "source": [
    "**Note** You can test whether the last test works as intended, by fixing the random seed, i.e., uncommenting that line in `create_folds`. Then, the test should not pass (as it is not a truly random assignment anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcb7af",
   "metadata": {
    "id": "aLLK5ytzzaOP"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def test_fold_size():\n",
    "    instances = list(range(100))\n",
    "    folds = create_folds(instances, k=5)\n",
    "    assert len(folds) == 5\n",
    "    for fold in folds:\n",
    "        assert len(fold['train']) == 80\n",
    "        assert len(fold['test']) == 20\n",
    "\n",
    "def test_coverage():\n",
    "    instances = list(range(100))\n",
    "    k = 5\n",
    "    folds = create_folds(instances, k=k)\n",
    "    train_counter, test_counter = Counter(), Counter()\n",
    "    for fold in folds:\n",
    "        train_counter.update(fold['train'])\n",
    "        test_counter.update(fold['test'])    \n",
    "\n",
    "    assert set(train_counter) == set(instances)\n",
    "    assert set(train_counter.values()) == set([k-1])\n",
    "    assert set(test_counter) == set(instances)\n",
    "    assert set(test_counter.values()) == set([1])\n",
    "\n",
    "def test_randomization():\n",
    "    instances = list(range(100))\n",
    "    k = 5\n",
    "    # Create a sufficiently large sample (i.e., repeat 1000x)\n",
    "    # and keep track of how many times we get the exact same test fold.\n",
    "    num_test_fold = defaultdict(int)\n",
    "    for _ in range(1000):\n",
    "        folds = create_folds(instances, k=k)\n",
    "        for fold_id, fold in enumerate(folds):\n",
    "            # We create a \"signature\" for each test fold by concatenating\n",
    "            # the ordered instance IDs.\n",
    "            signature = \"-\".join([str(_) for _ in sorted(fold[\"test\"])])\n",
    "            num_test_fold[signature] += 1\n",
    "\n",
    "    # If a given combination of instance IDs is observed more than 3 times,\n",
    "    # then the partitioning is probably not sufficiently random.\n",
    "    # Note, that 3 is a somewhat arbitrarily chosen theshold. It should be\n",
    "    # done more precisely (any likely using some statistical test).\n",
    "    # The smaller the number of instance, the more often this test fails \n",
    "    # incorrectly (e.g., with 28 instances, it'll fail once in a while; \n",
    "    # with less than 25 instances, it'll almost always fail.)\n",
    "    for _, count in num_test_fold.items():\n",
    "        assert count < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b358d1",
   "metadata": {
    "id": "WsEvNqgAzZVU"
   },
   "source": [
    "# Experiments with text classifiers in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03369539",
   "metadata": {
    "id": "5wkrFjwWzZVX"
   },
   "source": [
    "In this exercise we'll be experimenting with various classification algorithms in scikit learn using the [20 Newsgroups collection](http://people.csail.mit.edu/jrennie/20Newsgroups/).\n",
    "\n",
    "The first part of the notebook shows a detailed example usage of text classification using sklearn (based on [scikit learn's \"Working with text data\" tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)).\n",
    "The real exercise is at the bottom, where you'll be asked to perform various experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cad161",
   "metadata": {
    "id": "s719rR66zZVX"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a3486",
   "metadata": {
    "id": "wTCwWnNmzZVY"
   },
   "source": [
    "In order to get faster execution times, we will work on a partial dataset with only 5 categories out of the 20 available in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db352e",
   "metadata": {
    "id": "ghak5ZNgzzO2"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db8ed0",
   "metadata": {
    "id": "X6GeX5yjzZVY"
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"soc.religion.christian\", \n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"comp.sys.mac.hardware\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be929a76",
   "metadata": {
    "id": "bMpzLSitzZVZ"
   },
   "source": [
    "We load the documents from those categories, divided into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58deaf",
   "metadata": {
    "id": "ScYYuK6IzZVa"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=123)\n",
    "test = fetch_20newsgroups(subset=\"test\", categories=categories, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87426ac",
   "metadata": {
    "id": "btKGbZKjzZVa"
   },
   "source": [
    "Check which categories got loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8c0e3",
   "metadata": {
    "id": "8w87HviRzZVb"
   },
   "outputs": [],
   "source": [
    "print(train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c5fef",
   "metadata": {
    "id": "oWQ_L3iJzZVc"
   },
   "source": [
    "Check the size of training and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec636d7",
   "metadata": {
    "id": "qjZiT0h9zZVc"
   },
   "outputs": [],
   "source": [
    "print(\"Training instances: {}\".format(len(train.data)))\n",
    "print(\"Test instances:     {}\".format(len(test.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9445511",
   "metadata": {
    "id": "s4hxcU99zZVc"
   },
   "source": [
    "Check target labels of some of the train and test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5098334",
   "metadata": {
    "id": "qJiHcb4izZVc"
   },
   "outputs": [],
   "source": [
    "print(train.target[:10])\n",
    "print(test.target[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5313b07",
   "metadata": {
    "id": "OdEI8_vtzZVd"
   },
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c71806",
   "metadata": {
    "id": "mcml1_YVzZVd"
   },
   "source": [
    "Bag-of-words document representation, using raw term counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2e4f5",
   "metadata": {
    "id": "K7boAI3LzZVd"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a26c79",
   "metadata": {
    "id": "xREGZRI2zZVd"
   },
   "source": [
    "Check dimensionality (instances x features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5cf769",
   "metadata": {
    "id": "uoZHOQmfzZVd"
   },
   "outputs": [],
   "source": [
    "print(X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0150cd",
   "metadata": {
    "id": "DWJWOTC3zZVe"
   },
   "source": [
    "Check vocabulary (sample 10 terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb41cd",
   "metadata": {
    "id": "DB9DFnYVzZVe"
   },
   "outputs": [],
   "source": [
    "for idx, term in enumerate(count_vect.vocabulary_.keys()):\n",
    "    if idx < 10:\n",
    "        print(f\"{term} (ID: {count_vect.vocabulary_[term]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b793769",
   "metadata": {
    "id": "oX8q_ieIzZVe"
   },
   "source": [
    "Learn a Naive Bayes model on the training data (by default it uses Laplace smoothing with alpha=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87a720",
   "metadata": {
    "id": "ibU3WR94zZVe"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB(alpha=1.0)\n",
    "classifier.fit(X_train_counts, train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d859d",
   "metadata": {
    "id": "tkcxksxZzZVe"
   },
   "source": [
    "## Apply the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d730a",
   "metadata": {
    "id": "qWnv7pMIzZVf"
   },
   "source": [
    "First, extract the same feature representation by re-using the `CountVectorizer` from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2142bf",
   "metadata": {
    "id": "RKOUTIHszZVf"
   },
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea4fce",
   "metadata": {
    "id": "YaRzekVHzZVf"
   },
   "source": [
    "Check dimensionality (documents x features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a6b5c",
   "metadata": {
    "id": "yrZ1ggT8zZVf"
   },
   "outputs": [],
   "source": [
    "print(X_test_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458ce54",
   "metadata": {
    "id": "EidAzsHgzZVf"
   },
   "source": [
    "Then, predict labels for test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f134f",
   "metadata": {
    "id": "rGH8oqLBzZVf"
   },
   "outputs": [],
   "source": [
    "predicted = classifier.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ee0e6",
   "metadata": {
    "id": "B78fcNfgzZVf"
   },
   "source": [
    "Look at some of the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0553b",
   "metadata": {
    "id": "mSnJFC89zZVf"
   },
   "outputs": [],
   "source": [
    "print(predicted[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491236e",
   "metadata": {
    "id": "QkbnrOltzZVf"
   },
   "source": [
    "## Evaluate model performance\n",
    "\n",
    "We use Accuracy as our measure here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9471809",
   "metadata": {
    "id": "pA4Bl2WHzZVf"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(f\"{metrics.accuracy_score(test.target, predicted):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f8d1a",
   "metadata": {
    "id": "0ZgiZ86OzZVg"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "1) Use TF weighting instead of the raw counts. (See the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) for `TfidfTransformer` usage.)\n",
    "\n",
    "2) Try at least one different classifier, e.g., [linear SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) (or [other SVMs](https://scikit-learn.org/stable/modules/svm.html#svm-classification)).\n",
    "\n",
    "3) Record the results you got in the table below. How far can you push accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c3f47",
   "metadata": {
    "id": "_JlwGaW8zZVg"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028a18d",
   "metadata": {
    "id": "339UoHUmzZVg"
   },
   "source": [
    "Building a pipeline for each row in the table, then running an evaluating them in a single loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2a024",
   "metadata": {
    "id": "3NkvMIEWzZVg"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc3f37",
   "metadata": {
    "id": "gTtr7_1ZzZVg"
   },
   "source": [
    "#### Naive Bayes variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bee487",
   "metadata": {
    "id": "wEzogpa6zZVg"
   },
   "outputs": [],
   "source": [
    "pipeline_nb_raw = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e47339",
   "metadata": {
    "id": "qKX64ELSzZVg"
   },
   "outputs": [],
   "source": [
    "pipeline_nb_tf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f2404",
   "metadata": {
    "id": "wunbIC4szZVg"
   },
   "source": [
    "#### SVM variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91167b8",
   "metadata": {
    "id": "hmuKZfL6zZVg"
   },
   "outputs": [],
   "source": [
    "pipeline_svm_raw = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66e967",
   "metadata": {
    "id": "v39uMKY6zZVh"
   },
   "outputs": [],
   "source": [
    "pipeline_svm_tf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf', SGDClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20baec96",
   "metadata": {
    "id": "_AeBm0nzzZVh"
   },
   "outputs": [],
   "source": [
    "for pipeline in [\n",
    "    pipeline_nb_raw, pipeline_nb_tf, \n",
    "    pipeline_svm_raw, pipeline_svm_tf\n",
    "]:\n",
    "    pipeline.fit(train.data, train.target)\n",
    "    predicted = pipeline.predict(test.data)\n",
    "    print(f\"{metrics.accuracy_score(test.target, predicted):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cccfcca",
   "metadata": {
    "id": "iRkFu7JgzZVm"
   },
   "source": [
    "### Results\n",
    "\n",
    "| Model | Term weighting | Accuracy |\n",
    "| -- | -- |:--:|\n",
    "| Naive Bayes | Raw counts | 0.864 |\n",
    "| Naive Bayes | TF | 0.667 |\n",
    "| SVM | Raw counts | 0.819 |\n",
    "| SVM | TF | 0.851 |\n",
    "| ... | ... | ... | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9bf747",
   "metadata": {
    "id": "nk7QEVvLzZVm"
   },
   "source": [
    "## Optional exercise\n",
    "\n",
    "Can you push performance ever further? You could try, for example, more sophisticated text preprocessing (tokenization, stopwords removal, and stemming) using [NLTK](https://www.nltk.org/) (which is part of the Anaconda distribution). See, e.g., [this article](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a) for some hints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12243d62",
   "metadata": {
    "id": "rL4_BaGE0zEv"
   },
   "source": [
    "# Building an inverted index\n",
    "\n",
    "  - You are given a sample (1000 documents) from the [The Reuters-21578 data collection](http://www.daviddlewis.com/resources/testcollections/reuters21578/) in `data/reuters21578-000.xml`\n",
    "  - The code that parses the XML and extract a list of preprocessed terms (tokenized, lowercased, stopwords removed) is already given\n",
    "  - You are also given an `InvertedIndex` class that manages the posting lists operations\n",
    "  - Your task is to build an inverted index from the input collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64374c7e",
   "metadata": {
    "id": "L7G74gLV05oy"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf87c4",
   "metadata": {
    "id": "kJ76N1E_0zEy"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import re\n",
    "\n",
    "from typing import List, Dict, Union, Any, Callable\n",
    "from collections import Counter, defaultdict\n",
    "from xml.dom import minidom\n",
    "from dataclasses import dataclass\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd478496",
   "metadata": {
    "id": "VbhPTOIs0zEz"
   },
   "source": [
    "## Parsing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808eae84",
   "metadata": {
    "id": "5QB7FC-F0zEz"
   },
   "source": [
    "Stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a04274",
   "metadata": {
    "id": "Ibx30S7o0zEz"
   },
   "outputs": [],
   "source": [
    "STOPWORDS = [\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4de92",
   "metadata": {
    "id": "xnq3ujD-0zE0"
   },
   "source": [
    "Stripping tags inside <> using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09b1af",
   "metadata": {
    "id": "GJ1EN1OM0zE0"
   },
   "outputs": [],
   "source": [
    "def striptags(text: str) -> str:\n",
    "    \"\"\"Removes xml tags.\n",
    "\n",
    "    Args:\n",
    "        text: Text string with xml tags.\n",
    "\n",
    "    Returns:\n",
    "        String without xml tags.\n",
    "    \"\"\"\n",
    "    p = re.compile(r\"<.*?>\")\n",
    "    return p.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714edf46",
   "metadata": {
    "id": "AR4RMMMP0zE1"
   },
   "source": [
    "Parse input text and return a list of indexable terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78866c32",
   "metadata": {
    "id": "cIFWI9Rt0zE2"
   },
   "outputs": [],
   "source": [
    "def parse(text: str) -> List[str]:\n",
    "    \"\"\"Parses documents and removes xml tags and punctuation.\n",
    "\n",
    "    Args:\n",
    "        text: Text to parse.\n",
    "\n",
    "    Returns:\n",
    "        List of tokens.\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    # Replace specific characters with space\n",
    "    chars = [\"'\", \".\", \":\", \",\", \"!\", \"?\", \"(\", \")\"]\n",
    "    for ch in chars:\n",
    "        text = text.replace(ch, \" \")\n",
    "\n",
    "    # Remove tags\n",
    "    text = striptags(text)\n",
    "\n",
    "    # Tokenization\n",
    "    # default behavior of the split is to split on one or more whitespaces\n",
    "    return [term.lower() for term in text.split() if term not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac06262",
   "metadata": {
    "id": "K3QM1JQD0zE3"
   },
   "source": [
    "## Processing the input document collection\n",
    "\n",
    "  - The collection is given as a single XML file. \n",
    "  - Each document is inside `<REUTERS ...> </REUTERS>`.\n",
    "  - We extract the contents of the `<DATE>`, `<TITLE>`, and `<BODY>` tags.\n",
    "  - After each extracted document, the provided callback function is called and all document data is passed in a single dict argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b75d0",
   "metadata": {
    "id": "QD0K0elm0zE3"
   },
   "outputs": [],
   "source": [
    "def process_collection(input_file:str, callback: Callable) -> None:\n",
    "    \"\"\"Processes file and calls the callback function for each document in the\n",
    "    file.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to file to process.\n",
    "        callback: Function that will be called for each document.\n",
    "    \"\"\"\n",
    "    xmldoc = minidom.parse(input_file)\n",
    "    # Iterate documents in the XML file\n",
    "    itemlist = xmldoc.getElementsByTagName(\"REUTERS\")\n",
    "    for doc_id, doc in enumerate(itemlist):\n",
    "        date = doc.getElementsByTagName(\"DATE\")[0].firstChild.nodeValue\n",
    "        # Skip documents without a title or body\n",
    "        if not (doc.getElementsByTagName(\"TITLE\") and doc.getElementsByTagName(\"BODY\")):\n",
    "            continue\n",
    "        title = doc.getElementsByTagName(\"TITLE\")[0].firstChild.nodeValue\n",
    "        body = doc.getElementsByTagName(\"BODY\")[0].firstChild.nodeValue\n",
    "        callback({\n",
    "            \"doc_id\": doc_id+1,\n",
    "            \"date\": date,\n",
    "            \"title\": title,\n",
    "            \"body\": body\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b89b8f",
   "metadata": {
    "id": "7fHekO5f0zE4"
   },
   "source": [
    "Prints a document\"s contents (used as a callback function passed to `process_collection`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1832a5",
   "metadata": {
    "id": "TeG8m6ZS0zE4"
   },
   "outputs": [],
   "source": [
    "def print_doc(doc: Dict[str, Union[str, int]]) -> None:\n",
    "    \"\"\"Print details of the first 5 documents.\n",
    "\n",
    "    Args:\n",
    "        doc: Dictionary with document details.\n",
    "    \"\"\"\n",
    "    if doc[\"doc_id\"] <= 5:  # print only the first 5 documents\n",
    "        print(\"docID:\", doc[\"doc_id\"])\n",
    "        print(\"date:\", doc[\"date\"])\n",
    "        print(\"title:\", doc[\"title\"])\n",
    "        print(\"body:\", doc[\"body\"])\n",
    "        print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016ed7f",
   "metadata": {
    "id": "MAtvfGkHyA47"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget --output-document=\"data/reuters21578-000.xml\" \"https://raw.githubusercontent.com/iai-group/ir-course-2022/main/resources/reuters21578-000.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01327b0",
   "metadata": {
    "id": "qjRWxXtZ0zE4"
   },
   "outputs": [],
   "source": [
    "process_collection(\"data/reuters21578-000.xml\", print_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34d387",
   "metadata": {
    "id": "7N8OHNMT0zE5"
   },
   "source": [
    "## Task 1: Complete the inverted index class\n",
    "\n",
    "  - The inverted index is an object with methods for adding and fetching postings.\n",
    "  - The data is stored in a map, where keys are terms and values are lists of postings.\n",
    "  - Each posting is an object that holds the doc_id and an optional payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5195f8",
   "metadata": {
    "id": "IY-UMYh60zE5"
   },
   "outputs": [],
   "source": [
    "# Since this is a simple data class, intializing it can be abstracted with\n",
    "# the use of dataclass decorator.\n",
    "# https://docs.python.org/3/library/dataclasses.html\n",
    "\n",
    "@dataclass\n",
    "class Posting:\n",
    "    doc_id: int\n",
    "    payload: Any = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b102f5",
   "metadata": {
    "id": "yk4YXBzB0zE5"
   },
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._index = defaultdict(list)\n",
    "    \n",
    "    def add_posting(self, term: str, doc_id: int, payload: Any=None) -> None:\n",
    "        \"\"\"Adds a document to the posting list of a term.\"\"\"\n",
    "        # append new posting to the posting list\n",
    "        self._index[term].append(Posting(doc_id, payload))\n",
    "\n",
    "    def get_postings(self, term: str) -> List[Posting]:\n",
    "        \"\"\"Fetches the posting list for a given term.\"\"\"\n",
    "        return self._index.get(term)\n",
    "\n",
    "    def get_terms(self) -> List[str]:\n",
    "        \"\"\"Returns all unique terms in the index.\"\"\"\n",
    "        return self._index.keys() \n",
    "    \n",
    "    def write_to_file(self, filename_index: str) -> None:\n",
    "        \"\"\"Saves the index to a textfile.\"\"\"\n",
    "        with open(filename_index, \"w\") as f:\n",
    "            for term, postings in self._index.items():\n",
    "                f.write(term)\n",
    "                for posting in postings:\n",
    "                    f.write(f\" {posting.doc_id}\")\n",
    "                    if posting.payload:\n",
    "                        f.write(f\":{str(posting.payload)}\")\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172dccb5",
   "metadata": {
    "id": "NMmgDSyK0zE5"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd227345",
   "metadata": {
    "id": "l6iG5YzW0zE5"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_postings():\n",
    "    ind = InvertedIndex()\n",
    "    ind.add_posting(\"term\", 1, 1)\n",
    "    ind.add_posting(\"term\", 2, 4)\n",
    "    # Testing existing term\n",
    "    postings = ind.get_postings(\"term\")\n",
    "    assert len(postings) == 2\n",
    "    assert postings[0].doc_id == 1\n",
    "    assert postings[0].payload == 1\n",
    "    assert postings[1].doc_id == 2\n",
    "    assert postings[1].payload == 4\n",
    "    # Testing non-existent term\n",
    "    assert ind.get_postings(\"xyx\") is None\n",
    "\n",
    "def test_vocabulary():\n",
    "    ind = InvertedIndex()\n",
    "    ind.add_posting(\"term1\", 1)\n",
    "    ind.add_posting(\"term2\", 1)\n",
    "    ind.add_posting(\"term3\", 2)\n",
    "    ind.add_posting(\"term2\", 3)\n",
    "    assert set(ind.get_terms()) == set([\"term1\", \"term2\", \"term3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c76645",
   "metadata": {
    "collapsed": true,
    "id": "2qcqTTxL0zE6"
   },
   "source": [
    "## Task 2: Build an inverted index from the input collection\n",
    "\n",
    "**TODO**: Complete the code to index the entire document collection.  (The content for each document should be the title and body concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109086f",
   "metadata": {
    "id": "bwwoSc8G0zE6"
   },
   "outputs": [],
   "source": [
    "ind = InvertedIndex()\n",
    "\n",
    "def index_doc(doc: Dict[str, Union[str, int]]) -> None:\n",
    "    \"\"\"Index document by concatenating document title and body.\n",
    "\n",
    "    Args:\n",
    "        doc: Document details.\n",
    "    \"\"\"\n",
    "    text = doc[\"title\"] + \" \" + doc[\"body\"]\n",
    "    terms = parse(text)  # list of terms in the document\n",
    "    tc = Counter(terms)  # dict with term counts\n",
    "    for term, freq in tc.items():\n",
    "        ind.add_posting(term, doc[\"doc_id\"], freq)\n",
    "    \n",
    "process_collection(\"data/reuters21578-000.xml\", index_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736e24b",
   "metadata": {
    "id": "nbDosh_20zE6"
   },
   "source": [
    "## Task 3: Save the inverted index to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d1eb4",
   "metadata": {
    "id": "qGuVr-Uf0zE6"
   },
   "source": [
    "Save the inverted index to a file (`data/index.dat`). Use a simple text format with `termID docID1:freq1 docID2:freq2 ...` per line, e.g.,\n",
    "\n",
    "```\n",
    "xxx 1:1 2:1 3:2\n",
    "yyy 2:1 4:2\n",
    "zzz 1:3 3:1 5:2\n",
    "...\n",
    "```\n",
    "\n",
    "Implement this by (1) adding a `write_to_file(self, filename)` method to the `InvertedIndex` class and then (2) invoking that method in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b78ac",
   "metadata": {
    "id": "ME1Qj5rl0zE7"
   },
   "outputs": [],
   "source": [
    "ind.write_to_file(\"data/index.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b286bd0",
   "metadata": {
    "id": "GYKS32Iz0zE7"
   },
   "source": [
    "## Task 4 (advanced, optional): Plot collection size against index size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6205b80",
   "metadata": {
    "id": "6HvWcEiK0zE7"
   },
   "source": [
    "Create a plot that compares the size of the document collection (bytes) against the size of the corresponding index (bytes) on the y-axis vs. with respect to the number of documents on the x-axis. You may use [Matplotlib](https://www.tutorialspoint.com/jupyter/jupyter_notebook_plotting.htm) for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c4f72",
   "metadata": {
    "id": "x9_vplhy0zE7"
   },
   "source": [
    "In our solution, we create a different callback function and use that one for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ba3d2",
   "metadata": {
    "id": "0dzpL9070zE7"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "ind = InvertedIndex()\n",
    "tmp_file = \"data/index_tmp.dat\"\n",
    "stats = {\n",
    "    \"i\": 0,\n",
    "    \"sum_bytes\": 0,\n",
    "    \"num_docs\": [],\n",
    "    \"size_index\": [],\n",
    "    \"size_docs\": []\n",
    "}\n",
    "\n",
    "def index_doc_with_stats(doc: Dict[str, Union[str, int]]) -> None:\n",
    "    \"\"\"Indexes documents and updates stats dictionary.\n",
    "\n",
    "    Args:\n",
    "        doc: [description]\n",
    "    \"\"\"\n",
    "    index_doc(doc)\n",
    "    # Stats are stored in a global variable (not very elegant but quick solution)\n",
    "    stats[\"i\"] += 1\n",
    "    stats[\"sum_bytes\"] += sys.getsizeof(str(doc))  # String document representation is a good proxy for doc size\n",
    "    # We measure index size and document collection size after every 100 docs\n",
    "    if stats[\"i\"] % 100 == 0:\n",
    "        stats[\"num_docs\"].append(stats[\"i\"])\n",
    "        stats[\"size_docs\"].append(stats[\"sum_bytes\"])\n",
    "        # To get index size, we dump it to a file and get file size\n",
    "        # Alternatively, the pympler package may be used to measure the size of Python objects\n",
    "        ind.write_to_file(tmp_file)\n",
    "        stats[\"size_index\"].append(os.path.getsize(tmp_file))\n",
    "        \n",
    "process_collection(\"data/reuters21578-000.xml\", index_doc_with_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1714608",
   "metadata": {
    "id": "mAyPt7yD0zE8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rendering plots inline in Jupyter notebooks.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00768b",
   "metadata": {
    "id": "N5FqGWSD0zE8"
   },
   "outputs": [],
   "source": [
    "plt.plot(stats[\"num_docs\"], stats[\"size_docs\"], label=\"Collection size\")\n",
    "plt.plot(stats[\"num_docs\"], stats[\"size_index\"], label=\"Index size\")\n",
    "plt.xlabel(\"Number of documents\")\n",
    "plt.ylabel(\"Bytes\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a3b61",
   "metadata": {
    "id": "MdMKpDqp0sx9"
   },
   "source": [
    "# Query processing with document-at-a-time scoring\n",
    "\n",
    "Implement term-at-a-time scoring using a simple retrieval function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389e4b1",
   "metadata": {
    "id": "aYlVD4N-06KN"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f150aad",
   "metadata": {
    "id": "ahi7ZfdE0szL"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "ipytest.autoconfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6202c",
   "metadata": {
    "id": "4ArgSZJv0szQ"
   },
   "source": [
    "### Inverted index\n",
    "\n",
    "For simplicity, the inverted index for the document collection is given as a dictionary, with a terms as keys and posting lists as values. Each posting is a (document ID, term frequency) tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd4b9d",
   "metadata": {
    "id": "blnMJ3e-0szR"
   },
   "outputs": [],
   "source": [
    "index = {\n",
    "    \"beijing\": [(1, 1), (4, 1)],\n",
    "    \"dish\": [(1, 1), (4, 1)],\n",
    "    \"duck\": [(0, 3), (1, 2), (2, 2), (4, 1)],\n",
    "    \"rabbit\": [(2, 1), (3, 1)],\n",
    "    \"recipe\": [(2, 1), (3, 1), (4, 1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4952d1",
   "metadata": {
    "id": "XA9pt5Or0szR"
   },
   "source": [
    "### Document lengths\n",
    "\n",
    "The length of each document is provided in a list. (Normally, this information would be present in a document metadata store or in a forward index.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9e49f",
   "metadata": {
    "id": "bLLrQzD20szR"
   },
   "outputs": [],
   "source": [
    "doc_len = [3, 4, 4, 2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c6172",
   "metadata": {
    "id": "svhgGxNn0szS"
   },
   "source": [
    "### Document-at-a-time scoring\n",
    "\n",
    "We utilize the fact that the posting lists are ordered by document ID.  Then, it\"s enough to iterate through each query term\"s posting list only once.  We keep a pointer for each query term.\n",
    "\n",
    "Normally, document scores would be kept in a priority queue. Here, for simplicity, we will keep them in a dictionary.\n",
    "\n",
    "The retrieval function we use is the following:\n",
    "\n",
    "$$score(q,d) = \\sum_{t \\in q} w_{t,d} \\times w_{t,q}$$\n",
    "\n",
    "where $w_{t,d}$ and $w_{t,q}$ are length-normalized term frequencies. I.e., $w_{t,d}=\\frac{c_{t,d}}{|d|}$, where $c_{t,d}$ is the number of occurrences of term $t$ in document $d$ and $|d|$ is the document length (=total number of terms). (It goes analogously for the query.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482ad8f",
   "metadata": {
    "id": "QjtZdr360szX"
   },
   "outputs": [],
   "source": [
    "def score_collection(index: Dict[str, List[Tuple[int, int]]], \n",
    "                    doc_len: List[int], \n",
    "                    query: str) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Scores all documents in the collection.\n",
    "    \n",
    "    Args:\n",
    "        index: Dict holding the inverted index.\n",
    "        doc_len: List with document lengths.\n",
    "        query: Search query.\n",
    "    \n",
    "    Returns:\n",
    "        List with (document_id, score) tuples, ordered by score desc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Turns the query string into a \"term: freq\" dictionary.\n",
    "    query_freqs = dict(Counter(query.split()))\n",
    "    # Computes query length (i.e., sum of all query term frequencies).\n",
    "    query_len = sum(query_freqs.values())\n",
    "\n",
    "    doc_scores = {}  # Holds the final document scores (this should be a priority list, but for simplicity we use a dictionary here).\n",
    "    \n",
    "    pos = {term: 0 for term in query_freqs}  # Holds a pointer for each query term\"s posting list.\n",
    "        \n",
    "    # Iterate through each document.\n",
    "    for doc_id in range(len(doc_len)):            \n",
    "        # First, we collect the document term frequencies from the index.\n",
    "        # (Essentially, we just \"recover\" the document\"s contents from the index.)\n",
    "        c_td = {}  # Holds the term frequencies in the document\n",
    "        for term in query_freqs.keys(): \n",
    "            # Get the term frequency from the posting list.\n",
    "            # Utilize the fact that the posting lists are ordered by document ID!\n",
    "            if pos[term] == len(index[term]):  # The end of the posting list has been reached.\n",
    "                continue\n",
    "            (d, freq) = index[term][pos[term]]\n",
    "            if d == doc_id:\n",
    "                c_td[term] = freq\n",
    "                pos[term] += 1\n",
    "            else:\n",
    "                # This means that d > doc_id, i.e., the term is not present in this doc.\n",
    "                pass\n",
    "                    \n",
    "        # Then, we score the document.\n",
    "        score = 0  # Holds the document\"s retrieval score\n",
    "        for term, c_tq in query_freqs.items():\n",
    "            # Incement the document\"s score according to the given query term\n",
    "            w_td = c_td.get(term, 0) / doc_len[doc_id]\n",
    "            w_tq = c_tq / query_len\n",
    "            score += w_td * w_tq\n",
    "        # Record final document score.\n",
    "        doc_scores[doc_id] = score\n",
    "        \n",
    "    return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2950d4",
   "metadata": {
    "id": "Ep_vjAij0szZ"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c1753",
   "metadata": {
    "id": "0okZZ0Jm0szZ"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_scoring():\n",
    "    scores = score_collection(index, doc_len, \"beijing duck recipe\")    \n",
    "    assert scores[0][0] == 0\n",
    "    assert scores[0][1] == pytest.approx(1/3, rel=1e-2)\n",
    "    assert scores[2][0] == 2\n",
    "    assert scores[2][1] == pytest.approx(1/4, rel=1e-2)\n",
    "    assert scores[4][0] == 3\n",
    "    assert scores[4][1] == pytest.approx(1/6, rel=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f37ca",
   "metadata": {
    "id": "gIEyon5l6IQU"
   },
   "source": [
    "# Document-term matrix generation\n",
    "\n",
    "In this exercise, you'll have to generate a document-term matrix from an input list of preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf63dc3",
   "metadata": {
    "id": "k7pnZko86I95"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca303a5",
   "metadata": {
    "id": "RvamgsW46IQW"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf02a285",
   "metadata": {
    "id": "S5tZ450q6IQX"
   },
   "source": [
    "Input documents are given as lists of tokenized terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38570b78",
   "metadata": {
    "id": "FrH3en926IQY"
   },
   "outputs": [],
   "source": [
    "DOCUMENTS = [\n",
    "    [\"aaa\", \"bbb\", \"ccc\"],\n",
    "    [\"eee\", \"fff\"],\n",
    "    [\"aaa\", \"eee\", \"aaa\", \"ccc\", \"fff\", \"fff\", \"ggg\", \"aaa\"],\n",
    "    [\"bbb\", \"bbb\", \"bbb\"],\n",
    "    [\"ggg\", \"fff\", \"ccc\", \"aaa\", \"ccc\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441481cf",
   "metadata": {
    "id": "91B-rxqF6IQY"
   },
   "source": [
    "### Solution #1\n",
    "\n",
    "Simple (naive) solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66875140",
   "metadata": {
    "id": "ispUaZ_g6IQZ"
   },
   "outputs": [],
   "source": [
    "def get_doc_term_matrix(docs: List[List[str]]) -> Tuple[List[List[int]], List[str]]:\n",
    "    \"\"\"Generates a document-term matrix and the corresponding vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of documents, each given by a list of tokenized terms.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple consisting of the document-term matrix and the corresponding vocabulary.\n",
    "        In the document-term matrix row `i` corresponds to `docs[i]` and column `j`\n",
    "        corresponds to the jth element of the vocabulary. Values represent the number\n",
    "        of times the term appears in the document.\n",
    "        Terms may be in any order in the vocabulary.\n",
    "    \"\"\"\n",
    "    vocabulary = list(set([term for doc in docs for term in doc]))\n",
    "    doc_term_matrix = []\n",
    "    for doc in docs:\n",
    "        doc_term_vector = []\n",
    "        for term in vocabulary:\n",
    "            doc_term_vector.append(doc.count(term))\n",
    "        doc_term_matrix.append(doc_term_vector)\n",
    "    return doc_term_matrix, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d5d0d6",
   "metadata": {
    "id": "U1hF9NkH6IQa"
   },
   "source": [
    "### Solution #2\n",
    "\n",
    "More effective solution avoiding iterating through document vectors multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ebb5c",
   "metadata": {
    "id": "SjADxUzD6IQb"
   },
   "outputs": [],
   "source": [
    "def get_doc_term_matrix(docs: List[List[str]]) -> Tuple[List[List[int]], List[str]]:\n",
    "    \"\"\"Generates a document-term matrix and the corresponding vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of documents, each given by a list of tokenized terms.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple consisting of the document-term matrix and the corresponding vocabulary.\n",
    "        In the document-term matrix row `i` corresponds to `docs[i]` and column `j`\n",
    "        corresponds to the jth element of the vocabulary. Values represent the number\n",
    "        of times the term appears in the document.\n",
    "        Terms may be in any order in the vocabulary.\n",
    "    \"\"\"\n",
    "    # The vocabulary is represented internally as a dictionary \n",
    "    # to allow for effective lookup of term indices.\n",
    "    vocabulary = {term: idx for idx, term in enumerate(set([term for doc in docs for term in doc]))}\n",
    "    # Initialize doc-term matrix with zeros.\n",
    "    doc_term_matrix = [[0] * len(vocabulary) for _ in range(len(docs))]\n",
    "    # Iterate through the content of each document only once and increase term counts.\n",
    "    for doc_idx, doc in enumerate(docs):\n",
    "        for term in doc:\n",
    "            term_idx = vocabulary[term]\n",
    "            doc_term_matrix[doc_idx][term_idx] += 1\n",
    "            \n",
    "    return doc_term_matrix, list(vocabulary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e22c6",
   "metadata": {
    "id": "exlFHcME6IQc"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070c78e",
   "metadata": {
    "id": "70ZsClfy6IQc"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_num_docs():\n",
    "    doc_term_matrix, _ = get_doc_term_matrix(DOCUMENTS)\n",
    "    assert len(doc_term_matrix) == len(DOCUMENTS)\n",
    "    \n",
    "def test_vocabulary():\n",
    "    _, vocabulary = get_doc_term_matrix(DOCUMENTS)\n",
    "    assert set(vocabulary) == {\"aaa\", \"bbb\", \"ccc\", \"eee\", \"fff\", \"ggg\"}\n",
    "    \n",
    "def test_term_counts():\n",
    "    doc_term_matrix, vocabulary = get_doc_term_matrix(DOCUMENTS)\n",
    "    idx_aaa = vocabulary.index(\"aaa\")\n",
    "    idx_ccc = vocabulary.index(\"ccc\")\n",
    "    idx_fff = vocabulary.index(\"fff\")\n",
    "    assert doc_term_matrix[0][idx_aaa] == 1\n",
    "    assert doc_term_matrix[0][idx_ccc] == 1\n",
    "    assert doc_term_matrix[0][idx_fff] == 0\n",
    "    assert doc_term_matrix[2][idx_aaa] == 3\n",
    "    assert doc_term_matrix[2][idx_ccc] == 1\n",
    "    assert doc_term_matrix[2][idx_fff] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d9050",
   "metadata": {
    "id": "1m-_nq1W5TNc"
   },
   "source": [
    "# TF-IDF weighting\n",
    "\n",
    "In this exercise, you'll have to compute term weightings (TF, IDF, and TF-IDF) based on a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892124a",
   "metadata": {
    "id": "e9V5y1Ds5h7s"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b70f1",
   "metadata": {
    "id": "Zi8CmXHl5TNe"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import ipytest\n",
    "import math\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b639fb9",
   "metadata": {
    "id": "QJ64Rg6w5TNe"
   },
   "source": [
    "The document-term vector contains the raw term frequencies for each term in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728e1dd",
   "metadata": {
    "id": "V7741y9c5TNf"
   },
   "outputs": [],
   "source": [
    "DOC_TERM_MATRIX = [\n",
    "    [0, 0, 3, 0, 0, 0],\n",
    "    [1, 1, 2, 0, 0, 0],\n",
    "    [0, 0, 2, 1, 1, 0],\n",
    "    [0, 0, 0, 1, 1, 0],\n",
    "    [1, 1, 1, 0, 1, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f7ff2",
   "metadata": {
    "id": "Sfs_i1C-5TNf"
   },
   "source": [
    "## Task 1: TF weighting\n",
    "\n",
    "Compute the L1-normalized term frequency vector for a given document.\n",
    "\n",
    "The L1-normalized frequency of a single term in a document is given by:\n",
    "\n",
    "$$tf_{t,d}=\\frac{c_{t,d}}{|d|}$$ \n",
    "\n",
    "where $c_{t,d}$ is the count of occurrences of term $t$ in document $d$ and $|d|$ is the document length (total number of terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e90a8",
   "metadata": {
    "id": "_JJUX9MQ5TNf"
   },
   "outputs": [],
   "source": [
    "def get_tf_vector(doc_term_vector: List[int]) -> List[float]:    \n",
    "    \"\"\"Computes the normalized term frequency vector from a raw term-frequency vector.\"\"\"\n",
    "    sum_freq = sum(doc_term_vector)\n",
    "    if sum_freq == 0:  # This would mean that the document has no content.\n",
    "        return None    \n",
    "    tf_vector = [freq / sum_freq for freq in doc_term_vector]\n",
    "    return tf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e0c77",
   "metadata": {
    "id": "b03c0Swf5TNg"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d940c24",
   "metadata": {
    "id": "u7439ty55TNg"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_tf_doc0():\n",
    "    assert get_tf_vector(DOC_TERM_MATRIX[0]) == [0, 0, 1, 0, 0, 0]\n",
    "    \n",
    "def test_tf_doc1():\n",
    "    assert get_tf_vector(DOC_TERM_MATRIX[1]) == [0.25, 0.25, 0.5, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad099d01",
   "metadata": {
    "id": "OQV_64DX5TNh"
   },
   "source": [
    "## Task 2: IDF weighting\n",
    "\n",
    "Compute the IDF weight of a term given by\n",
    "\n",
    "$$idf_{t}=\\log \\frac{N}{n_t}$$ \n",
    "\n",
    "where $N$ is the total number of documents and $n_t$ is the number of documents that contain term $t$.\n",
    "**Use base-10 logarithm in this exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1592e",
   "metadata": {
    "id": "M2N8Sorg5TNi"
   },
   "outputs": [],
   "source": [
    "def get_term_idf(doc_term_matrix: List[List[int]], term_index: int) -> float:\n",
    "    \"\"\"Computes the IDF value of a term, given by its index, based on a document-term matrix.\"\"\"\n",
    "    N = len(doc_term_matrix)\n",
    "    n_t = sum([1 if doc_freqs[term_index] > 0 else 0 for doc_freqs in doc_term_matrix])\n",
    "    return math.log10(N / n_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546f7b6",
   "metadata": {
    "id": "2VYGt3bd5TNi"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31fd59f",
   "metadata": {
    "id": "GL6doMTW5TNi"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_idf_term0():\n",
    "    assert get_term_idf(DOC_TERM_MATRIX, 0) == pytest.approx(0.3979, rel=1e-3)\n",
    "    \n",
    "def test_idf_term2():\n",
    "    assert get_term_idf(DOC_TERM_MATRIX, 2) == pytest.approx(0.0969, rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563c0bd",
   "metadata": {
    "id": "RMqwh8Um5TNj"
   },
   "source": [
    "## Task 3: TF-IDF weighting\n",
    "\n",
    "Compute the TF-IDF vector for a given document, where the TF-IDF weight of a term in a document is given by:\n",
    "\n",
    "$$ tfidf_{t,d} = tf_{t,d} \\times idf_{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cb1fc",
   "metadata": {
    "id": "0cg7q5jD5TNj"
   },
   "outputs": [],
   "source": [
    "def get_tfidf_vector(doc_term_matrix: List[List[int]], doc_index: int) -> List[float]:\n",
    "    \"\"\"Computes the TFIDF vector from a raw term-frequency vector.\"\"\"\n",
    "    tf_vector = get_tf_vector(doc_term_matrix[doc_index])\n",
    "    tfidf_vector = []\n",
    "    for term_index, tf in enumerate(tf_vector):\n",
    "        idf = get_term_idf(doc_term_matrix, term_index)\n",
    "        tfidf_vector.append(tf * idf)\n",
    "    return tfidf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273c835",
   "metadata": {
    "id": "wap1vDrE5TNj"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da1a5b",
   "metadata": {
    "id": "sHdsOX5P5TNj"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_tfidf_doc0():\n",
    "    assert get_tfidf_vector(DOC_TERM_MATRIX, 0) == pytest.approx([0, 0, 0.0969, 0, 0, 0], rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ee007",
   "metadata": {
    "id": "E78DPKQC1Tm1"
   },
   "source": [
    "# Vector space retrieval\n",
    "\n",
    "This exercise is about scoring a (toy-sized) document collection against a query using various retrieval functions instantiated in the vector space model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0042657",
   "metadata": {
    "id": "qVlPANtN1aSF"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa79fa8",
   "metadata": {
    "id": "7pze-sF31Tm5"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import math\n",
    "import pytest\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d574ba",
   "metadata": {
    "id": "GwY1u57S1Tm6"
   },
   "source": [
    "Term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5f1f1",
   "metadata": {
    "id": "t82wzTv_1Tm6"
   },
   "outputs": [],
   "source": [
    "TD_MATRIX_TYPE = Dict[str, List[int]]\n",
    "DOCUMENT_SCORES_TYPE = List[Tuple[int, float]]\n",
    "TD_MATRIX = {\n",
    "    \"beijing\": [0, 1, 0, 0, 1],\n",
    "    \"dish\": [0, 1, 0, 0, 1],\n",
    "    \"duck\": [3, 2, 2, 0, 1],\n",
    "    \"rabbit\": [0, 0, 1, 1, 0],\n",
    "    \"recipe\": [0, 0, 1, 1, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197c558",
   "metadata": {
    "id": "6FlCmtqo1Tm6"
   },
   "source": [
    "## Scoring\n",
    "\n",
    "The general scoring function is \n",
    "\n",
    "$$score(d,q) = \\sum_{t \\in q} w_{t,d} \\times w_{t,q}$$\n",
    "\n",
    "where $w_{t,d}$ is the term\"s weight in the document and $w_{t,q}$ is the term\"s weight in the query.\n",
    "\n",
    "The `Scorer` class below provides an abstract implementation of the above function. For a specific instantiation,  you\"ll need to create a child class and implement `_get_query_term_weight()` and `_get_doc_term_weight()`.\n",
    "\n",
    "For your convenience, the collection is provided in the form of a term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb4bb2",
   "metadata": {
    "id": "pmZDbMke1Tm7"
   },
   "outputs": [],
   "source": [
    "class AbstractScorer(ABC):\n",
    "    def __init__(self, td_matrix: TD_MATRIX_TYPE) -> None:\n",
    "        \"\"\"Initialize the scorer abstract class.\n",
    "\n",
    "        Args:\n",
    "            td_matrix: Dictionary of \"term: term count\" pairs.\n",
    "        \"\"\"\n",
    "        self._td_matrix = td_matrix\n",
    "        self._num_docs = len(list(td_matrix.values()))\n",
    "        self._query_terms = None\n",
    "\n",
    "    def _parse_query(self, query: str) -> None:\n",
    "        \"\"\"Parses the input query to a sequence of vocabulary terms and stores\n",
    "        it in a member variable.\n",
    "        \"\"\"\n",
    "        self._query_terms = [term for term in query.split() if term in self._td_matrix]\n",
    "\n",
    "\n",
    "        \n",
    "    def score_documents(self, query: str) -> DOCUMENT_SCORES_TYPE:\n",
    "        \"\"\"Score all documents in the collection.\n",
    "        \n",
    "        Params:\n",
    "            query: Query string\n",
    "        \n",
    "        Returns:\n",
    "            List of (document ID, score) tuples ordered by score descending, then by doc ID ascending.\n",
    "        \"\"\"\n",
    "        scores = {doc_id: 0 for doc_id in range(self._num_docs)}\n",
    "        self._parse_query(query)\n",
    "        \n",
    "        for term in set(self._query_terms):\n",
    "            for doc_id in range(self._num_docs):\n",
    "                scores[doc_id] += self._get_doc_term_weight(doc_id, term) * self._get_query_term_weight(term)\n",
    "                \n",
    "        return [(doc_id, score) for doc_id, score in sorted(scores.items(), key=lambda x: (x[1], -x[0]), reverse=True)]\n",
    "        \n",
    "    @abstractmethod\n",
    "    def _get_query_term_weight(self, term: str) -> float:\n",
    "        return 1\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_doc_term_weight(self, doc_id: int, term: str) -> float:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca65c55",
   "metadata": {
    "id": "W3yiIAG51Tm8"
   },
   "source": [
    "## Task 1: Binary scorer\n",
    "\n",
    "Set $w_{t,d}$ to 1 if $t$ is present in the document otherwise $0$.\n",
    "Similarly, Set $w_{t,q}$ to 1 if $t$ is present in the query otherwise $0$.\n",
    "\n",
    "This method will then score documents based on the number of matching (unique) query terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975170a7",
   "metadata": {
    "id": "-1oMp-hc1Tm8"
   },
   "outputs": [],
   "source": [
    "class BinaryScorer(AbstractScorer):\n",
    "    \n",
    "    def _get_query_term_weight(self, term: str) -> int:\n",
    "        return 1 if term in self._query_terms else 0\n",
    "    \n",
    "    def _get_doc_term_weight(self, doc_id: int, term: str) -> int:\n",
    "        return 1 if self._td_matrix[term][doc_id] > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d34b6",
   "metadata": {
    "id": "gDK4ogd71Tm9"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48959fdf",
   "metadata": {
    "id": "mdZO0JOa1Tm9"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"td_matrix,query,correct_values\", [\n",
    "    (TD_MATRIX, \"beijing\", [(1, 1), (4, 1), (0, 0), (2, 0), (3, 0)]),\n",
    "    (TD_MATRIX, \"beijing duck recipe\", [(4, 3), (1, 2), (2, 2), (0, 1), (3, 1)]),\n",
    "])\n",
    "def test_binary_scorer(td_matrix: TD_MATRIX_TYPE, query: str, correct_values: DOCUMENT_SCORES_TYPE):  \n",
    "    scorer = BinaryScorer(td_matrix)\n",
    "    assert scorer.score_documents(query) == correct_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd3c9d",
   "metadata": {
    "id": "wphghG4W1Tm-"
   },
   "source": [
    "## Task 2: TF scorer\n",
    "\n",
    "Set $w_{t,d}=\\frac{c_{t,d}}{|d|}$, that is, the relative frequency of the term in the document.\n",
    "\n",
    "For $w_{t,q}$, use the frequency (count) of the term in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1841f2a8",
   "metadata": {
    "id": "bSCaEj3s1Tm-"
   },
   "outputs": [],
   "source": [
    "class TFScorer(AbstractScorer):\n",
    "    \n",
    "    def __init__(self, td_matrix: TD_MATRIX_TYPE) -> None:\n",
    "        \"\"\"Initialize TFScorer. Here, the lengths of documents are precomputed\n",
    "        for more efficient scoring.\n",
    "\n",
    "        Args:\n",
    "            td_matrix: Dictionary of \"term: term count\" pairs.\n",
    "        \"\"\"\n",
    "        super(TFScorer,self).__init__(td_matrix)\n",
    "        # Pre-compute the length of documents for more efficient scoring.\n",
    "        self._doc_len = {}\n",
    "        for doc_id in range(self._num_docs):\n",
    "            self._doc_len[doc_id] = sum(self._td_matrix[term][doc_id] for term in self._td_matrix.keys())\n",
    "    \n",
    "    def _get_query_term_weight(self, term: str) -> int:\n",
    "        return self._query_terms.count(term)\n",
    "    \n",
    "    def _get_doc_term_weight(self, doc_id, term: str) -> float:\n",
    "        return self._td_matrix[term][doc_id] / self._doc_len[doc_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9876b",
   "metadata": {
    "id": "EPScSDbu1Tm-"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b47a1",
   "metadata": {
    "id": "z7XHoSaC1Tm-"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"td_matrix,query,correct_values\", [\n",
    "    (TD_MATRIX, \"beijing\", [(1, 0.25), (4, 0.25), (0, 0), (2, 0), (3, 0)]),\n",
    "    (TD_MATRIX, \"duck duck\", [(0, 2), (1, 1), (2, 1), (4, 0.5), (3, 0)]),\n",
    "    (TD_MATRIX, \"beijing duck recipe\", [(0, 1.0), (1, 0.75), (2, 0.75), (4, 0.75), (3, 0.5)]),\n",
    "])\n",
    "def test_tf_scorer(td_matrix: DOCUMENT_SCORES_TYPE, query: str, correct_values: DOCUMENT_SCORES_TYPE):  \n",
    "    scorer = TFScorer(td_matrix)\n",
    "    assert scorer.score_documents(query) == correct_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40956f4f",
   "metadata": {
    "id": "MOlsDY3E1Tm_"
   },
   "source": [
    "## Task 3: TD-IDF scorer\n",
    "\n",
    "Implement the scoring function \n",
    "\n",
    "$$score(d,q) = \\sum_{t \\in q} tf_{t,q} \\times tf_{t,d} \\times idf_t$$\n",
    "\n",
    "Use normalized frequencies for TF weight, i.e., $tf_{t,d}=\\frac{c_{t,d}}{|d|}$, where $c_{t,d}$ is the number of occurrences of term $t$ in document $d$ and $|d|$ is the document length (=total number of terms). (Analogously for the query.)\n",
    "\n",
    "Compute IDF values using the following formula: $idf_{t}=\\log \\frac{N}{n_t}$, where $N$ is the total number of documents and $n_t$ is the number of documents that contain term $t$.  Use base-10 the logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274a9ed",
   "metadata": {
    "id": "5fm7oRkU1Tm_"
   },
   "outputs": [],
   "source": [
    "class TFIDFScorer(AbstractScorer):\n",
    "    \n",
    "    def __init__(self, td_matrix: TD_MATRIX_TYPE) -> None:\n",
    "        \"\"\"Initializes TFIDFScorer. Here, both document lengts and IDF values\n",
    "        are precomputes.\n",
    "\n",
    "        Args:\n",
    "            td_matrix: Dictionary of \"term: term count\" pairs.\n",
    "        \"\"\"\n",
    "        super(TFIDFScorer,self).__init__(td_matrix)\n",
    "        # Pre-compute the length of documents for more efficient scoring.\n",
    "        self._doc_len = {}\n",
    "        for doc_id in range(self._num_docs):\n",
    "            self._doc_len[doc_id] = sum(self._td_matrix[term][doc_id] for term in self._td_matrix.keys())\n",
    "        # Pre-compute IDF values.\n",
    "        self._idf = {}\n",
    "        for term, freqs in self._td_matrix.items():\n",
    "            nt = sum(1 if f > 0 else 0 for f in freqs)\n",
    "            self._idf[term] = math.log10(self._num_docs / nt)\n",
    "    \n",
    "    def _get_query_term_weight(self, term: str) -> float:\n",
    "        return self._query_terms.count(term) / len(self._query_terms)\n",
    "    \n",
    "    def _get_doc_term_weight(self, doc_id: int, term: str) -> float:\n",
    "        return self._td_matrix[term][doc_id] / self._doc_len[doc_id] * self._idf[term]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537c459",
   "metadata": {
    "id": "RSE8FcP91Tm_"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d1cd5",
   "metadata": {
    "id": "iQTSwLII1Tm_"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"td_matrix,query,correct_values\", [\n",
    "    (TD_MATRIX, \"beijing\", [(1, 0.0995), (4, 0.0995), (0, 0), (2, 0), (3, 0)]),\n",
    "    (TD_MATRIX, \"duck duck\", [(0, 0.0969), (1, 0.0485), (2, 0.0485), (4, 0.0242), (3, 0)]),\n",
    "    (TD_MATRIX, \"beijing duck recipe\", [(4, 0.0597), (1, 0.0493), (3, 0.0369), (2, 0.0346), (0, 0.0323)]),\n",
    "])\n",
    "def test_tfidf_scorer(td_matrix: TD_MATRIX_TYPE, query: str, correct_values: DOCUMENT_SCORES_TYPE):  \n",
    "    scorer = TFIDFScorer(td_matrix)\n",
    "    ranking = scorer.score_documents(query)\n",
    "    assert [x[0] for x in ranking] == [x[0] for x in correct_values]  # Checking ranking\n",
    "    assert [x[1] for x in ranking] == pytest.approx([x[1] for x in correct_values], rel=1e-2)  # Checking scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d19de2",
   "metadata": {
    "id": "VAvhvrho5G0i"
   },
   "source": [
    "# Rocchio feedback\n",
    "\n",
    "Compute an expanded query model using Rocchio feedback, given a set of positive and negative documents as expicit feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badab0db",
   "metadata": {
    "id": "DMz2Om5b5KJP"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da50b0",
   "metadata": {
    "id": "2oPPH8335G0l"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "from typing import List\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df78cc9",
   "metadata": {
    "id": "jb9882cW5G0m"
   },
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0695f",
   "metadata": {
    "id": "Hcv8xnWj5G0n"
   },
   "outputs": [],
   "source": [
    "VOCAB = ['news', 'about', 'presidental', 'campaign', 'food', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0c50e7",
   "metadata": {
    "id": "C3ZJ2XBG5G0n"
   },
   "source": [
    "Query vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59854bb",
   "metadata": {
    "id": "O-jdXQ8_5G0n"
   },
   "outputs": [],
   "source": [
    "Q = [1, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a25a4",
   "metadata": {
    "id": "FUDanX_V5G0n"
   },
   "source": [
    "Document-term matrix (each row corresponds to a document vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcaff18",
   "metadata": {
    "id": "3Baj_rPJ5G0o"
   },
   "outputs": [],
   "source": [
    "DT_MATRIX = [\n",
    "    [1.5, 0.1, 0, 0, 0, 0],\n",
    "    [1.5, 0.1, 0, 2, 2, 0],\n",
    "    [1.5, 0, 3, 2, 0, 0],\n",
    "    [1.5, 0, 4, 2, 0, 0], \n",
    "    [1.5, 0, 0, 6, 2, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69e056",
   "metadata": {
    "id": "AlSxZUU45G0o"
   },
   "source": [
    "Feedback: IDs (indices) of positive and negative documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f464cc",
   "metadata": {
    "id": "P7YQRW7g5G0p"
   },
   "outputs": [],
   "source": [
    "D_POS = [2, 3]\n",
    "D_NEG = [0, 1, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877cadc",
   "metadata": {
    "id": "3TbcRl4s5G0p"
   },
   "source": [
    "## Rocchio feedback\n",
    "\n",
    "Compute the updated query according to:\n",
    "$$\\vec{q}_m = \\alpha \\vec{q} + \\frac{\\beta}{|D^+|}\\sum_{d \\in D^+}\\vec{d} - \\frac{\\gamma}{|D^-|}\\sum_{d \\in D^-}\\vec{d}$$\n",
    "\n",
    "where\n",
    "  - $\\vec{q}$ is the original query vector\n",
    "  - $\\vec{d}$ is the term vector of document $d$\n",
    "  - $D^+, D^-$ are set of relevant and non-relevant feedback documents\n",
    "  - $\\alpha, \\beta, \\gamma$ are parameters that control the movement of the original vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41010429",
   "metadata": {
    "id": "S2QDBH6g5G0p"
   },
   "outputs": [],
   "source": [
    "def get_updated_query(\n",
    "    q: List[int], d_pos: List[int], d_neg: List[int], \n",
    "    alpha: float, beta: float, gamma: float\n",
    ") -> List[int]:\n",
    "    \"\"\"Computes an updated query model using Rocchio feedback.\n",
    "    \n",
    "    Args:\n",
    "        q: Query vector.\n",
    "        d_pos: List of positive feedback document IDs.\n",
    "        d_neg: List of positive feedback document IDs.\n",
    "        alpha: Feedback parameter alpha.\n",
    "        beta: Feedback parameter beta.\n",
    "        gamma: Feedback parameter gamma.\n",
    "    \n",
    "    Returns:\n",
    "        Updated query vector.\n",
    "    \"\"\"\n",
    "    q_m = [alpha * t for t in q]\n",
    "    \n",
    "    # Positive feedback docs\n",
    "    for idx in d_pos:\n",
    "        for t in range(len(VOCAB)):\n",
    "            q_m[t] += beta / len(d_pos) * DT_MATRIX[idx][t]\n",
    "        \n",
    "    # Negative feedback docs\n",
    "    for idx in d_neg:\n",
    "        for t in range(len(VOCAB)):\n",
    "            q_m[t] -= gamma / len(d_neg) * DT_MATRIX[idx][t]\n",
    "        \n",
    "    return q_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7e8f4",
   "metadata": {
    "id": "I7Rq8j8D5G0q"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b65e43",
   "metadata": {
    "id": "8nNr9t2E5G0q"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_no_expansion():\n",
    "    q_m = get_updated_query(Q, D_POS, D_NEG, 1, 0, 0)\n",
    "    assert q_m == Q\n",
    "\n",
    "def test_expansion():\n",
    "    q_m = get_updated_query(Q, D_POS, D_NEG, 0.6, 0.2, 0.2)\n",
    "    assert q_m == pytest.approx([0.600, 0.587, 1.300, 0.467, -0.267, 0], rel=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03ee8f",
   "metadata": {
    "id": "Wv0uJoff5E5L"
   },
   "source": [
    "# PageRank calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023b355",
   "metadata": {
    "id": "8ag2D1ta5JK_"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74505770",
   "metadata": {
    "id": "dr8ob1225E5N"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b879a",
   "metadata": {
    "id": "m7RZ_FyJ5E5O"
   },
   "source": [
    "You're given a web graph in form of an edge list. Each edge is represented as a `(from_node, to_node)` tuple.\n",
    "(We assume that there is at most one link between any pair of nodes and that the input is correct.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bddd87",
   "metadata": {
    "id": "DnZrVa2m5E5O"
   },
   "source": [
    "## Input 1\n",
    "\n",
    "![](https://raw.githubusercontent.com/iai-group/ir-course-2022/main/resources/pagerank1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe7f03",
   "metadata": {
    "id": "RUkTR4oW5E5P"
   },
   "outputs": [],
   "source": [
    "WEB_GRAPH_1 = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"C\"), (\"C\", \"A\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dfdf2",
   "metadata": {
    "id": "m3kmW3Ch5E5P"
   },
   "source": [
    "## Input 2\n",
    "\n",
    "![](https://raw.githubusercontent.com/iai-group/ir-course-2022/main/resources/pagerank2.png)\n",
    "\n",
    "Mind that this web graph contains rank sinks, i.e., nodes that have only incoming edges but no outgoing ones. You'll need to deal with those by adding an incoming link from all nodes (including the very node itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4f2b3",
   "metadata": {
    "id": "x2daG6No5E5P"
   },
   "outputs": [],
   "source": [
    "WEB_GRAPH_2 = [(1, 2), (1, 3), (3, 1), (3, 2), (3, 5), (4, 5), (4, 6), (5, 4), (5, 6), (6, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ff889",
   "metadata": {
    "id": "sVLOmXGZ5E5Q"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46289bb",
   "metadata": {
    "id": "ixPfigQG5E5Q"
   },
   "outputs": [],
   "source": [
    "def get_all_nodes(web_graph: List[Tuple[Any, Any]]) -> Set[Any]:\n",
    "    \"\"\"Returns a list of nodes given a web graph.\n",
    "    \n",
    "    Params:\n",
    "        web_graph: List of edges.\n",
    "\n",
    "    Returns:\n",
    "        Set of nodes.\n",
    "    \"\"\"\n",
    "    nodes = set()\n",
    "    for (from_node, to_node) in web_graph:\n",
    "        nodes.add(from_node)\n",
    "        nodes.add(to_node)\n",
    "    \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4dbdd5",
   "metadata": {
    "id": "cYMEUI-05E5R"
   },
   "outputs": [],
   "source": [
    "def get_outlinks_num(web_graph: List[Tuple[Any, Any]]) -> Dict[Any, int]:\n",
    "    \"\"\"Computes the number of outgoing links for each node in a web graph.\n",
    "    \n",
    "    Param:\n",
    "        web_graph: List of edges.\n",
    "\n",
    "    Returns:\n",
    "        Dict with nodes as keys and the number of outgoing nodes as values.\n",
    "    \"\"\"\n",
    "    outlinks = {node: 0 for node in get_all_nodes(web_graph)}\n",
    "    for (from_node, to_node) in web_graph:\n",
    "        outlinks[from_node] += 1    \n",
    "    return outlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13af82b",
   "metadata": {
    "id": "I7mcmfk05E5R"
   },
   "source": [
    "## PageRank calculation\n",
    "\n",
    "The pagerank of a given node $a$ is computed using:\n",
    "\n",
    "$$PR(a) = \\frac{q}{T} + (1-q) \\sum_{i=1}^n \\frac{PR(p_i)}{L(p_i)}$$\n",
    "\n",
    "where \n",
    "  - $q$ is the probability of jumping to a random page\n",
    "  - $T$ is the total number of pages (nodes) in the Web graph\n",
    "  - $p_1\\dots p_n$ are pages that **point to** page $a$\n",
    "  - $PR(p_i)$ is the PageRank value of page $p_i$\n",
    "  - $L(p_i)$ is the number of outgoing links of page $p_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b524d9a",
   "metadata": {
    "id": "v4miVIZK5E5R"
   },
   "outputs": [],
   "source": [
    "def pagerank(web_graph: List[Tuple[Any, Any]], q: float = 0.15, iterations: int = 3) -> Dict[Any, float]:\n",
    "    \"\"\"Computes PageRank for all nodes in a web graph.\n",
    "    \n",
    "    Params:\n",
    "        web_graph: List of edges.\n",
    "        q: Random jump probability.\n",
    "        iterations: Number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "        Dict with node names as keys and PageRank scores as values.    \n",
    "    \"\"\"\n",
    "    nodes = get_all_nodes(web_graph)\n",
    "    # Calculate the number of outgoing links of each page.\n",
    "    outlinks_num = get_outlinks_num(web_graph)\n",
    "    # Collect all inlinks of a page for more efficient PageRank computation.\n",
    "    inlinks = {node: [] for node in nodes}\n",
    "    for (from_node, to_node) in web_graph:\n",
    "        inlinks[to_node].append(from_node)\n",
    "    \n",
    "    # Identify and deal with rank sinks.\n",
    "    for node, lnum in outlinks_num.items():\n",
    "        if lnum == 0:\n",
    "            print('Node {} is a rank sink!'.format(node))\n",
    "            # Add links to all nodes (including the node itself).\n",
    "            for to_node in nodes:\n",
    "                inlinks[to_node].append(node)\n",
    "            # Update outlinks count.\n",
    "            outlinks_num[node] = len(nodes)\n",
    "    \n",
    "    # Initialize pagerank values.\n",
    "    pr = {node: 1/len(nodes) for node in nodes}\n",
    "    \n",
    "    # Calculate pagerank scores iteratively.\n",
    "    for i in range(iterations):\n",
    "        pr_old = pr.copy()\n",
    "        for node in pr.keys():\n",
    "            pr[node] = q / len(nodes)\n",
    "            # Iterating over all pages p_i that link to node. \n",
    "            for from_node in inlinks[node]:\n",
    "                pr[node] += (1 - q) * pr_old[from_node] / outlinks_num[from_node]\n",
    "    \n",
    "    return pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c476cd",
   "metadata": {
    "id": "5i8pxuUl5E5S"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d435ea",
   "metadata": {
    "id": "e2znTYS75E5S"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"web_graph,q,iterations,correct_values\", [\n",
    "    (WEB_GRAPH_1, 0.5, 0, {\"A\": 1/3, \"B\": 1/3, \"C\": 1/3}),\n",
    "    (WEB_GRAPH_1, 0.5, 1, {\"A\": 0.3333, \"B\": 0.25, \"C\": 0.4166}),\n",
    "    (WEB_GRAPH_1, 0.5, 2, {\"A\": 0.375, \"B\": 0.25, \"C\": 0.375}),\n",
    "    (WEB_GRAPH_1, 0.5, 3, {\"A\": 0.3541, \"B\": 0.2604, \"C\": 0.3854}),\n",
    "    (WEB_GRAPH_2, 0.15, 0, {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}),\n",
    "    (WEB_GRAPH_2, 0.15, 1, {1: 0.0958, 2: 0.1666, 3: 0.1194, 4: 0.2611, 5: 0.1666, 6: 0.1902}),\n",
    "    (WEB_GRAPH_2, 0.15, 2, {1: 0.0824, 2: 0.1231, 3: 0.0893, 4: 0.2811, 5: 0.1934, 6: 0.2304}),\n",
    "])\n",
    "def test_pagerank(web_graph, q, iterations, correct_values):    \n",
    "    assert pagerank(web_graph, q=q, iterations=iterations) == pytest.approx(correct_values, rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6246f0a2",
   "metadata": {
    "id": "qlYrt9EfvGNm"
   },
   "source": [
    "# Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0bb3d",
   "metadata": {
    "id": "vL_lAVJfvITk"
   },
   "source": [
    "Compare the effectiveness of System A and System B on a test collection consisting of three queries. The table below contains the rankings generated by the two systems as well as the ground truth. We assume that relevance is binary, i.e., the ground truth column contains a set of the relevant documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0497893",
   "metadata": {
    "id": "lKt78N65vKJo"
   },
   "source": [
    "Document rankings produced by two systems and binary relevance judgements:\n",
    "\n",
    "\\begin{array}{|l|l|l|l|}\n",
    "    \\hline    \n",
    "    \\textbf{Query} & \\textbf{System A ranking} & \\textbf{System B ranking} & \\textbf{Ground truth} \\\\\n",
    "    \\hline\n",
    "    \\hline\n",
    "    Q1 & \\textbf{1}, 2, 4, 5, \\textbf{3}, 6, 9, 8, 10, 7 & 2, 4, \\textbf{3}, 10, 5, 6, 7, 8, 9, \\textbf{1} & 1, 3 \\\\\n",
    "    \\hline\n",
    "    Q2 & 1, \\textbf{2}, \\textbf{4}, \\textbf{5}, 3, 9, 8, \\textbf{6}, 10, 7 & \\textbf{5}, \\textbf{6}, \\textbf{4}, 1, 7, 8, 9, 10, 3, \\textbf{2} & 2, 4, 5, 6 \\\\\n",
    "    \\hline\n",
    "    Q3 & 1, \\textbf{7}, 4, 5, 3, 6, 9, 8, 10, 2 & 2, 4, 3, \\textbf{7}, 5, 6, 1, 8, 9, 10 & 7 \\\\\n",
    "    \\hline\n",
    "  \\end{array}\n",
    "\n",
    "We boldface the relevant documents in the table for a better overview."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f76dd",
   "metadata": {
    "id": "NliOc3klvNJ-"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f1ae3",
   "metadata": {
    "id": "Vs1Z-1k2x41P"
   },
   "source": [
    "\n",
    "First we compute effectiveness metrics for individual queries (rows 1--3 in the table below. Then, we average these number over the set of queries (row 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa7f75",
   "metadata": {
    "id": "MdLT7nWmvNxF"
   },
   "source": [
    "\n",
    "Effectiveness measures fo **System A**: \n",
    "\n",
    "\\begin{array}{|l||c|c|c|c|}\n",
    "    \\hline    \n",
    "    \\textbf{Query} & P@5 & P@10 & (M)AP & (M)RR \\\\\n",
    "    \\hline\n",
    "    \\hline\n",
    "\tQ1 & \n",
    "\t\t\\frac{2}{5} & \\frac{2}{10} & (\\frac{1}{1} + \\frac{2}{5}) / 2 & \\frac{1}{1} \\\\\n",
    "    \\hline\n",
    "\tQ2 & \n",
    "\t\t\\frac{3}{5} & \\frac{4}{10} & (\\frac{1}{2} + \\frac{2}{3} + \\frac{3}{4} + \\frac{4}{8}) / 4 & \\frac{1}{2} \\\\\n",
    "    \\hline\n",
    "\tQ3 & \n",
    "\t \\frac{1}{5} & \\frac{1}{10} & (\\frac{1}{2}) / 1 & \\frac{1}{2} \\\\\n",
    "    \\hline\n",
    "    \\hline\t\t    \n",
    "\tAverage & \n",
    "\t\t0.4 & 0.233 & 0.601 & 0.666 \\\\\n",
    "    \\hline\n",
    "  \\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2704d88",
   "metadata": {
    "id": "P37WY0XpvPi7"
   },
   "source": [
    "\n",
    "Effectiveness measures fo **System B**: \n",
    "\n",
    "\\begin{array}{|l||c|c|c|c|}\n",
    "    \\hline    \n",
    "    \\textbf{Query} & P@5 & P@10 & (M)AP & (M)RR \\\\\n",
    "    \\hline\n",
    "    \\hline\n",
    "\tQ1 & \\frac{1}{5} & \\frac{2}{10} & (\\frac{1}{3} + \\frac{2}{10}) / 2 & \\frac{1}{3} \\\\\n",
    "    \\hline\n",
    "\tQ2 & \\frac{3}{5} & \\frac{4}{10} & (\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + \\frac{4}{10}) / 4  & \\frac{1}{1} \\\\\n",
    "    \\hline\n",
    "\tQ3 & \\frac{1}{5} & \\frac{1}{10} & (\\frac{1}{4}) / 1 & \\frac{1}{4} \\\\\n",
    "    \\hline\n",
    "    \\hline\t\t    \n",
    "\tAverage & 0.333 & 0.233 & 0.455 & 0.527 \\\\\n",
    "    \\hline\n",
    "  \\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f541d8",
   "metadata": {
    "id": "3jrAhMZM358y"
   },
   "source": [
    "# Interpolated Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b821e",
   "metadata": {
    "id": "wIl_QLWd3580"
   },
   "source": [
    "In this exercise, you'll have to calculate interpolated precision for generating smooth precision-recall graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cc09a",
   "metadata": {
    "id": "vPw1y5783-Mg"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb95bc",
   "metadata": {
    "id": "ylnunqPc3581"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2769c",
   "metadata": {
    "id": "GCH_Z55Z3582"
   },
   "source": [
    "You're given precision and recall values measured at various rank positions (indexed from 0) for a given ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f0ebf",
   "metadata": {
    "id": "L-c_RSN_3582"
   },
   "outputs": [],
   "source": [
    "precision = [1.0, 0.5, 0.67, 0.75, 0.8, 0.83, 0.71, 0.63, 0.56, 0.6]\n",
    "recall = [0.17, 0.17, 0.33, 0.5, 0.67, 0.83, 0.83, 0.83, 0.83, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef0b52",
   "metadata": {
    "id": "bvnkiegX3582"
   },
   "source": [
    "We can plot these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b97425",
   "metadata": {
    "id": "atBVz2QH3582"
   },
   "outputs": [],
   "source": [
    "plt.plot(recall, precision)\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa330f20",
   "metadata": {
    "id": "YoYcnAuC3583"
   },
   "source": [
    "As you can see, this is not exactly a pretty plot. \n",
    "\n",
    "Instead, we'd like to report on standard recall levels R'=(0.0, 0.1, ... 1.0) using interpolated precision values.\n",
    "\n",
    "We calculate interpolated precision at a given recall level using\n",
    "\n",
    "$$P(R) = \\max \\{ P' : R' \\geq R \\wedge (R',P') \\in S \\} ~,$$\n",
    "\n",
    "where S is the set of observed (R,P) points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b1fd2",
   "metadata": {
    "id": "qlS13RUM3584"
   },
   "outputs": [],
   "source": [
    "recall_levels = np.arange(0, 1.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a958af7",
   "metadata": {
    "id": "wtMh12VY3584"
   },
   "outputs": [],
   "source": [
    "interpolated_precision = []\n",
    "for r_prime in recall_levels:\n",
    "    interpolated_precision.append(\n",
    "        max([p for (p, r) in zip(precision, recall) if r >= r_prime])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909fc14",
   "metadata": {
    "id": "e0rklQwa3585"
   },
   "source": [
    "We can now generate a new plot using standard recall levels and interpolated precision values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52054f3",
   "metadata": {
    "id": "CMnYwwXK3585"
   },
   "outputs": [],
   "source": [
    "plt.plot(recall_levels, interpolated_precision)\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31798b3",
   "metadata": {
    "id": "5gtEalrR37oa"
   },
   "source": [
    "# NDCG Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4f6f3",
   "metadata": {
    "id": "aHmo-hyD37oc"
   },
   "source": [
    "In this exercise, you'll have to evaluate system rankings, by computing the Normalized Discounted Cumulative Gain (NDCG) measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9919ba",
   "metadata": {
    "id": "QkCf9_gk3_is"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc09a9",
   "metadata": {
    "id": "u6Vxxoj837od"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import math\n",
    "import pytest\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a74bd68",
   "metadata": {
    "id": "fbRq-Nl037oe"
   },
   "source": [
    "### Rankings produced for each query\n",
    "\n",
    "The key is the query ID (string), the value is a list of document IDs (ints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70030e26",
   "metadata": {
    "id": "QjPhihZe37oe"
   },
   "outputs": [],
   "source": [
    "system_rankings = {\n",
    "    \"q1\": [2, 1, 3, 4, 5, 6, 10, 7, 9, 8],\n",
    "    \"q2\": [1, 2, 9, 4, 5, 6, 7, 8, 3, 10],\n",
    "    \"q3\": [1, 7, 4, 5, 3, 6, 9, 8, 10, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a909f",
   "metadata": {
    "id": "ATFATI3M37oe"
   },
   "source": [
    "### Ground truth\n",
    "\n",
    "The key is the query ID, the value is a dictionary with (document ID, relevance) pairs. Relevance is measured on a 3-point scale: non-relevant (0), poor (1), good (2), excellent (3). Documents not listed here are non-relevant (relevance=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5158d99",
   "metadata": {
    "id": "W_GGNJ7h37oe"
   },
   "outputs": [],
   "source": [
    "ground_truth = {\n",
    "    \"q1\": {4: 3, 1: 2, 2: 1},\n",
    "    \"q2\": {3: 3, 4: 3, 1: 2, 2: 1, 8: 1},\n",
    "    \"q3\": {1: 3, 4: 3, 7: 2, 5: 2, 6: 1, 8: 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8f06a",
   "metadata": {
    "id": "xd917lkb37of"
   },
   "source": [
    "## Computing evaluation metrics\n",
    "\n",
    "Discounted cumulative gain at rank $k$ is computed as:\n",
    "\n",
    "$$DCG_k = rel_1 + \\sum_{i=2}^k\\frac{rel_i}{\\log_2 i}$$\n",
    "\n",
    "Normalized discounted cumulative gain at rank $k$ is computed as:\n",
    "\n",
    "$$NDCG_k = \\frac{DCG_k}{IDCG_k}$$\n",
    "\n",
    "where $IDCG_k$ is the $DCG_k$ score of an idealized (perfect) ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7415c1",
   "metadata": {
    "id": "P9qGPph637og"
   },
   "outputs": [],
   "source": [
    "def dcg(relevances: List[int], k: int) -> float:\n",
    "    \"\"\"Computes DCG@k, given the corresponding relevance levels for a ranked list of documents.\n",
    "    \n",
    "    For example, given a ranking [2, 3, 1] where the relevance levels according to the ground \n",
    "    truth are {1:3, 2:4, 3:1}, the input list will be [4, 1, 3].\n",
    "    \n",
    "    Args:\n",
    "        relevances: List with the ground truth relevance levels corresponding to a ranked list of documents.\n",
    "        k: Rank cut-off.\n",
    "        \n",
    "    Returns:\n",
    "        DCG@k (float).\n",
    "    \"\"\"\n",
    "    # Note: Rank position is indexed from 1.\n",
    "    return relevances[0] + sum(\n",
    "        rel / math.log(i + 2, 2) \n",
    "         for i, rel in enumerate(relevances[1:k])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfccd7f4",
   "metadata": {
    "id": "t_ZtaVPKi34z"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"relevances,k,correct_value\", [\n",
    "    ([4, 1, 3], 2, 5.0),\n",
    "    ([4, 1, 3], 5, 6.893)\n",
    "])\n",
    "def test_dcg(relevances, k, correct_value):    \n",
    "    assert dcg(relevances, k) == pytest.approx(correct_value, rel=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebdedcd",
   "metadata": {
    "id": "tYOEa9Vh37og"
   },
   "outputs": [],
   "source": [
    "def ndcg(system_ranking: List[int], ground_truth: List[int], k:int = 10) -> float:\n",
    "    \"\"\"Computes NDCG@k for a given system ranking.\n",
    "    \n",
    "    Args:\n",
    "        system_ranking: Ranked list of document IDs (from most to least relevant).\n",
    "        ground_truth: Dict with document ID: relevance level pairs. Document not present here are to be taken with relevance = 0.\n",
    "        k: Rank cut-off.\n",
    "    \n",
    "    Returns:\n",
    "        NDCG@k (float).\n",
    "    \"\"\"\n",
    "    # Relevance levels for the ranked docs.\n",
    "    relevances = [ground_truth.get(rank,0) for rank in system_ranking]\n",
    "\n",
    "    # Relevance levels of the idealized ranking.\n",
    "    relevances_ideal = sorted(ground_truth.values(), reverse=True)\n",
    "    \n",
    "    return dcg(relevances, k) / dcg(relevances_ideal, k)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e971a",
   "metadata": {
    "id": "CAtpsZ0d37oh"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65add8eb",
   "metadata": {
    "id": "H5HeF2SZ37oh"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"qid,k,correct_value\", [\n",
    "    (\"q1\", 5, 0.799),\n",
    "    (\"q1\", 10, 0.799),\n",
    "    (\"q2\", 5, 0.549),\n",
    "    (\"q2\", 10, 0.705),\n",
    "    (\"q3\", 5, 0.908),\n",
    "    (\"q3\", 10, 0.949),\n",
    "])\n",
    "def test_queries(qid, k, correct_value):    \n",
    "    assert ndcg(system_rankings[qid], ground_truth[qid], k) == pytest.approx(correct_value, rel=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72bc613",
   "metadata": {
    "id": "TCtgThg9q0K8"
   },
   "source": [
    "# Student's paired sample t-test\n",
    "\n",
    "In this exercise you'll need to complete the code for computing the t statistic and p-value of a Student's paired sample t-test.\n",
    "\n",
    "$H_0: \\bar{x}_A = \\bar{x}_B$\n",
    "\n",
    "$H_1: \\bar{x}_A \\neq \\bar{x}_B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd822979",
   "metadata": {
    "id": "nW4i0zYNrT2k"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb92e2c2",
   "metadata": {
    "id": "8nuiMIvKxGZ2"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import numpy as np\n",
    "import pytest\n",
    "from scipy import stats\n",
    "from typing import List\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a212889",
   "metadata": {
    "id": "6xgkvXdGri_6"
   },
   "source": [
    "## t statistic\n",
    "\n",
    "$$t = \\frac{\\bar{x}_D}{\\frac{s_D}{\\sqrt[]{n}}}$$\n",
    "\n",
    "with $\\bar{x}_D$ and $s_D$ as the average and standard deviation of the differences between all pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8344ddb",
   "metadata": {
    "id": "F7CBqgWOTDFr"
   },
   "outputs": [],
   "source": [
    "def t_stat(a: List[float], b: List[float], n: int) -> float:\n",
    "    \"\"\"Computes the t statistic between two systems.\n",
    "    \n",
    "    Args:\n",
    "      a: System A recorded metric for each topic.\n",
    "      b: System B recorded metric for each topic.\n",
    "      n: Size of the sample.\n",
    "\n",
    "    Retuns:\n",
    "      t statistic for t-test between two systems.\n",
    "    \"\"\"\n",
    "    n = min(len(a), n)\n",
    "    x = np.array(a[:n]) - np.array(b[:n])\n",
    "\n",
    "    x_D = np.mean(x)\n",
    "    s_D = np.sqrt(sum((x-x_D)**2) / (n-1))\n",
    "\n",
    "    return x_D / (s_D/np.sqrt(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ecf8e2",
   "metadata": {
    "id": "W-K_S2_1sodD"
   },
   "source": [
    "# p-value\n",
    "\n",
    "$$\\text{p-value} = P(T(X^*) \\leq T(X_0) \\mid H_0) + P(T(X^*) \\geq T(X_0) \\mid H_0)$$\n",
    "\n",
    "Each probability composing the p-value can be computed using the cumulative distribution function (CDF) of the t-Student distribution. SciPy has an implementation of the [CDF](https://docs.scipy.org/doc/scipy-1.9.1/reference/generated/scipy.stats.t.html): `cdf(x, df, loc=0, scale=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48528b3",
   "metadata": {
    "id": "gzxGL-oCxZVM"
   },
   "outputs": [],
   "source": [
    "def p_value(n: int, t_stat: float) -> float:\n",
    "    \"\"\"Computes the p-value.\n",
    "    \n",
    "    Args:\n",
    "      n: Size of the sample.\n",
    "      t_stat: t statisitic.\n",
    "      \n",
    "    Returns:\n",
    "      p-value for t statistic.\n",
    "    \"\"\"\n",
    "    df = n - 1\n",
    "    p = (1.0 - stats.t.cdf(abs(t_stat), df)) * 2.0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe341bd",
   "metadata": {
    "id": "zvyIC2fntM8D"
   },
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0964fa",
   "metadata": {
    "id": "51wA7IT_txnq"
   },
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_lecture_example():\n",
    "    system_A = [0.2215, 0.3924, 0.654, 0.5611, 0.9186, 0.1104, 0.6086, 0.5062, 0.9688, 0.995]\n",
    "    system_B = [0.0765, 0.0426, 0.5738, 0.1571, 0.9881, 0.7164, 0.7507, 0.435, 0.3959, 0.8709]\n",
    "    n = len(system_A)\n",
    "    t = t_stat(system_A, system_B, n)\n",
    "    p = p_value(n, t)\n",
    "\n",
    "    assert t.round(3) == 0.897\n",
    "    assert p.round(3) == 0.393"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc91ae",
   "metadata": {
    "id": "_AslXmX2qc6Z"
   },
   "source": [
    "# Elasticsearch\n",
    "\n",
    "Run this example to index and search a toy-sized collection of documents using Elasticsearch.  There is nothing for you to add/complete here, it's just to make sure you're all set for the next exercise.\n",
    "\n",
    "Before starting, make sure that you've \n",
    "\n",
    "1. Downloaded and started Elasticsearch\n",
    "1. Installed the `elasticsearch` Python package\n",
    "  - It's part of the standard Anaconda distribution; otherwise, you can run `conda install elasticsearch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41366edd",
   "metadata": {
    "id": "HyB5tQ86qzam"
   },
   "outputs": [],
   "source": [
    "!pip install ipytest\n",
    "!pip install elasticsearch==7.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360dbb9",
   "metadata": {
    "id": "QC1FkvXwqzzF"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
    "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
    "tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
    "sudo chown -R daemon:daemon elasticsearch-7.9.2/\n",
    "shasum -a 512 -c elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05bc18",
   "metadata": {
    "id": "lpCjtqNiq1wF"
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "\n",
    "sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c1331",
   "metadata": {
    "id": "HeGThFJ8q3Cu"
   },
   "outputs": [],
   "source": [
    "# Sleep for few seconds to let the instance start.\n",
    "import time\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7aab9",
   "metadata": {
    "id": "TS00zc42qc6b"
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109608d",
   "metadata": {
    "id": "qRd49L3wqc6c"
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"toy_index\"  # the name of the index\n",
    "\n",
    "INDEX_SETTINGS = {  # single shard with a single replica\n",
    "    \"settings\" : {\n",
    "        \"index\" : {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 1\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38a43f",
   "metadata": {
    "id": "cNpYM-qbqc6c"
   },
   "source": [
    "The collection of documents is given here as a Python dictionary. Each document has two fields: title and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43334963",
   "metadata": {
    "id": "cPoVgDjFqc6c"
   },
   "outputs": [],
   "source": [
    "DOCS = {\n",
    "    1: {\"title\": \"Rap God\",\n",
    "        \"content\": \"gonna, gonna, Look, I was gonna go easy on you and not to hurt your feelings\"\n",
    "        },\n",
    "    2: {\"title\": \"Lose Yourself\",\n",
    "        \"content\": \"Yo, if you could just, for one minute Or one split second in time, forget everything Everything that bothers you, or your problems Everything, and follow me\"\n",
    "        },\n",
    "    3: {\"title\": \"Love The Way You Lie\",\n",
    "        \"content\": \"Just gonna stand there and watch me burn But that's alright, because I like the way it hurts\"\n",
    "        },\n",
    "    4: {\"title\": \"The Monster\",\n",
    "        \"content\": [\"gonna gonna I'm friends with the monster\", \"That's under my bed Get along with the voices inside of my head\"]\n",
    "        },\n",
    "    5: {\"title\": \"Beautiful\",\n",
    "        \"content\": \"Lately I've been hard to reach I've been too long on my own Everybody has a private world Where they can be alone\"\n",
    "        }\n",
    "}  # Eminem rulez ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438719f",
   "metadata": {
    "id": "LuGYRmCIqc6d"
   },
   "source": [
    "### Create Elasticsearch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e19b3",
   "metadata": {
    "id": "omodjTL8qc6d"
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b71681",
   "metadata": {
    "id": "oCa4eLctqc6d"
   },
   "source": [
    "Check if service is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d366d",
   "metadata": {
    "id": "wpoorIB-qc6e"
   },
   "outputs": [],
   "source": [
    "es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e4d10",
   "metadata": {
    "id": "2iko_4n4qc6f"
   },
   "source": [
    "### Create index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39005d43",
   "metadata": {
    "id": "0Fe58DYxqc6f"
   },
   "source": [
    "If the index exists, we delete it (normally, you don't want to do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69446932",
   "metadata": {
    "id": "D5Sh_6yjqc6f"
   },
   "outputs": [],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34827f",
   "metadata": {
    "id": "Fjj0-IWCqc6f"
   },
   "source": [
    "We set the number of shards and replicas to be used for each index when it's created. (We use a single shard instead of the default 5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e2deb",
   "metadata": {
    "id": "OQnoea6aqc6f"
   },
   "outputs": [],
   "source": [
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8489ed2",
   "metadata": {
    "id": "4BDBqSE2qc6g"
   },
   "source": [
    "### Add documents to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ff460",
   "metadata": {
    "id": "gsQnNiDIqc6g"
   },
   "outputs": [],
   "source": [
    "for doc_id, doc in DOCS.items():\n",
    "    es.index(index=INDEX_NAME, doc_type=\"_doc\", id=doc_id, body=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a73712",
   "metadata": {
    "id": "sblW5KvLqc6g"
   },
   "source": [
    "### Check what has been indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b03599",
   "metadata": {
    "id": "xF9F25a8qc6g"
   },
   "source": [
    "Get the contents of doc #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78e378",
   "metadata": {
    "id": "JUL5jgx1qc6g"
   },
   "outputs": [],
   "source": [
    "doc = es.get(index=INDEX_NAME, id=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a76227",
   "metadata": {
    "id": "9oLdfJkAqc6g"
   },
   "outputs": [],
   "source": [
    "pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8bf1b7",
   "metadata": {
    "id": "7v6KRp1cqc6h"
   },
   "source": [
    "Get the term vector for doc #3.\n",
    "\n",
    "`termvectors` returns information and statistics on terms in the fields of a particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e572b7",
   "metadata": {
    "id": "U1U_YWfoqc6h"
   },
   "outputs": [],
   "source": [
    "tv = es.termvectors(index=INDEX_NAME, doc_type=\"_doc\", id=3, fields=\"title,content\", term_statistics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303b66a",
   "metadata": {
    "id": "EqhyQ7WYqc6h"
   },
   "outputs": [],
   "source": [
    "pprint(tv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96022fa",
   "metadata": {
    "id": "7gxq-W-gqc6h"
   },
   "source": [
    "Interpretation of the returned values\n",
    "  * `[{field}]['field_statistics']`: \n",
    "    - `doc_count`: how many documents contain this field\n",
    "    - `sum_ttf`: the sum of all term frequencies in this field\n",
    "  * `[{field}][{term}]`:\n",
    "    - `doc_freq`: how many document contain this term\n",
    "    - `term_freq`: frequency (number of occurrences) of the term in this document field\n",
    "    - `ttf`: total term frequency, i.e., number of occurrences of the term in this field in all documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c87768b",
   "metadata": {
    "id": "WtCjTSv3qc6h"
   },
   "source": [
    "Note that Elasticsearch splits indices into multiple shards (by default: 5). This means that when you ask for term statistics, these are computed by shard. In case of a large collection, this is typically not an issue as the statistics become \"normalized\" across the different shards and the differences are negligible. For smaller collections that fit on a single disk, you may set the number of shards to 1 to avoid this issue alltogether (like we've done in this example in `INDEX_SETTINGS`).\n",
    "\n",
    "Check the following documents for further information:\n",
    "  - https://www.elastic.co/guide/en/elasticsearch/reference/6.2/_basic_concepts.html\n",
    "  - https://www.elastic.co/blog/practical-bm25-part-1-how-shards-affect-relevance-scoring-in-elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07dc2bf",
   "metadata": {
    "id": "O-YQ10U_qc6h"
   },
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfecc4c",
   "metadata": {
    "id": "FFyYnsjZqc6h"
   },
   "outputs": [],
   "source": [
    "query = \"rap monster\"\n",
    "res = es.search(index=INDEX_NAME, q=query, _source=False, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0942c69",
   "metadata": {
    "id": "dMpbU2LBqc6h"
   },
   "source": [
    "Print full response (`hits` holds the results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b5f27d",
   "metadata": {
    "id": "q5EKPfzlqc6i"
   },
   "outputs": [],
   "source": [
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbb5a8",
   "metadata": {
    "id": "ABmqU5oBqc6i"
   },
   "source": [
    "Print only search results (ranked list of docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c31321",
   "metadata": {
    "id": "uZsAHC5Uqc6i"
   },
   "outputs": [],
   "source": [
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(\"Doc ID: %3r  Score: %5.2f\" % (hit[\"_id\"], hit[\"_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6cdd2a",
   "metadata": {
    "id": "5VbXr_OVqc6i"
   },
   "source": [
    "## Elasticsearch query language\n",
    "\n",
    "Elasticsearch supports structured queries based on its own [DSL query language](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html).\n",
    "\n",
    "Mind that certain queries expect analyzed query terms (e.g., [term queries](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html)), while other query types (e.g., [match](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html)) perform analysis as part of the processing. Make sure you check the respective documentation carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce460b71",
   "metadata": {
    "id": "2WBz7o8_qc6i"
   },
   "source": [
    "### Building a second toy index with position information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c40a9",
   "metadata": {
    "id": "9FS6owHWqc6i"
   },
   "outputs": [],
   "source": [
    "INDEX_NAME2 = \"toy_index2\"  \n",
    "\n",
    "INDEX_SETTINGS2 = {\n",
    "    \"settings\" : {\n",
    "        \"index\" : {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 1\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"my_english_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"stopwords\": \"_english_\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\",\n",
    "                        \"filter_english_minimal\"\n",
    "                    ]                \n",
    "                }\n",
    "            },\n",
    "            \"filter\" : {\n",
    "                \"filter_english_minimal\" : {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"name\": \"minimal_english\"\n",
    "                },\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"term_vector\": \"with_positions\",\n",
    "                \"analyzer\": \"my_english_analyzer\"\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"term_vector\": \"with_positions\",\n",
    "                \"analyzer\": \"my_english_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec4eb1",
   "metadata": {
    "id": "8tIXQFsTqc6i"
   },
   "outputs": [],
   "source": [
    "if es.indices.exists(INDEX_NAME2):\n",
    "    es.indices.delete(index=INDEX_NAME2)\n",
    "    \n",
    "es.indices.create(index=INDEX_NAME2, body=INDEX_SETTINGS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c5546",
   "metadata": {
    "id": "8UriGNv4qc6i"
   },
   "outputs": [],
   "source": [
    "for doc_id, doc in DOCS.items():\n",
    "    es.index(index=INDEX_NAME2, doc_type=\"_doc\", id=doc_id, body=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a9e618",
   "metadata": {
    "id": "U1Gychahqc6j"
   },
   "source": [
    "Check that term position information has been added to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb937e",
   "metadata": {
    "id": "czHnWgNeqc6j"
   },
   "outputs": [],
   "source": [
    "tv = es.termvectors(index=INDEX_NAME2, doc_type=\"_doc\", id=3, fields=\"title\", term_statistics=True)\n",
    "\n",
    "pprint(tv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186a6f4",
   "metadata": {
    "id": "jJU29xkFqc6j"
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1ccca",
   "metadata": {
    "id": "jd46oNFwqc6j"
   },
   "source": [
    "Searching for documents that must match a [boolean combination](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html) of multiple terms (in any order).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbfcbe8",
   "metadata": {
    "id": "Dl5nBmbMqc6j"
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"bool\": {\n",
    "        \"must\": [\n",
    "            {\"match\": {\"content\": \"gonna\"}}, \n",
    "            {\"match\": {\"content\": \"monster\"}}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "res = es.search(index=INDEX_NAME2, body={\"query\": query})\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631d2bf",
   "metadata": {
    "id": "d_VDo-N9qc6j"
   },
   "source": [
    "Searching for documents that match an [extract phrase](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query-phrase.html) (terms in that exact order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69371a30",
   "metadata": {
    "id": "qq5-bo7Sqc6j"
   },
   "outputs": [],
   "source": [
    "query = {\"match_phrase\": {\"content\": \"split second\"}}\n",
    "\n",
    "res = es.search(index=INDEX_NAME2, body={'query': query})\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41843207",
   "metadata": {
    "id": "NjQ2Z6T7rhpE"
   },
   "source": [
    "# Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5d04b",
   "metadata": {
    "id": "AzXIOfu1rhpF"
   },
   "source": [
    "In this exercise, you'll first build an Elasticsearch index of a toy document collection, then request various term statistics from that index.\n",
    "\n",
    "Remember to make sure that the Elasticsearch service is running (i.e., has been started in a terminal window).\n",
    "\n",
    "See [this document](Elasticsearch.md) for help on Elasticsearch usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5e333",
   "metadata": {
    "id": "tKM5DuCDrvrj"
   },
   "outputs": [],
   "source": [
    "!pip install ipytest\n",
    "!pip install elasticsearch==7.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd9057",
   "metadata": {
    "id": "_bdm5vChrxVS"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
    "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
    "tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
    "sudo chown -R daemon:daemon elasticsearch-7.9.2/\n",
    "shasum -a 512 -c elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b00be",
   "metadata": {
    "id": "xt3kii7Try4B"
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "\n",
    "sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff75bb3",
   "metadata": {
    "id": "r2Y9jtG8r0y3"
   },
   "outputs": [],
   "source": [
    "# Sleep for few seconds to let the instance start.\n",
    "import time\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9270d56",
   "metadata": {
    "id": "ri-oeCIMrhpF"
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4a024",
   "metadata": {
    "id": "gaaY3tArrhpG"
   },
   "source": [
    "This is to check that the Elasticsearch service is running on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6068f",
   "metadata": {
    "id": "oc7m47NLrhpG"
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4aed4",
   "metadata": {
    "id": "wgvnAB4qrhpG"
   },
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a707ae",
   "metadata": {
    "id": "9VW5plmirhpH"
   },
   "source": [
    "We use a toy data collection with 5 documents, each with title and content fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f5db1",
   "metadata": {
    "id": "yVdSF66LrhpH"
   },
   "outputs": [],
   "source": [
    "DOCS = [\n",
    "    {\"doc_id\": \"D1\",\n",
    "     \"title\": \"First document\",\n",
    "     \"content\": \"House on the hill\"\n",
    "    },\n",
    "    {\"doc_id\": \"D2\",\n",
    "     \"title\": \"Second title\",\n",
    "     \"content\": \"Downtown Stavanger is beautiful\"\n",
    "    },\n",
    "    {\"doc_id\": \"D3\",\n",
    "     \"title\": \"First, second, and third\",\n",
    "     \"content\": \"Never step on snakes\"\n",
    "    },\n",
    "    {\"doc_id\": \"D4\",\n",
    "     \"title\": \"Document number four\",\n",
    "     \"content\": \"House, house. It's a beautiful house you have\"\n",
    "    },\n",
    "    {\"doc_id\": \"D5\",\n",
    "     \"title\": \"This document is the last document\",\n",
    "     \"content\": \"There can be only one matching result\"\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e8728",
   "metadata": {
    "id": "ER2FcTewrhpI"
   },
   "outputs": [],
   "source": [
    "INDEX_SETTINGS = {  # single shard with a single replica\n",
    "    \"settings\" : {\n",
    "        \"index\" : {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 1\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1252c",
   "metadata": {
    "id": "QGBlWtAnrhpI"
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"test_e6-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f98a3e",
   "metadata": {
    "id": "nmYgy1xCrhpI"
   },
   "outputs": [],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e638b",
   "metadata": {
    "id": "j3vWlp87rhpJ"
   },
   "source": [
    "Add documents in `DOC` to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a394536",
   "metadata": {
    "id": "ruSc-NlarhpJ"
   },
   "outputs": [],
   "source": [
    "for doc in DOCS:\n",
    "    es.index(index=INDEX_NAME, doc_type=\"_doc\", id=doc[\"doc_id\"],\n",
    "             body={\"title\": doc[\"title\"], \"content\": doc[\"content\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4648f5c4",
   "metadata": {
    "id": "dv7oxIdvrhpJ"
   },
   "source": [
    "## Term statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6bd7d",
   "metadata": {
    "id": "50xQChnqrhpJ"
   },
   "source": [
    "Complete the methods below for getting various term statistics from the index.\n",
    "\n",
    "Consult [this notebook](2-Elasticsearch.ipynb) for the interpretation of term vector statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7bc48",
   "metadata": {
    "id": "9fnFQtmbrhpJ"
   },
   "outputs": [],
   "source": [
    "def get_doc_term_freqs(index_name: str, doc_id: str, field: str) -> Dict[str, int]:\n",
    "    \"\"\"Returns the terms along with their frequencies contained in a given document.\n",
    "    \n",
    "    Args:\n",
    "        index_name: Name of index.\n",
    "        doc_id: Document ID.\n",
    "        field: Field name.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with terms as keys and corresponding frequencies (i.e., \n",
    "        number of occurrences within the given document field) as values.\n",
    "    \"\"\"\n",
    "    tv = es.termvectors(index=index_name, doc_type=\"_doc\", id=doc_id, fields=field, term_statistics=True)\n",
    "    if tv[\"_id\"] != doc_id:\n",
    "        return None\n",
    "    if field not in tv[\"term_vectors\"]:\n",
    "        return None\n",
    "    term_freqs = {}\n",
    "    for term, term_stat in tv[\"term_vectors\"][field][\"terms\"].items():\n",
    "        term_freqs[term] = term_stat[\"term_freq\"]\n",
    "    return term_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4f0df",
   "metadata": {
    "id": "H_ufllIHrhpK"
   },
   "outputs": [],
   "source": [
    "def get_doc_field_len(index_name: str, doc_id: str, field: str) -> int:\n",
    "    \"\"\"Returns the length of a given document field.\n",
    "    \n",
    "    Length is defined as the total number of terms contained in that field.\n",
    "    \n",
    "    Args:\n",
    "        index_name: Name of index.\n",
    "        doc_id: Document ID.\n",
    "        field: Field name.\n",
    "    \n",
    "    Returns:\n",
    "        Field length.    \n",
    "    \"\"\"\n",
    "    term_freqs = get_doc_term_freqs(index_name, doc_id, field)\n",
    "    if term_freqs is not None:\n",
    "        return sum(term_freqs.values())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef8290",
   "metadata": {
    "id": "MFpmCM9krhpK"
   },
   "outputs": [],
   "source": [
    "def get_doc_containing_term(index_name: str, field: str, term: str) -> Optional[str]:\n",
    "    \"\"\"Returns any document ID that contains term in a given field or None.\n",
    "    \n",
    "    Args:\n",
    "        index_name: Name of index.\n",
    "        field: Field name.\n",
    "        term: Term.\n",
    "\n",
    "    Returns:\n",
    "        ID of a document that contains `term` or None.\n",
    "    \"\"\"\n",
    "    # Use a boolean query to find a document that contains the term.\n",
    "    hits = es.search(index=index_name, body={\"query\": {\"match\": {field: term}}},\n",
    "                               _source=False, size=1).get(\"hits\", {}).get(\"hits\", {})\n",
    "    return hits[0][\"_id\"] if len(hits) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb4b14",
   "metadata": {
    "id": "ofzqbRROrhpL"
   },
   "outputs": [],
   "source": [
    "def get_term_doc_count(index_name: str, field: str, term: str) -> int:\n",
    "    \"\"\"Returns the total number of documents that contain a given term within a specific field.\n",
    "    \n",
    "    Args:\n",
    "        index_name: Name of index.\n",
    "        field: Field name.\n",
    "        term: Term.\n",
    "        \n",
    "    Returns:\n",
    "        Number of documents that contain the given term within `field`.\n",
    "    \"\"\"\n",
    "    # Find a document that contains the term.\n",
    "    doc_id = get_doc_containing_term(index_name, field, term)\n",
    "    if doc_id is None:\n",
    "        return 0\n",
    "    # Request term statistics for that document and extract the \n",
    "    # requested information from there.\n",
    "    tv = es.termvectors(index=index_name, doc_type=\"_doc\", id=doc_id, fields=field, term_statistics=True)\n",
    "    return tv[\"term_vectors\"][field][\"terms\"][term][\"doc_freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c8390",
   "metadata": {
    "id": "nD9xpGS7rhpL"
   },
   "outputs": [],
   "source": [
    "def get_term_coll_freq(index_name: str, field: str, term: str) -> int:\n",
    "    \"\"\"Returns the total collection term frequency of a term in a given field.\n",
    "    \n",
    "    Args:\n",
    "        index_name: Name of index.\n",
    "        field: Field name.\n",
    "        term: Term.\n",
    "        \n",
    "    Returns:\n",
    "        Total number of occurrences of `term` in all documents within `field`.\n",
    "    \"\"\"\n",
    "    # Find a document that contains the term.\n",
    "    doc_id = get_doc_containing_term(index_name, field, term)\n",
    "    if doc_id is None:\n",
    "        return 0\n",
    "    # Request term statistics for that document and extract the \n",
    "    # requested information from there.\n",
    "    tv = es.termvectors(index=index_name, doc_type=\"_doc\", id=doc_id, fields=field, term_statistics=True)\n",
    "    return tv[\"term_vectors\"][field][\"terms\"][term][\"ttf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d4459",
   "metadata": {
    "id": "xO4Kqu3irhpL"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734349cd",
   "metadata": {
    "id": "o1ATpy6lrhpL"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_doc_term_freqs():\n",
    "    assert get_doc_term_freqs(INDEX_NAME, \"D2\", \"title\") == {\"second\": 1, \"title\": 1}\n",
    "    assert get_doc_term_freqs(INDEX_NAME, \"D4\", \"content\") == {\"a\": 1, \"beautiful\": 1, \"have\": 1,\n",
    "                                                               \"house\": 3, \"it's\": 1, \"you\": 1}    \n",
    "def test_doc_field_len():\n",
    "    assert get_doc_field_len(INDEX_NAME, \"D2\", \"title\") == 2\n",
    "    assert get_doc_field_len(INDEX_NAME, \"D4\", \"content\") == 8\n",
    "    \n",
    "def test_doc_containing_term():\n",
    "    assert get_doc_containing_term(INDEX_NAME, \"title\", \"document\") in [\"D1\", \"D4\", \"D5\"]\n",
    "    assert get_doc_containing_term(INDEX_NAME, \"content\", \"house\") in [\"D1\", \"D4\"]\n",
    "    \n",
    "def test_term_doc_count():\n",
    "    assert get_term_doc_count(INDEX_NAME, \"title\", \"document\") == 3\n",
    "    assert get_term_doc_count(INDEX_NAME, \"content\", \"house\") == 2    \n",
    "    \n",
    "def test_term_coll_freq():\n",
    "    assert get_term_coll_freq(INDEX_NAME, \"title\", \"this\") == 1\n",
    "    assert get_term_coll_freq(INDEX_NAME, \"title\", \"document\") == 4\n",
    "    assert get_term_coll_freq(INDEX_NAME, \"content\", \"house\") == 4       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642fec3",
   "metadata": {
    "id": "yX2mdTxJpru9"
   },
   "source": [
    "# Trivia quiz using DBpedia\n",
    "\n",
    "In this exercise, you'll have to answer some trivia questions with the help of DBpedia.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887197d",
   "metadata": {
    "id": "M_Sxf1lApv04"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0dc0e4",
   "metadata": {
    "id": "nvvbHjUOpru-"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Tuple\n",
    "\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d00c7",
   "metadata": {
    "id": "Dw2zfapNpru_"
   },
   "source": [
    "Utility method for fetching all triples associated with a given entity from DBpedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6430b",
   "metadata": {
    "id": "8jjpKQtBprvA"
   },
   "outputs": [],
   "source": [
    "def get_triples(entity_id: str) -> Tuple[str, str, Any]:\n",
    "    \"\"\"Returns all triples from DBpedia associated with a given entity.\n",
    "        \n",
    "    Args:\n",
    "        entity_id: Unique entity identifier (e.g., \"Kimi_Räikkönen\")\n",
    "    \n",
    "    Yields:\n",
    "        (subject, predicate, object) triples where the given entity\n",
    "        stands either as subject or as object.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If invalid or not the canonical entity ID is provided.\n",
    "    \"\"\"\n",
    "    # Fetch DBpedia entry in JSON format.\n",
    "    # <SPO> triples are represented in the JSON as {S: P: [O1, O2, ...]},\n",
    "    # i.e., multi-valued predicates are grouped together.\n",
    "    data = requests.get(f\"http://dbpedia.org/data/{entity_id}.json\").json()\n",
    "    \n",
    "    # Check whether valid entity ID is provided.\n",
    "    entity_uri = f\"http://dbpedia.org/resource/{entity_id}\"\n",
    "    if entity_uri not in data:\n",
    "        raise ValueError(\"Invalid entity ID.\")\n",
    "        \n",
    "    # Check whether this is the canonical entity ID.\n",
    "    if \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" not in data[entity_uri]:\n",
    "        raise ValueError(\"Not the canonical entity ID.\")\n",
    "\n",
    "    for subj, vals in data.items():                \n",
    "        for pred, objs in vals.items():\n",
    "            for obj in objs:\n",
    "                yield subj, pred, obj[\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949026d",
   "metadata": {
    "id": "n5g6hNdxprvG"
   },
   "source": [
    "## Example\n",
    "\n",
    "Compare with the information at https://dbpedia.org/page/Kimi_R%C3%A4ikk%C3%B6nen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6c97f",
   "metadata": {
    "id": "LPLXtKbXprvH"
   },
   "outputs": [],
   "source": [
    "triples_kimi = [(s, p, o) for s, p, o in get_triples(\"Kimi_Räikkönen\")]\n",
    "\n",
    "for s, p, o in triples_kimi:\n",
    "    print(s, p, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484a4eb",
   "metadata": {
    "id": "k2Sbl87VprvJ"
   },
   "source": [
    "Types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca697b81",
   "metadata": {
    "id": "ZKFMGHeTprvJ"
   },
   "outputs": [],
   "source": [
    "for s, p, o in triples_kimi:\n",
    "    if s != \"http://dbpedia.org/resource/Kimi_Räikkönen\":\n",
    "        continue\n",
    "    if p == \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\":\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b9cd0",
   "metadata": {
    "id": "efkFiJuoprvK"
   },
   "source": [
    "Birth place and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da745a",
   "metadata": {
    "id": "TRTJKWJ6prvL"
   },
   "outputs": [],
   "source": [
    "for s, p, o in triples_kimi:\n",
    "    if s != \"http://dbpedia.org/resource/Kimi_Räikkönen\":\n",
    "        continue\n",
    "    if \"birthPlace\" in p or \"birthDate\" in p:\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3569d6",
   "metadata": {
    "id": "RQpkNyGVprvM"
   },
   "source": [
    "## Quiz questions\n",
    "\n",
    "Answer the questions below.\n",
    "\n",
    "You're allowed to find the exact answers either manually or programmatically, as long as you get the underlying data programmatically using the provided `fetch_entity()` method.\n",
    "\n",
    "Since tests for this exercise would reveal the correct answer, they are based on MD5 hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d638f86",
   "metadata": {
    "id": "9vAb4S0bprvM"
   },
   "source": [
    "### 1) How many Formula 1 Grand Prixes has Kimi Räikkönen won?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50df040",
   "metadata": {
    "id": "I0chq__UprvM"
   },
   "outputs": [],
   "source": [
    "wins = [s for s, p, o in triples_kimi if \"ontology/firstDriver\" in p and \"Grand_Prix\" in s]\n",
    "num_wins = len(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83907f85",
   "metadata": {
    "id": "PsUDQlAFprvN"
   },
   "outputs": [],
   "source": [
    "assert hashlib.md5(np.int8(num_wins)).hexdigest() == \"f5a7e477cd3042b49a9085d62307cd28\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7712e18",
   "metadata": {
    "id": "X9zCgA2vprvN"
   },
   "source": [
    "### 2) Is Kimi Räikkönen married?\n",
    "\n",
    "Return the string \"Y\" or \"N\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977eda95",
   "metadata": {
    "id": "oHfcdYsVprvN"
   },
   "outputs": [],
   "source": [
    "married = \"N\"\n",
    "for s, p, o in triples_kimi:\n",
    "    if \"ontology/spouse\" in p:\n",
    "        married = \"Y\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95b32e",
   "metadata": {
    "id": "NnGNjYgSprvO"
   },
   "outputs": [],
   "source": [
    "assert hashlib.md5(bytes(married, \"utf-8\")).hexdigest() == \"57cec4137b614c87cb4e24a3d003a3e0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05f97d",
   "metadata": {
    "id": "1T_wRO98prvO"
   },
   "source": [
    "### 3) In which country did Kimi win his first Grand Prix?\n",
    "\n",
    "Return the canonical name of the country as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aabf6c",
   "metadata": {
    "id": "xtX3t_NjprvO"
   },
   "source": [
    "Utility functions to help answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4b83f",
   "metadata": {
    "id": "nONjIB0hprvO"
   },
   "outputs": [],
   "source": [
    "def get_entity_id(uri: str) -> str:\n",
    "    \"\"\"Return the entity_id part of an URI (part after the last slash).\"\"\"\n",
    "    return uri.split(\"/\")[-1]\n",
    "\n",
    "def is_country(uri: str) -> bool:\n",
    "    \"\"\"Determines if a given URI is a country.\"\"\"\n",
    "    for _, p, o in get_triples(get_entity_id(uri)):\n",
    "        if p != \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\":\n",
    "            continue\n",
    "        if o == \"http://dbpedia.org/ontology/Country\":\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faffc180",
   "metadata": {
    "id": "XabCopoBprvO"
   },
   "outputs": [],
   "source": [
    "# Get first win.\n",
    "first_win = None\n",
    "for s, p, o in triples_kimi:\n",
    "    if \"ontology/firstWin\" in p:\n",
    "        first_win = get_entity_id(o)  # Keep only entity_id, not the whole URI.\n",
    "        break\n",
    "\n",
    "# Determine the location of that GP.\n",
    "country = None\n",
    "for _, p, o in get_triples(first_win):\n",
    "    if \"ontology/location\" in p:\n",
    "        if is_country(o):\n",
    "            country = get_entity_id(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b48122",
   "metadata": {
    "id": "sDd4WSidprvO"
   },
   "outputs": [],
   "source": [
    "assert hashlib.md5(bytes(country, \"utf-8\")).hexdigest() == \"3f0e49c46cbde0c7adf5ea04a97ab261\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1152a6b",
   "metadata": {
    "id": "HWQgIBxfXyzM"
   },
   "source": [
    "# Bigram matches in Elasticsearch\n",
    "\n",
    "This exercise is about getting ordered and unordered bigram matches using Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd691775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4P727p0YvXo",
    "outputId": "109bfac6-9b56-4440-de69-b81c3b61f053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ipytest\n",
      "  Downloading ipytest-0.12.0-py3-none-any.whl (15 kB)\n",
      "Collecting pytest>=5.4\n",
      "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 4.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.11.0)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.12.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.1)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 32.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n",
      "Installing collected packages: pluggy, jedi, iniconfig, pytest, ipytest\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 0.7.1\n",
      "    Uninstalling pluggy-0.7.1:\n",
      "      Successfully uninstalled pluggy-0.7.1\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 3.6.4\n",
      "    Uninstalling pytest-3.6.4:\n",
      "      Successfully uninstalled pytest-3.6.4\n",
      "Successfully installed iniconfig-1.1.1 ipytest-0.12.0 jedi-0.18.1 pluggy-1.0.0 pytest-7.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86f307",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UF8_be86YyBJ",
    "outputId": "6ee9fcd6-5a66-4739-ebba-062f65c530dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting elasticsearch==7.9.0\n",
      "  Downloading elasticsearch-7.9.0-py2.py3-none-any.whl (213 kB)\n",
      "\u001b[K     |████████████████████████████████| 213 kB 4.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch==7.9.0) (1.24.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch==7.9.0) (2022.6.15)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch==7.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629452c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fl8ghWrbdrTJ",
    "outputId": "dc3efebd-2b96-43ee-a6cc-0049192d77dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticsearch-oss-7.9.2-linux-x86_64.tar.gz: OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
    "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
    "tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
    "sudo chown -R daemon:daemon elasticsearch-7.9.2/\n",
    "shasum -a 512 -c elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8747d",
   "metadata": {
    "id": "1eP24fq7dwpA"
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "\n",
    "sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507a5f9",
   "metadata": {
    "id": "twQRaoCedyYW"
   },
   "outputs": [],
   "source": [
    "# Sleep for few seconds to let the instance start.\n",
    "import time\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40641e4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlUnRTAFd4uX",
    "outputId": "f2fb58f6-237f-44d1-a8d3-625dabe5f130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root         168     166  0 08:42 ?        00:00:00 sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch\n",
      "daemon       169     168 99 08:42 ?        00:00:20 /content/elasticsearch-7.9.2/jdk/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -XX:+ShowCodeDetailsInExceptionMessages -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.locale.providers=SPI,COMPAT -Xms1g -Xmx1g -XX:+UseG1GC -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -Djava.io.tmpdir=/tmp/elasticsearch-7080371589566594553 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m -XX:MaxDirectMemorySize=536870912 -Des.path.home=/content/elasticsearch-7.9.2 -Des.path.conf=/content/elasticsearch-7.9.2/config -Des.distribution.flavor=oss -Des.distribution.type=tar -Des.bundled_jdk=true -cp /content/elasticsearch-7.9.2/lib/* org.elasticsearch.bootstrap.Elasticsearch\n",
      "root         413     411  0 08:43 ?        00:00:00 grep elasticsearch\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ps -ef | grep elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c831be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oBRxkdpd64Q",
    "outputId": "5d80d8e7-f8f5-4894-b1c5-35898a7ab21a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"449c557fdc0b\",\n",
      "  \"cluster_name\" : \"elasticsearch\",\n",
      "  \"cluster_uuid\" : \"VkbUjcNeQ7-fcKUWfXBlXA\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"7.9.2\",\n",
      "    \"build_flavor\" : \"oss\",\n",
      "    \"build_type\" : \"tar\",\n",
      "    \"build_hash\" : \"d34da0ea4a966c4e49417f2da2f244e3e97b4e6e\",\n",
      "    \"build_date\" : \"2020-09-23T00:45:33.626720Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"8.6.2\",\n",
      "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -sX GET \"localhost:9200/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5aa1d",
   "metadata": {
    "id": "LR5PftFXXyzN"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from elasticsearch import Elasticsearch\n",
    "from pprint import pprint\n",
    "\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c30a5",
   "metadata": {
    "id": "UGHBzBQNXyzN"
   },
   "source": [
    "## Indexing a toy collection \n",
    "\n",
    "This time, we store **term position information** and perform minimal stemming, i.e., removing only plurals (for that, we specify a custom analyzer).\n",
    "\n",
    "Check the [Elasticsearch documentation on analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039e425",
   "metadata": {
    "id": "AvETQfFnXyzN"
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"toy_index\"  \n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    'settings' : {\n",
    "        'index' : {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 1\n",
    "        },\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'my_english_analyzer': {\n",
    "                    'type': \"custom\",\n",
    "                    'tokenizer': \"standard\",\n",
    "                    'stopwords': \"_english_\",\n",
    "                    'filter': [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\",\n",
    "                        \"filter_english_minimal\"\n",
    "                    ]                \n",
    "                }\n",
    "            },\n",
    "            'filter' : {\n",
    "                'filter_english_minimal' : {\n",
    "                    'type': \"stemmer\",\n",
    "                    'name': \"minimal_english\"\n",
    "                },\n",
    "                'english_stop': {\n",
    "                    'type': \"stop\",\n",
    "                    'stopwords': \"_english_\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'title': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            },\n",
    "            'content': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ec528",
   "metadata": {
    "id": "nQvCcwDrXyzO"
   },
   "outputs": [],
   "source": [
    "DOCS = {\n",
    "    1: {\"title\": \"Rap God\",\n",
    "        \"content\": \"gonna, gonna, Look, I was gonna go easy on you and not to hurt your feelings\"\n",
    "        },\n",
    "    2: {\"title\": \"Lose Yourself\",\n",
    "        \"content\": \"Yo, if you could just, for one minute Or one split second in time, forget everything Everything that bothers you, or your problems Everything, and follow me\"\n",
    "        },\n",
    "    3: {\"title\": \"Love The Way You Lie\",\n",
    "        \"content\": \"Just gonna stand there and watch me burn But that's alright, because I like the way it hurts\"\n",
    "        },\n",
    "    4: {\"title\": \"The Monster\",\n",
    "        \"content\": [\"gonna gonna I'm friends with the monster\", \"That's under my bed Get along with the voices inside of my head\"]\n",
    "        },\n",
    "    5: {\"title\": \"Beautiful\",\n",
    "        \"content\": \"Lately I've been hard to reach I've been too long on my own Everybody has a private world Where they can be alone\"\n",
    "        },\n",
    "    6: {\"title\": \"Fake Eminem 1\",\n",
    "        \"content\": \"This is not real Eminem, just some text to get more matches for a split second for a split second.\"\n",
    "        },\n",
    "    7: {\"title\": \"Fake Eminem 2\",\n",
    "        \"content\": \"I have a monster friend and I'm friends with the monster and then there are some more friends who are monsters.\"\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d3edd",
   "metadata": {
    "id": "kfPB4wdyXyzO"
   },
   "outputs": [],
   "source": [
    "ES_NODES = \"http://localhost:9200\"\n",
    "es = Elasticsearch(hosts = [ES_NODES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b61647",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozCr_Z1ehg23",
    "outputId": "193b42dd-89fe-4cb1-a1bf-7bf348651114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '449c557fdc0b',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'VkbUjcNeQ7-fcKUWfXBlXA',\n",
       " 'version': {'number': '7.9.2',\n",
       "  'build_flavor': 'oss',\n",
       "  'build_type': 'tar',\n",
       "  'build_hash': 'd34da0ea4a966c4e49417f2da2f244e3e97b4e6e',\n",
       "  'build_date': '2020-09-23T00:45:33.626720Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.6.2',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82bd67",
   "metadata": {
    "id": "ZgYzlrcrXyzO"
   },
   "outputs": [],
   "source": [
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1423f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILV1N_6aer2d",
    "outputId": "5cd4aabd-fc7c-4017-ef0e-6ed75679b4f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'toy_index'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c9d94d",
   "metadata": {
    "id": "KHHE6EqbXyzP"
   },
   "source": [
    "Testing our analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7f100",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmb5UUaOXyzP",
    "outputId": "f4683506-4e57-4303-f28a-cea8f07b7efc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'token': 'monster',\n",
       "   'start_offset': 0,\n",
       "   'end_offset': 8,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 0},\n",
       "  {'token': 'my',\n",
       "   'start_offset': 12,\n",
       "   'end_offset': 14,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 2},\n",
       "  {'token': 'bed',\n",
       "   'start_offset': 15,\n",
       "   'end_offset': 18,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 3}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.analyze(index=INDEX_NAME, body={'analyzer': 'my_english_analyzer', 'text': 'monsters in my bed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685fe6c",
   "metadata": {
    "id": "GaXc0R7pXyzP"
   },
   "outputs": [],
   "source": [
    "for doc_id, doc in DOCS.items():\n",
    "    es.index(index=INDEX_NAME, id=doc_id, body=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a52f19",
   "metadata": {
    "id": "u-kQL4WpXyzP"
   },
   "source": [
    "Notice that you also get term position information when requesting a term vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999ae24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQJXKCQRXyzP",
    "outputId": "b769520c-c548-4085-87ab-be73069dccd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '2',\n",
      " '_index': 'toy_index',\n",
      " '_type': '_doc',\n",
      " '_version': 1,\n",
      " 'found': True,\n",
      " 'term_vectors': {'content': {'field_statistics': {'doc_count': 7,\n",
      "                                                   'sum_doc_freq': 85,\n",
      "                                                   'sum_ttf': 101},\n",
      "                              'terms': {'bother': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 18}]},\n",
      "                                        'could': {'term_freq': 1,\n",
      "                                                  'tokens': [{'position': 3}]},\n",
      "                                        'everything': {'term_freq': 3,\n",
      "                                                       'tokens': [{'position': 15},\n",
      "                                                                  {'position': 16},\n",
      "                                                                  {'position': 23}]},\n",
      "                                        'follow': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 25}]},\n",
      "                                        'forget': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 14}]},\n",
      "                                        'just': {'term_freq': 1,\n",
      "                                                 'tokens': [{'position': 4}]},\n",
      "                                        'me': {'term_freq': 1,\n",
      "                                               'tokens': [{'position': 26}]},\n",
      "                                        'minute': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 7}]},\n",
      "                                        'one': {'term_freq': 2,\n",
      "                                                'tokens': [{'position': 6},\n",
      "                                                           {'position': 9}]},\n",
      "                                        'problem': {'term_freq': 1,\n",
      "                                                    'tokens': [{'position': 22}]},\n",
      "                                        'second': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 11}]},\n",
      "                                        'split': {'term_freq': 1,\n",
      "                                                  'tokens': [{'position': 10}]},\n",
      "                                        'time': {'term_freq': 1,\n",
      "                                                 'tokens': [{'position': 13}]},\n",
      "                                        'yo': {'term_freq': 1,\n",
      "                                               'tokens': [{'position': 0}]},\n",
      "                                        'you': {'term_freq': 2,\n",
      "                                                'tokens': [{'position': 2},\n",
      "                                                           {'position': 19}]},\n",
      "                                        'your': {'term_freq': 1,\n",
      "                                                 'tokens': [{'position': 21}]}}},\n",
      "                  'title': {'field_statistics': {'doc_count': 7,\n",
      "                                                 'sum_doc_freq': 16,\n",
      "                                                 'sum_ttf': 16},\n",
      "                            'terms': {'lose': {'term_freq': 1,\n",
      "                                               'tokens': [{'position': 0}]},\n",
      "                                      'yourself': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 1}]}}}},\n",
      " 'took': 90}\n"
     ]
    }
   ],
   "source": [
    "tv = es.termvectors(index=INDEX_NAME, id=2, fields='title,content')\n",
    "pprint(tv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62f4ed",
   "metadata": {
    "id": "aaCY3YwyXyzP"
   },
   "source": [
    "## Recovering ordered sequence of terms from inverted index\n",
    "\n",
    "This method returns the sequence of terms for a given document field, with `None` values for stopwords that got removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127cc9e5",
   "metadata": {
    "id": "zr0L-mPHXyzQ"
   },
   "outputs": [],
   "source": [
    "def get_term_sequence(es, doc_id, field):\n",
    "    tv = es.termvectors(index=INDEX_NAME, id=doc_id, fields=[field])\n",
    "    # We first put terms in a position-indexed dict.\n",
    "    pos = {}\n",
    "    for term, tinfo in tv['term_vectors'][field]['terms'].items():\n",
    "        for token in tinfo['tokens']:\n",
    "            pos[token['position']] = term\n",
    "    # Then, turn that dict to a list.\n",
    "    seq = [None] * (max(pos.keys()) + 1)\n",
    "    for p, term in pos.items():\n",
    "        seq[p] = term\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c469070",
   "metadata": {
    "id": "tEV_0i6YXyzQ"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b60c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwWlKt5nXyzQ",
    "outputId": "88b3d0de-55db-4036-e625-b6bf38d04de4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_get_term_sequence():\n",
    "    assert get_term_sequence(es, 4, 'title') == [None, 'monster']\n",
    "    assert get_term_sequence(es, 7, 'content') == ['i', 'have', None, 'monster', 'friend', None, \"i'm\", 'friend', None, None, 'monster', None, None, None, None, 'some', 'more', 'friend', 'who', None, 'monster']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73a5ec",
   "metadata": {
    "id": "o_nG_1EUXyzQ"
   },
   "source": [
    "## Getting ordered bigram matches\n",
    "\n",
    "Use the `get_term_sequence()` method to get the document field's content as a sequence of terms, then check for ordered bigram matches yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfce28",
   "metadata": {
    "id": "c7Q20RIdXyzQ"
   },
   "outputs": [],
   "source": [
    "def count_ordered_bigram_matches(es, doc_id, field, bigram):\n",
    "    \"\"\"Counts the number of ordered bigram matches in a given document field. \n",
    "    \n",
    "    Args:\n",
    "        es: Elasticsearch instance\n",
    "        doc_id: Document ID\n",
    "        field: Document field\n",
    "        bigram: A sequence of two terms given as a list\n",
    "    \n",
    "    Returns:\n",
    "        Number of times the bigram can be found in this exact order.\n",
    "    \"\"\"\n",
    "    # Get the document field's content as a sequence of terms.\n",
    "    text = get_term_sequence(es, doc_id, field)\n",
    "    # Count the number of matches    \n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == bigram[0]:\n",
    "            if text[i + 1] == bigram[1]:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa2252",
   "metadata": {
    "id": "eko0CqLfXyzQ"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89cbcf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3AfABhmXyzQ",
    "outputId": "18221a77-4f98-4c2f-cfac-3d2163178968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize('doc_id, field, bigram, correct_value', [\n",
    "    (6, 'content', ['split', 'second'], 2),\n",
    "    (2, 'content', ['split', 'second'], 1),\n",
    "    (1, 'content', ['split', 'second'], 0),\n",
    "])\n",
    "def test_count_ordered_bigram_matches(doc_id, field, bigram, correct_value):\n",
    "    assert count_ordered_bigram_matches(es, doc_id, field, bigram) == correct_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265cb4d",
   "metadata": {
    "id": "AiFiq-9JXyzQ"
   },
   "source": [
    "## Getting unordered bigram matches\n",
    "\n",
    "As before, use the `get_term_sequence()` method to get the document field's content as a sequence of terms, then check for ordered bigram matches yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74201fc",
   "metadata": {
    "id": "X0NKDqCpXyzQ"
   },
   "outputs": [],
   "source": [
    "def count_unordered_bigram_matches(es, doc_id, field, bigram, w=4):\n",
    "    \"\"\"Counts the number of unordered bigram matches in a given document field. \n",
    "    \n",
    "    Args:\n",
    "        es: Elasticsearch instance\n",
    "        doc_id: Document ID\n",
    "        field: Document field\n",
    "        bigram: A sequence of two terms given as a list\n",
    "        w: The maximum distance between the two query terms that still counts as a match\n",
    "    \n",
    "    Returns:\n",
    "        Number of times the bigram can be found within a distance of w from each other in any order.\n",
    "    \"\"\"\n",
    "    text = get_term_sequence(es, doc_id, \"content\")\n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] in bigram:\n",
    "            other_term = bigram[0] if text[i] == bigram[1] else bigram[1]\n",
    "            count += Counter(text[i+1:i+w])[other_term]\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c0eb50",
   "metadata": {
    "id": "RKnWleAUXyzR"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35de648",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-2SygNxXyzR",
    "outputId": "9a9b3f48-d190-4092-89fb-08a516ec9528"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize('doc_id, field, bigram, correct_value', [\n",
    "    (7, 'title', ['friend', 'monster'], 3),\n",
    "    (4, 'title', ['friend', 'monster'], 1),\n",
    "    (1, 'title', ['friend', 'monster'], 0),\n",
    "])\n",
    "def test_count_ordered_bigram_matches(doc_id, field, bigram, correct_value):\n",
    "    assert count_unordered_bigram_matches(es, doc_id, field, bigram) == correct_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e858574",
   "metadata": {
    "id": "qFTW4AlUXyzR"
   },
   "source": [
    "## Feedback\n",
    "\n",
    "Please give (anonymous) feedback on this exercise by filling out [this form](https://forms.gle/2jPayczbFhEcC9K68)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75a7e5",
   "metadata": {
    "id": "fToaG7gNskV6"
   },
   "source": [
    "# Entity linking\n",
    "\n",
    "You are provided with the data from a knowledge graph and asked to annotate an input document using a general entity linking pipeline approach, consisting of mention detection, candidate selection, and disambiguation steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e699796",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYr-PgAMsmBa",
    "outputId": "d97a23fd-eba1-4665-998a-212c2031b1c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ipytest\n",
      "  Downloading ipytest-0.12.0-py3-none-any.whl (15 kB)\n",
      "Collecting pytest>=5.4\n",
      "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 5.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.11.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.12.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 45.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n",
      "Installing collected packages: pluggy, jedi, iniconfig, pytest, ipytest\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 0.7.1\n",
      "    Uninstalling pluggy-0.7.1:\n",
      "      Successfully uninstalled pluggy-0.7.1\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 3.6.4\n",
      "    Uninstalling pytest-3.6.4:\n",
      "      Successfully uninstalled pytest-3.6.4\n",
      "Successfully installed iniconfig-1.1.1 ipytest-0.12.0 jedi-0.18.1 pluggy-1.0.0 pytest-7.1.3\n"
     ]
    }
   ],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a30016",
   "metadata": {
    "id": "1Z97d_r4skV8"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1d854",
   "metadata": {
    "id": "l5-3BIR3skV9"
   },
   "source": [
    "## 1) Mention detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc75099",
   "metadata": {
    "id": "eWyIthUkskV9"
   },
   "source": [
    "You are given an excerpt from a surface form dictionary in the format `SF_DICT[mention][entity] = count`, where `count` refers to the number of times `mention` was linked to `entity` in a given training corpus.\n",
    "The total count of linked occurrences of a mention is given as the key `_total` (i.e., this is the number of times mention was linked to any entity in the training corpus).\n",
    "Note that not all linked entities are listed in the dictionary, hence the counts do not necessarily sum up to `_total`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a5f72",
   "metadata": {
    "id": "iPGaf-GTskV-"
   },
   "outputs": [],
   "source": [
    "SF_DICT = {\n",
    "    \"1992 elections\": {\n",
    "        \"wikipedia:Philippine_general_election,_1992\": 9,\n",
    "        \"wikipedia:Angolan_presidential_election,_1992\": 1,\n",
    "        \"_total\": 98\n",
    "    },\n",
    "    \"angola\": {\n",
    "        \"wikipedia:Angola\": 4026,\n",
    "        \"wikipedia:Angola_(Portugal)\": 6,\n",
    "        \"wikipedia:Angola_national_football_team\": 120,\n",
    "        \"_total\": 4298\n",
    "    },\n",
    "    \"democracy\": {\n",
    "        \"wikipedia:Democracy\": 108,\n",
    "        \"wikipedia:Democracy_(album)\": 3,\n",
    "        \"_total\": 2162\n",
    "    },\n",
    "    \"multiparty democracy\": {\n",
    "        \"wikipedia:multiparty_democracy\": 11,\n",
    "        \"_total\": 11\n",
    "    },\n",
    "    \"one party\": {\n",
    "        \"wikipedia:Non-possessors\": 1,\n",
    "        \"wikipedia:Single-party_state\": 5,\n",
    "        \"_total\": 983\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ea0d4",
   "metadata": {
    "id": "KVnnFgGcskV-"
   },
   "outputs": [],
   "source": [
    "TEXT = (\n",
    "    \"Angola changed from a one-party Marxist-Leninist system \"\n",
    "    \"ruled by the MPLA to a formal multiparty democracy \"\n",
    "    \"following the 1992 elections\"\n",
    ").lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208dee6",
   "metadata": {
    "id": "qL774JMgskV-"
   },
   "source": [
    "We perform mention detection based on the following heuristic:\n",
    "\n",
    "  - At each term position\n",
    "    - Start with the longest possible n-gram (n = 8). \n",
    "    - If the n-gram is found in the dictionary, the mention and the corresponding entities are kept (and the shorter n-grams are ignored). Otherwise, we try to match the (n-1)-grams. \n",
    "    - Repeat until a mention is found or n reaches 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452a3c2",
   "metadata": {
    "id": "DWlm3gSNskV_"
   },
   "outputs": [],
   "source": [
    "def detect_mentions(text, sf_dict):\n",
    "    \"\"\"Performs mention detection in text against a given surface form dictionary.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text.\n",
    "        sf_dict: Surface form dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        List of matches as `(pos, mention, entity)` tuples ordered by pos, mention, and entity.\n",
    "        (Term positions are indexed from 0.)\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    tokens = text.split()\n",
    "    for pos, term in enumerate(tokens):\n",
    "        n = max(8, len(tokens) - pos)\n",
    "        while n > 0:\n",
    "            # Check for matching n-gram\n",
    "            n_gram = \" \".join(tokens[pos:pos+n])\n",
    "            if n_gram in sf_dict:\n",
    "                for entity in sorted(sf_dict[n_gram].keys()):\n",
    "                    if entity != \"_total\":\n",
    "                        matches.append((pos, n_gram, entity))\n",
    "                break\n",
    "            n -= 1\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a5a56",
   "metadata": {
    "id": "Y3JtCTF2skWA"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe102b2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmHfpX_dskWA",
    "outputId": "74cad9db-2307-4ab7-edbb-5c90d4b6b692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_detect_mentions():\n",
    "    assert detect_mentions(TEXT, SF_DICT) == [\n",
    "        (0, \"angola\", \"wikipedia:Angola\"),\n",
    "        (0, \"angola\", \"wikipedia:Angola_(Portugal)\"),\n",
    "        (0, \"angola\", \"wikipedia:Angola_national_football_team\"),\n",
    "        (14, \"multiparty democracy\", \"wikipedia:multiparty_democracy\"),\n",
    "        (15, \"democracy\", \"wikipedia:Democracy\"),\n",
    "        (15, \"democracy\", \"wikipedia:Democracy_(album)\"),\n",
    "        (18, \"1992 elections\", \"wikipedia:Angolan_presidential_election,_1992\"),\n",
    "        (18, \"1992 elections\", \"wikipedia:Philippine_general_election,_1992\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65edf1",
   "metadata": {
    "id": "-0EG7mrSskWB"
   },
   "source": [
    "## 2) Entity ranking\n",
    "\n",
    "Entity ranking is based on the commonness score:\n",
    "\n",
    "$$Commonness(e, m) = p(e|m) = \\frac{n(m, e)}{\\sum_{e'} n(m, e')}$$\n",
    "\n",
    "where $n(m, e)$ denotes the number of times entity $e$ is the link target of mention $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80762e74",
   "metadata": {
    "id": "DKbRkhRhskWB"
   },
   "outputs": [],
   "source": [
    "def commonness(entity, mention, sf_dict):\n",
    "    \"\"\"Computes the commonness for an entity-mention pair given a surface form dictionary.\n",
    "    \n",
    "    Args:\n",
    "        entity: Entity.\n",
    "        mention: Mention.\n",
    "        sf_dict: Surface form dictionary (containing entity-mention count statistics).\n",
    "        \n",
    "    Returns:\n",
    "        Commonness (float).    \n",
    "    \"\"\"\n",
    "    if mention not in sf_dict:\n",
    "        return None\n",
    "    return sf_dict[mention].get(entity, 0) / sf_dict[mention][\"_total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e62f9f",
   "metadata": {
    "id": "_e9VOdBYskWB"
   },
   "outputs": [],
   "source": [
    "def rank_entities(mentions, sf_dict, k=5):\n",
    "    \"\"\"Ranks candidate entities for each mention based on commonness and retains \n",
    "    the top-k highest-scoring entities for each mention.\n",
    "    \n",
    "    Args:\n",
    "        mentions: Detected mentions (list of `(mention, entity, pos)` tuples).\n",
    "        sf_dict: Surface form dictionary.\n",
    "        k: Number of top-scoring entities to keep for each mention.\n",
    "        \n",
    "    Returns:\n",
    "        Candidate entities with scores for each mention. Each mention is \n",
    "        represented as a dict `{'mention': xxx, 'pos': yyy, 'entities': zzz`,\n",
    "        where entities is a list of `(entity, score)` tuples ordered by score desc.\n",
    "    \"\"\"\n",
    "    # Reorganize input for more convenient processing.\n",
    "    entities_per_mention = {}\n",
    "    for (pos, mention, entity) in mentions:\n",
    "        key = \"{}::{}\".format(pos, mention)\n",
    "        if key not in entities_per_mention:\n",
    "            entities_per_mention[key] = []\n",
    "        entities_per_mention[key].append(entity)\n",
    "    \n",
    "    # Score all candidate entities for each mention.\n",
    "    mentions_entities = []\n",
    "    for key, entities in entities_per_mention.items():\n",
    "        pos, mention = key.split(\"::\")\n",
    "        entity_scores = sorted([(entity, commonness(entity, mention, sf_dict))\n",
    "                               for entity in entities], key=lambda x: x[1], reverse=True)\n",
    "        mentions_entities.append({\n",
    "            \"mention\": mention,\n",
    "            \"pos\": int(pos),\n",
    "            \"entities\": entity_scores[:k]\n",
    "        })\n",
    "    return mentions_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f10a8",
   "metadata": {
    "id": "FSYJgX89skWB"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673c11f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZv6IgVUskWB",
    "outputId": "2a87beb4-c7e9-4965-d1bc-b4d399cfa049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                     [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"mention,entity,correct_value\", [\n",
    "    (\"1992 elections\", \"wikipedia:Philippine_general_election,_1992\", 9/98),\n",
    "    (\"1992 elections\", \"wikipedia:Angolan_presidential_election,_1992\", 1/98),\n",
    "    (\"angola\", \"wikipedia:Angola\", 4026 / 4298),\n",
    "    (\"angola\", \"wikipedia:Angola_national_football_team\", 120 / 4298),\n",
    "    (\"democracy\", \"wikipedia:Democracy\", 108/2162),\n",
    "    (\"democracy\", \"wikipedia:Democracy_(album)\", 3/2162),\n",
    "    (\"multiparty democracy\", \"wikipedia:multiparty_democracy\", 1)    \n",
    "])\n",
    "def test_commonness(entity, mention, correct_value):    \n",
    "    assert commonness(entity, mention, SF_DICT) == pytest.approx(correct_value, rel=1e-3)\n",
    "    \n",
    "def test_rank_entities():\n",
    "    mentions = detect_mentions(TEXT, SF_DICT)\n",
    "    ranked_entities = rank_entities(mentions, SF_DICT, k=2)\n",
    "    assert ranked_entities[0] == {\"mention\": \"angola\",\n",
    "                                  \"pos\": 0,\n",
    "                                  \"entities\": [\n",
    "                                      (\"wikipedia:Angola\", 0.9367147510469986),\n",
    "                                      (\"wikipedia:Angola_national_football_team\", 0.02791996277338297)\n",
    "                                  ]\n",
    "                                 }\n",
    "    assert ranked_entities[1] == {\"mention\": \"multiparty democracy\",\n",
    "                                  \"pos\": 14,\n",
    "                                  \"entities\": [(\"wikipedia:multiparty_democracy\", 1.0)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c4a71",
   "metadata": {
    "id": "go9-nmc0skWC"
   },
   "source": [
    "## 3) Disambiguation\n",
    "\n",
    "Perform disambiguation by simply returning the entity for each mention with the highest score and only if it is above the given threshold.\n",
    "\n",
    "In case of containment or overlapping mentions, keep only the one with the higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855710f",
   "metadata": {
    "id": "T46idzd2skWC"
   },
   "outputs": [],
   "source": [
    "def disambiguate(ranked_entities, threshold=0.1):\n",
    "    \"\"\"Disambiguates entities for each mention by keeping only the highest-scoring one.\n",
    "    \n",
    "    Args:\n",
    "        ranked_entities: List of mentions along with a ranked list of candidate entities.\n",
    "        threshold: Score threshold\n",
    "    \n",
    "    Returns:\n",
    "        Entity annotations as a list of `(pos, mention, entity)` tuples.\n",
    "    \"\"\"\n",
    "    # For each term position, we keep track of the highest scoring entity\n",
    "    # that is linked to a mention in that position. We can greedily replace \n",
    "    # the annotations in case a higher scoring one is found.\n",
    "    annotations = {}\n",
    "    for candidates in ranked_entities:\n",
    "        (entity, score) = candidates[\"entities\"][0]\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        start_pos = candidates[\"pos\"]\n",
    "        mention = candidates[\"mention\"]\n",
    "        mention_length = len(mention.split())\n",
    "        # Add mention-entity annotation if all term positions are\n",
    "        # empty or are lower scoring.\n",
    "        add_annotation = True\n",
    "        for pos in range(start_pos, start_pos + mention_length):\n",
    "            if pos in annotations:\n",
    "                if annotations[pos][\"score\"] > score:\n",
    "                    add_annotation = False\n",
    "        \n",
    "        if add_annotation:\n",
    "            # For each term position, check if there is an existing \n",
    "            # annotation to be replaced.\n",
    "            for pos in range(start_pos, start_pos + mention_length):\n",
    "                if pos in annotations:\n",
    "                    print(\"Replace on \", pos)\n",
    "                    # Replace existing annotation.\n",
    "                    start_pos_old = annotatations[pos][\"start_pos\"]\n",
    "                    mention_length_old = annotatations[pos][\"mention_length\"]\n",
    "                    for i in range(start_pos_old, start_pos_old + mention_length_old):\n",
    "                        del annotations[i]\n",
    "                \n",
    "                # Store new annotation.\n",
    "                annotations[pos] = {\n",
    "                    \"score\": score,\n",
    "                    \"entity\": entity,\n",
    "                    \"mention\": mention,\n",
    "                    \"start_pos\": start_pos,\n",
    "                    \"mention_length\": mention_length\n",
    "                }\n",
    "\n",
    "    # Converting output to desired format.\n",
    "    linked_entities = []\n",
    "    for pos, annotation in sorted(annotations.items()):\n",
    "        if pos == annotation[\"start_pos\"]:\n",
    "            linked_entities.append((pos, annotation[\"mention\"], annotation[\"entity\"]))\n",
    "    return linked_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e034ab",
   "metadata": {
    "id": "Jl4QYnIQskWC"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a80e3c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C75Uv1rUskWC",
    "outputId": "978ece9f-3de0-4c98-91fe-bf42881ea1b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_disambiguate():\n",
    "    mentions = detect_mentions(TEXT, SF_DICT)\n",
    "    ranked_entities = rank_entities(mentions, SF_DICT)\n",
    "    linked_entities = disambiguate(ranked_entities, threshold=0.01)\n",
    "    assert linked_entities == [\n",
    "        (0, \"angola\", \"wikipedia:Angola\"),\n",
    "        (14, \"multiparty democracy\", \"wikipedia:multiparty_democracy\"),\n",
    "        (18, \"1992 elections\", \"wikipedia:Philippine_general_election,_1992\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97adca49",
   "metadata": {
    "id": "ibWxbNBhIXOh"
   },
   "source": [
    "# Entity linking evaluation\n",
    "You are provided with the documents annotations along with ground truth annotations and asked to evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc3ce3c",
   "metadata": {
    "id": "AuO8T7LUIONA"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3feeb",
   "metadata": {
    "id": "qbd9N7s5IiLM"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724ac57",
   "metadata": {
    "id": "8nu_VkIVgjnv"
   },
   "source": [
    "The annotations given by a entity linking system under evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21555d5",
   "metadata": {
    "id": "KL6yvSIcOkUU"
   },
   "outputs": [],
   "source": [
    "LINKED_ENTITIES_1 = [ \n",
    "    (0, 'angola', 'wikipedia:Angola'),\n",
    "    (14, 'multiparty democracy', 'wikipedia:multiparty_democracy'),\n",
    "    (18, '1992 elections', 'wikipedia:Philippine_general_election,_1992')\n",
    "]\n",
    "\n",
    "LINKED_ENTITIES_2 = [\n",
    "    (5, 'angola', 'wikipedia:Angola'),\n",
    "    (10, '1975', 'wikipedia:Philippine_general_election,_1992'),\n",
    "    (13, 'one party', 'wikipedia:Single-party_state')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04075697",
   "metadata": {
    "id": "gHVEsm3hgsCQ"
   },
   "source": [
    "Ground truth annotations (reference annotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470ebb8",
   "metadata": {
    "id": "4q_5Ij3fQWR0"
   },
   "outputs": [],
   "source": [
    "GROUND_TRUTH_ANNOTATIONS_1 = [ \n",
    "    (0, 'angola', 'wikipedia:Angola'),\n",
    "    (4, 'one-party', 'wikipedia:Single-party_state'),\n",
    "    (14, 'multiparty democracy', 'wikipedia:multiparty_democracy'),\n",
    "    (18, '1992 elections', 'wikipedia:Philippine_general_election,_1992')\n",
    "]\n",
    "\n",
    "GROUND_TRUTH_ANNOTATIONS_2 = [\n",
    "    (5, 'angola', 'wikipedia:Angola'),\n",
    "    (13, 'one party', 'wikipedia:Single-party_state'),\n",
    "    (14, 'Republic', 'wikipedia:Republic')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c947fd",
   "metadata": {
    "id": "DOsTzvL_g1e7"
   },
   "source": [
    "Set-based metrics where:\n",
    "- precision is defined as the fraction of correctly linked entities that have been annotated by the system\n",
    "- recall is defined as fraction of correctly linked entities that should be annotated \n",
    "- F-measure is a harmonic mean between precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98d873",
   "metadata": {
    "id": "ZMDYNJVZX6Hs"
   },
   "outputs": [],
   "source": [
    "def set_based_precision(annotations, relevance_annotations):\n",
    "  \"\"\"Computes set-based precision.\n",
    "  \n",
    "  Args:\n",
    "      annotations: All annotations for a set of documents.\n",
    "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
    "      \n",
    "  Returns:\n",
    "      Set-based precision.    \n",
    "  \"\"\"\n",
    "  return len(set(annotations).intersection(relevance_annotations))/len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c71699",
   "metadata": {
    "id": "YHITcKHIYK6r"
   },
   "outputs": [],
   "source": [
    "def set_based_recall(annotations, relevance_annotations):\n",
    "  \"\"\"Computes set-based recall.\n",
    "  \n",
    "  Args:\n",
    "      annotations: All annotations for a set of documents.\n",
    "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
    "      \n",
    "  Returns:\n",
    "      Set-based recall.    \n",
    "  \"\"\"\n",
    "  return len(set(annotations).intersection(relevance_annotations))/len(relevance_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9cdba",
   "metadata": {
    "id": "qWRO31XBdtcy"
   },
   "outputs": [],
   "source": [
    "def f1_score(precision, recall):\n",
    "  \"\"\"Computes F-measure.\n",
    "  \n",
    "  Args:\n",
    "      annotations: All annotations for a set of documents.\n",
    "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
    "      \n",
    "  Returns:\n",
    "      F-measure.    \n",
    "  \"\"\"\n",
    "  return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51cde3",
   "metadata": {
    "id": "tcBzUihbhb_j"
   },
   "source": [
    "## Metrics over the collection of documents\n",
    "\n",
    "Micro-averaged - averaged across mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeaad6b",
   "metadata": {
    "id": "hpSNh02zQ29O"
   },
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def micro_precision(annotations, ground_truth_annotations):\n",
    "  \"\"\"Computes micro-averaged precision.\n",
    "  \n",
    "  Args:\n",
    "      annotations: All annotations for a set of documents.\n",
    "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
    "      \n",
    "  Returns:\n",
    "      Micro-averaged precision.    \n",
    "  \"\"\"\n",
    "  all_annotations = list(itertools.chain(*annotations))\n",
    "  all_ground_truth_annotations = list(itertools.chain(*ground_truth_annotations))\n",
    "  return set_based_precision(all_annotations, all_ground_truth_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f3b27",
   "metadata": {
    "id": "KoKdOBzhSta7"
   },
   "outputs": [],
   "source": [
    "def micro_recall(all_annotations, ground_truth_annotations):\n",
    "  \"\"\"Computes micro-averaged recall.\n",
    "  \n",
    "  Args:\n",
    "      annotations: All annotations for a set of documents.\n",
    "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
    "      \n",
    "  Returns:\n",
    "      Micro-averaged recall.    \n",
    "  \"\"\"\n",
    "  all_annotations = list(itertools.chain(*all_annotations))\n",
    "  all_ground_truth_annotations = list(itertools.chain(*ground_truth_annotations))\n",
    "  return set_based_recall(all_annotations, all_ground_truth_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf64f14f",
   "metadata": {
    "id": "Np0FExe9TSpH"
   },
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8019d3",
   "metadata": {
    "id": "f78_TzYnTSUV"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_micro_precision():\n",
    "  assert micro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx(5/6, rel=1e-2)\n",
    "\n",
    "def test_micro_recall():\n",
    "  assert micro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx(5/7, rel=1e-2)\n",
    "\n",
    "def test_micro_f1():\n",
    "  micro_p = micro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n",
    "  micro_r = micro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n",
    "  assert f1_score(micro_p, micro_r) == pytest.approx((2 * 5/6 * 5/7) / (5/6 + 5/7), rel=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6773e68",
   "metadata": {
    "id": "6vNQN5oIhl7x"
   },
   "source": [
    "Macro-averaged - averaged across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb70df",
   "metadata": {
    "id": "0lYkqs2dXG0c"
   },
   "outputs": [],
   "source": [
    "def macro_precision(annotations, ground_truth_annotations):\n",
    "  \"\"\"Computes macro-averaged precision.\n",
    "  \n",
    "  Args:\n",
    "      annotations: All annotations for a set of documents.\n",
    "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
    "      \n",
    "  Returns:\n",
    "      Macro-averaged precision.    \n",
    "  \"\"\"\n",
    "  return sum(set_based_precision(annotation, ground_truth) for annotation, ground_truth \n",
    "             in zip(annotations, ground_truth_annotations))/len(ground_truth_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4baa1",
   "metadata": {
    "id": "KyXXm-_1YuKO"
   },
   "outputs": [],
   "source": [
    "def macro_recall(annotations, ground_truth_annotations):\n",
    "  \"\"\"Computes macro-averaged recall.\n",
    "  \n",
    "  Args:\n",
    "      annotations: All annotations for a set of documents.\n",
    "      relevance_annotations: All reference (ground truth) annotations for a set of documents.\n",
    "      \n",
    "  Returns:\n",
    "      Macro-averaged recall.    \n",
    "  \"\"\"\n",
    "  return sum(set_based_recall(annotation, ground_truth) for annotation, ground_truth \n",
    "             in zip(annotations, ground_truth_annotations))/len(ground_truth_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7d38c",
   "metadata": {
    "id": "YCkmcF6_hqP1"
   },
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6d043",
   "metadata": {
    "id": "rymX_XcYYy5n"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_macro_precision():\n",
    "  assert macro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx((1 + 2/3)/2, rel=1e-2)\n",
    "\n",
    "def test_macro_recall():\n",
    "  assert macro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2]) == pytest.approx((3/4 + 2/3)/2, rel=1e-2)\n",
    "\n",
    "def test_macro_f1():\n",
    "  macro_p = macro_precision([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n",
    "  macro_r = macro_recall([LINKED_ENTITIES_1, LINKED_ENTITIES_2], [GROUND_TRUTH_ANNOTATIONS_1, GROUND_TRUTH_ANNOTATIONS_2])\n",
    "  assert f1_score(macro_p, macro_r) == pytest.approx((2 * 5/6 * 17/24) / (5/6 + 17/24), rel=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf33ec6",
   "metadata": {
    "id": "CZmX7MHBNQbH"
   },
   "source": [
    "# Entity linking incorporated retrieval (ELR)\n",
    "\n",
    "In this exercise you will implement the entity matches feature function:  \n",
    "$$\t\t      \tf_{\\mathcal{E}}(e_i; e) = \\log \\sum_{f \\in \\mathcal{\\tilde{F}}} w_{f}^{\\mathcal{E}} \\left( (1- \\lambda )\\, \\mathbb{1}(e_i , f_{\\tilde{e}}) + \\lambda\\, \\frac{\\sum_{e' \\in \\mathcal{E}} \\mathbb{1}(e_i,f_{\\tilde{e}'})}{|\\{e' \\in \\mathcal{E} : f_{\\tilde{e}'} \\neq \\emptyset\\}|} \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db83403",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRgAvYf5NCms",
    "outputId": "32118bb6-5468-4878-8ee8-fa907a3a6d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ipytest\n",
      "  Downloading ipytest-0.12.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
      "Collecting pytest>=5.4\n",
      "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 20.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.11.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.12.0)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 16.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n",
      "Installing collected packages: pluggy, jedi, iniconfig, pytest, ipytest\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 0.7.1\n",
      "    Uninstalling pluggy-0.7.1:\n",
      "      Successfully uninstalled pluggy-0.7.1\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 3.6.4\n",
      "    Uninstalling pytest-3.6.4:\n",
      "      Successfully uninstalled pytest-3.6.4\n",
      "Successfully installed iniconfig-1.1.1 ipytest-0.12.0 jedi-0.18.1 pluggy-1.0.0 pytest-7.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d53023",
   "metadata": {
    "id": "2ooC3b5yl0d8"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import math\n",
    "import pytest\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236cad19",
   "metadata": {
    "id": "3R_LcRk3l1kw"
   },
   "source": [
    "Term-based representations. These representations are only given to provide some context for a better understanding of the entity-based representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ece96",
   "metadata": {
    "id": "7UGHUhUCmRlb"
   },
   "outputs": [],
   "source": [
    "TERM_BASED_REPS = [{\n",
    "    \"label\": \"Ann Dunham\",\n",
    "     \"abstract\": \"\"\"Stanley Ann Dunham the mother Barack Obama, was an American\n",
    "        anthropologist who ...\"\"\",\n",
    "     \"birthPlace\": \"Honolulu Hawaii ...\",\n",
    "     \"child\": \"Barack Obama\",\n",
    "     \"wikiPageWikiLink\": \"United States Family Barack Obama\",\n",
    "     },\n",
    "     {\n",
    "    \"label\": \"Michael Jackson\",\n",
    "     \"abstract\": \"\"\"Michael Joseph Jackson (August 29, 1958 – June 25, 2009) \n",
    "        was an American singer, songwriter, and dancer. Dubbed the \"King of \n",
    "        Pop\", he is regarded as one of the most significant cultural figures \n",
    "        of the 20th century. Over a four-decade career, his contributions to \n",
    "        music, dance, and fashion...\"\"\",\n",
    "     \"birthPlace\": \"Gary Indiana\",\n",
    "     \"wikiPageWikiLink\": \"35th_Annual_Grammy_Awards, A._R._Rahman, ...\",\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a26ad",
   "metadata": {
    "id": "boVynYeao1OQ"
   },
   "source": [
    "Entity-based representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f1064",
   "metadata": {
    "id": "r30TiXrJo1mM"
   },
   "outputs": [],
   "source": [
    "ENTITY_BASED_REPS = [{\n",
    "    \"birthPlace\": [\"<Honolulu>\", \"<Hawaii>\"],\n",
    "    \"child\": [\"<Barack_Obama>\"],\n",
    "    \"wikiPageWikiLink\": [\"<United_States>\", \"<Family_of_Barack_Obama>\"],\n",
    "    },\n",
    "    {\n",
    "    \"birthPlace\": [\"<Gary_Indiana>\"],\n",
    "    \"wikiPageWikiLink\": [\"<35th_Annual_Grammy_Awards>\", \"<A._R._Rahman>\"],\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d0eae",
   "metadata": {
    "id": "JvHePuLataJt"
   },
   "source": [
    "Field weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec509ec3",
   "metadata": {
    "id": "w9BK2HN-tasF"
   },
   "outputs": [],
   "source": [
    "FIELD_WEIGHTS = {\n",
    "    \"birthPlace\": 0.4,\n",
    "    \"child\": 0.4,\n",
    "    \"wikiPageWikiLink\": 0.2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083074b3",
   "metadata": {
    "id": "z_VAH6Sho_sc"
   },
   "source": [
    "Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed0a5a",
   "metadata": {
    "id": "W90DPLPupOeq"
   },
   "outputs": [],
   "source": [
    "QUERY = (\"barack obama parents\", [\"<Barack_Obama>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb02d3",
   "metadata": {
    "id": "-vbGQp3spiR3"
   },
   "source": [
    "## Entity matches feature function\n",
    "\n",
    "$$\t\t      \tf_{\\mathcal{E}}(e_i; e) = \\log \\sum_{f \\in \\mathcal{\\tilde{F}}} w_{f}^{\\mathcal{E}} \\left( (1- \\lambda )\\, \\mathbb{1}(e_i , f_{\\tilde{e}}) + \\lambda\\, \\frac{\\sum_{e' \\in \\mathcal{E}} \\mathbb{1}(e_i,f_{\\tilde{e}'})}{|\\{e' \\in \\mathcal{E} : f_{\\tilde{e}'} \\neq \\emptyset\\}|} \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942b247",
   "metadata": {
    "id": "oj_viy_griBK"
   },
   "source": [
    "First, we implement the binary indicator function:\n",
    "$$\\mathbb{1}(e_i , f_{\\tilde{e}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787b42a",
   "metadata": {
    "id": "tAheyJVqz5UL"
   },
   "outputs": [],
   "source": [
    "def binary_indicator_function(entity: str, field_uris: List[str]) -> int:\n",
    "  \"\"\"Indicates whether or not the entity is present in the field\n",
    "\n",
    "  Args: \n",
    "    entity: URI string.\n",
    "    field_uris: List of URI string in field.\n",
    "\n",
    "  Returns:\n",
    "    1 if entity is in the field, 0 otherwise.\n",
    "  \"\"\"\n",
    "  return 1 if entity in field_uris else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc93c6",
   "metadata": {
    "id": "b2JgbtKlrv1g"
   },
   "source": [
    "Then, we implement a function to get document frequencies.\n",
    "\n",
    "$$df_{e,f} = \\sum_{e' \\in \\mathcal{E}} \\mathbb{1}(e_i,f_{\\tilde{e}'})$$\n",
    "\n",
    "$$df_f = |\\{e' \\in \\mathcal{E} : f_{\\tilde{e}'} \\neq \\emptyset\\}|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f93ca48",
   "metadata": {
    "id": "PZAJCec4-meD"
   },
   "outputs": [],
   "source": [
    "def get_document_frequencies(f: str, entity: str, entity_based_reps: List[Dict]) -> Tuple[int, int]:\n",
    "  \"\"\"Computes document frequencies for entity matches feature score.\n",
    "\n",
    "  df_e_f is the total number of documents that contain the entity e in field f.\n",
    "  df_f is the number of documents with a non-empty field f.\n",
    "\n",
    "  Args:\n",
    "    f: Field.\n",
    "    entity: URI string.\n",
    "    entity_based_reps: All entity-based representations.\n",
    "    \n",
    "  Returns:\n",
    "    Tuple with df_e_f and df_f.\n",
    "  \"\"\"\n",
    "  df_e_f, df_f = 0, 0\n",
    "  for e in entity_based_reps: \n",
    "    if f in e.keys():\n",
    "      df_f += 1\n",
    "      if entity in e[f]:\n",
    "        df_e_f += 1\n",
    "\n",
    "  return df_e_f, df_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7de5cc",
   "metadata": {
    "id": "5vB3kcgcsud2"
   },
   "source": [
    "Based on the two previous functions, we implement the entity matches feature score.\n",
    "\n",
    "$$\t\t      \tf_{\\mathcal{E}}(e_i; e) = \\log \\sum_{f \\in \\mathcal{\\tilde{F}}} w_{f}^{\\mathcal{E}} \\left( (1- \\lambda )\\, \\mathbb{1}(e_i , f_{\\tilde{e}}) + \\lambda\\, \\frac{\\sum_{e' \\in \\mathcal{E}} \\mathbb{1}(e_i,f_{\\tilde{e}'})}{|\\{e' \\in \\mathcal{E} : f_{\\tilde{e}'} \\neq \\emptyset\\}|} \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83503992",
   "metadata": {
    "id": "mSY7yessph3V"
   },
   "outputs": [],
   "source": [
    "def compute_entity_matches_feature(entity:str, entity_based_rep:Dict, entity_based_reps:List[Dict], field_weights: Dict[str,float], smoothing_param:float=0.1) -> float:\n",
    "  \"\"\"Computes entity matches feature score for an entity.\n",
    "  \n",
    "  Args:\n",
    "    entity: URI string.\n",
    "    entity_based_rep: Entity-based representation.\n",
    "    entity_based_reps: All entity-based representations.\n",
    "    field_weights: Field weights may be set manually or via dynamic mapping \n",
    "      using PRMS.\n",
    "    smoothing_param: Smoothing parameter.Defaults to 0.1.\n",
    "  Returns:\n",
    "    Entity matches feature score.\n",
    "  \"\"\"\n",
    "  sum = 0\n",
    "  for f, w_f_e in field_weights.items():\n",
    "    e_presence = binary_indicator_function(entity, entity_based_rep[f]) if f in entity_based_rep else 0\n",
    "    df_e_f, df_f = get_document_frequencies(f, entity, entity_based_reps)\n",
    "    sum += w_f_e * ((1 - smoothing_param) * e_presence + smoothing_param * df_e_f / df_f)\n",
    "  return math.log(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7dc86",
   "metadata": {
    "id": "qsi1G_R0qNnb"
   },
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944987bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaB6GGHUqP2R",
    "outputId": "bd265a23-735b-4218-d87c-f33ff36567cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_binary_indicator_function():\n",
    "  assert 1 == binary_indicator_function(\"<Honolulu>\", [\"<Honolulu>\", \"<Hawaii>\"])\n",
    "  assert 0 == binary_indicator_function(\"<Honolulu>\", [\"<Gary_Indiana>\"])\n",
    "\n",
    "def test_get_document_frequencies():\n",
    "  assert (1, 1) == get_document_frequencies(\"child\", QUERY[1][0], ENTITY_BASED_REPS)\n",
    "  assert (0, 2) == get_document_frequencies(\"birthPlace\", QUERY[1][0], ENTITY_BASED_REPS)\n",
    "\n",
    "def test_compute_entity_matches_feature():\n",
    "  assert pytest.approx(math.log(0.4), rel=1e-2) == compute_entity_matches_feature(QUERY[1][0], ENTITY_BASED_REPS[0], ENTITY_BASED_REPS, FIELD_WEIGHTS)\n",
    "  assert pytest.approx(math.log(0.04), rel=1e-2) == compute_entity_matches_feature(QUERY[1][0], ENTITY_BASED_REPS[1], ENTITY_BASED_REPS, FIELD_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670fe0d",
   "metadata": {
    "id": "pCeKbZggn03J"
   },
   "source": [
    "# Target Entity Type Identification Evaluation\n",
    "\n",
    "In this exercise, you'll need to implement lenient evaluation measures for the target entity type identification task.\n",
    "\n",
    "As a reminder, _target entity type identification_ is the task of finding the target types of a given input query, from a type taxonomy, such that these types correspond to most specific types of entities that are relevant to the query.  Target types cannot lie on the same branch in the taxonomy.\n",
    "\n",
    "Our final measure is normalized discounted cumulative gain (NDCG), but we need to compute the gain values of answer types based on their distance from ground truth types in the type taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f86de5",
   "metadata": {
    "id": "9yeNl9v4n1oZ"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658f827",
   "metadata": {
    "id": "K2wJwxXyoJ-O"
   },
   "outputs": [],
   "source": [
    "!wget --output-document=\"dbpedia_types.tsv\" \"https://raw.githubusercontent.com/iai-group/ir-course-2022/main/resources/dbpedia_types.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7fddb",
   "metadata": {
    "id": "yGkvUONFn03L"
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import math\n",
    "import operator\n",
    "import pytest\n",
    "from typing import Callable, List, Optional, Set\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75642e00",
   "metadata": {
    "id": "__wBTAxCn03M"
   },
   "source": [
    "## Type taxonomy\n",
    "\n",
    "We use the DBpedia Ontology as our type taxonomy. It is given to you in a preprocessed format in `dbpedia_types.tsv`, where each line corresponds to a type, and the tab-separated columns, respectively, are: type identifier, depth in the hierarchy, and parent type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea2378",
   "metadata": {
    "id": "DcHqx4Ken03N"
   },
   "outputs": [],
   "source": [
    "class TypeTaxonomy:\n",
    "    \n",
    "    ROOT = \"owl:Thing\"\n",
    "    \n",
    "    def __init__(self, tsv_filename: str) -> None:\n",
    "        \"\"\"Initializes the type taxonomy by loading it from a TSV file.\n",
    "        \n",
    "        Args:\n",
    "            tsv_filename: Name of TSV file, with type_id, depth, and parent_id columns.\n",
    "        \"\"\"\n",
    "        self._types = {self.ROOT: {\"parent\": None, \"depth\": 0}}\n",
    "        self._max_depth = 0\n",
    "        with open(tsv_filename, \"r\") as tsv_file:\n",
    "            next(tsv_file)  # Skip header row\n",
    "            for line in tsv_file:\n",
    "                fields = line.rstrip().split(\"\\t\")\n",
    "                type_id, depth, parent_type = fields[0], int(fields[1]), fields[2]\n",
    "                self._types[type_id] = {\"parent\": parent_type, \"depth\": depth}\n",
    "                self._max_depth = max(depth, self._max_depth)\n",
    "                \n",
    "        # Once all types have been read in, we also populate each type with a list\n",
    "        # of its children for convenience (if the taxonomy is to be traversed\n",
    "        # downwards not just upwards).\n",
    "        for type_id in self._types:\n",
    "            if type_id == self.ROOT:\n",
    "                continue\n",
    "            parent_type = self._types[type_id][\"parent\"]            \n",
    "            if \"children\" not in self._types[parent_type]:\n",
    "                self._types[parent_type][\"children\"] = set()\n",
    "            self._types[parent_type][\"children\"].add(type_id)\n",
    "                        \n",
    "    def max_depth(self) -> int:\n",
    "        \"\"\"Returns the maximum depth of the type taxonomy.\"\"\"\n",
    "        return self._max_depth\n",
    "    \n",
    "    def is_root(self, type_id: str) -> bool:\n",
    "        \"\"\"Returns true if the type is the root of the taxonomy.\n",
    "        \n",
    "        Args:\n",
    "            type_id: Type ID.\n",
    "            \n",
    "        Returns:\n",
    "            True if root.\n",
    "        \"\"\"\n",
    "        return type_id == self.ROOT\n",
    "    \n",
    "    def depth(self, type_id: str) -> int:\n",
    "        \"\"\"Returns the depth of a type in the taxonomy.\n",
    "        \n",
    "        Args:\n",
    "            type_id: Type ID.\n",
    "            \n",
    "        Returns:\n",
    "            The depth of the type in the hierarchy (0 for root).\n",
    "        \"\"\"\n",
    "        return self._types.get(type_id, {}).get(\"depth\")\n",
    "\n",
    "    def parent(self, type_id: str) -> Optional[str]:\n",
    "        \"\"\"Returns the parent type of a type in the taxonomy.\n",
    "        \n",
    "        Args:\n",
    "            type_id: Type ID.\n",
    "            \n",
    "        Returns:\n",
    "            Parent type ID, or None if the input type is root.\n",
    "        \"\"\"\n",
    "        return self._types.get(type_id, {}).get(\"parent\")\n",
    "\n",
    "    def children(self, type_id: str) -> Set[str]:\n",
    "        \"\"\"Returns the set of children types of a type in the taxonomy.\n",
    "        \n",
    "        Args:\n",
    "            type_id: Type ID.\n",
    "            \n",
    "        Returns:\n",
    "            Set of type IDs (empty set if leaf type).\n",
    "        \"\"\"\n",
    "        return self._types.get(type_id, {}).get(\"children\", set())\n",
    "    \n",
    "    def dist(self, type_id1: str, type_id2: str) -> float:\n",
    "        \"\"\"Computes the distance between two types in the taxonomy.\n",
    "        \n",
    "        Args:\n",
    "            type_id1: ID of first type.\n",
    "            type_id2: ID of second type.\n",
    "            \n",
    "        Returns:\n",
    "            The distance between the two types in the type taxonomy, which is\n",
    "            the number of steps between them if they lie on the same branch,\n",
    "            and otherwise `math.inf`.\n",
    "        \"\"\"\n",
    "        # Find which type has higher depth and set if to type_a; the other is type_b.        \n",
    "        type_a, type_b = (type_id2, type_id1) if self.depth(type_id1) < self.depth(type_id2) \\\n",
    "                         else (type_id1, type_id2)\n",
    "        dist = self.depth(type_a) - self.depth(type_b)\n",
    "        \n",
    "        # If they lie on the same branch, then when traversing type_a for dist steps\n",
    "        # would make us end up with type_b; otherwise, they're not on the same branch.\n",
    "        for _ in range(dist):\n",
    "            type_a = self.parent(type_a)\n",
    "        \n",
    "        return dist if type_a == type_b else math.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d775bdec",
   "metadata": {
    "id": "iR6uoHdin03P"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23017207",
   "metadata": {
    "id": "eXm-xkOjn03P"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.fixture\n",
    "def dbpedia_types():\n",
    "    return TypeTaxonomy(\"dbpedia_types.tsv\")\n",
    "\n",
    "def test_max_depth(dbpedia_types):\n",
    "    assert dbpedia_types.max_depth() == 7\n",
    "\n",
    "@pytest.mark.parametrize(\"type_id,depth\", [\n",
    "    (\"owl:Thing\", 0),\n",
    "    (\"dbo:Agent\", 1),\n",
    "    (\"dbo:SportFacility\", 4),\n",
    "    (\"dbo:RaceTrack\", 5)\n",
    "])\n",
    "def test_depth(dbpedia_types, type_id, depth):\n",
    "    assert dbpedia_types.depth(type_id) == depth\n",
    "    \n",
    "@pytest.mark.parametrize(\"type_id,parent\", [\n",
    "    (\"owl:Thing\", None),\n",
    "    (\"dbo:Agent\", \"owl:Thing\"),\n",
    "    (\"dbo:SportFacility\", \"dbo:ArchitecturalStructure\"),\n",
    "    (\"dbo:RaceTrack\", \"dbo:SportFacility\")\n",
    "])\n",
    "def test_depth(dbpedia_types, type_id, parent):\n",
    "    assert dbpedia_types.parent(type_id) == parent\n",
    "\n",
    "@pytest.mark.parametrize(\"type_id1,type_id2,distance\", [\n",
    "    (\"dbo:Agent\", \"dbo:Agent\", 0),  # same type\n",
    "    (\"dbo:Agent\", \"dbo:Person\", 1),  # type2 is more specific\n",
    "    (\"dbo:Artist\", \"dbo:Agent\", 2),  # type2 is more generic\n",
    "    (\"dbo:Artist\", \"dbo:Broadcaster\", math.inf)  # different branch\n",
    "])  \n",
    "def test_distance(dbpedia_types, type_id1, type_id2, distance):\n",
    "    assert dbpedia_types.dist(type_id1, type_id2) == distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66819bb",
   "metadata": {
    "id": "SZX8eBAIn03Q"
   },
   "source": [
    "## Computing gain values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d8682b",
   "metadata": {
    "id": "9rmPQjSqn03R"
   },
   "source": [
    "For simplicity, refer to this global variable in the gain computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc504537",
   "metadata": {
    "id": "Cfw278Oen03R"
   },
   "outputs": [],
   "source": [
    "type_taxonomy = TypeTaxonomy(\"dbpedia_types.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282e6b6",
   "metadata": {
    "id": "7AT2cu8Jn03S"
   },
   "source": [
    "When defined in a _linear_ fashion, the gain of a type is computed as:\n",
    "\n",
    "$$r(y) = \\max_{\\hat{y} \\in \\hat{\\mathcal{T}}_q} \\big( 1 - \\frac{d(y,\\hat{y})}{h} \\big)$$\n",
    "\n",
    "where $\\hat{\\mathcal{T}}_q$ is the set of ground truth types, $\\hat{y}$ is a ground truth type, $y$ is an answer type, $d(y, \\hat{y})$ is the distance between types in the taxonomy, and $h$ is the maximum depth of the type taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798ea35",
   "metadata": {
    "id": "uGV8MdxFn03S"
   },
   "outputs": [],
   "source": [
    "def gain_linear(gt_types: Set[str], answer_type_id: str) -> float:\n",
    "    \"\"\"Computes the gain of an answer type in a linear fashion.\n",
    "    \n",
    "    Args:\n",
    "        gt_types: Set of ground truth type IDs.\n",
    "        answer_type_id: Answer type ID.\n",
    "    \n",
    "    Returns:\n",
    "        Gain value.\n",
    "    \"\"\"\n",
    "    # Note: if the distance between two types is inf, we set the linear gain to 0.\n",
    "    return max([\n",
    "        1 - type_taxonomy.dist(gt_type_id, answer_type_id) / type_taxonomy.max_depth() \n",
    "        if type_taxonomy.dist(gt_type_id, answer_type_id) < math.inf else 0\n",
    "        for gt_type_id in gt_types\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ef0e8",
   "metadata": {
    "id": "81O75ju2n03S"
   },
   "source": [
    "Alternatively, the gain of an answer type can be defined using an _exponential_ decay function:\n",
    "\n",
    "$$r(y) = \\max_{\\hat{y} \\in \\hat{\\mathcal{T}}_q} \\big ( b^{-d(y,\\hat{y})} \\big )$$\n",
    "\n",
    "where $b$ is the base of the exponential function (here: $b=2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188867ff",
   "metadata": {
    "id": "vIIt30O2n03T"
   },
   "outputs": [],
   "source": [
    "def gain_exponential(gt_types: Set[str], answer_type_id: str) -> float:\n",
    "    \"\"\"Computes the gain of an answer type using exponential decay.\n",
    "    \n",
    "    Args:\n",
    "        gt_types: Set of ground truth type IDs.\n",
    "        answer_type_id: Answer type ID.\n",
    "    \n",
    "    Returns:\n",
    "        Gain value.\n",
    "    \"\"\"\n",
    "    # Note: if the distance between two types is inf, we set the exponential gain to 0.\n",
    "    return max([\n",
    "        2**(-type_taxonomy.dist(gt_type_id, answer_type_id))\n",
    "        if type_taxonomy.dist(gt_type_id, answer_type_id) < math.inf else 0\n",
    "        for gt_type_id in gt_types\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cff186",
   "metadata": {
    "id": "ICEF67gKn03T"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39574f90",
   "metadata": {
    "id": "WYgYRq4On03T"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"gt_types,answer_type_id,gain\", [\n",
    "    ([\"dbo:Agent\"], \"dbo:Agent\", 1),  # same type\n",
    "    ([\"dbo:Agent\"], \"dbo:Person\", 1-1/7),  # type2 is more specific\n",
    "    ([\"dbo:Artist\"], \"dbo:Agent\", 1-2/7),  # type2 is more generic\n",
    "    ([\"dbo:NationalSoccerClub\"], \"dbo:Organisation\", 1-3/7),  # type2 is more generic\n",
    "    ([\"dbo:Artist\"], \"dbo:Broadcaster\", 0),  # different branch\n",
    "    ([\"dbo:DisneyCharacter\"], \"dbo:MythologicalFigure\", 0),  # sibling categories\n",
    "])\n",
    "def test_gain_linear(gt_types, answer_type_id, gain):\n",
    "    assert gain_linear(gt_types, answer_type_id) == pytest.approx(gain)\n",
    "    \n",
    "@pytest.mark.parametrize(\"gt_types,answer_type_id,gain\", [\n",
    "    ([\"dbo:Agent\"], \"dbo:Agent\", 1),  # same type\n",
    "    ([\"dbo:Agent\"], \"dbo:Person\", 1/2),  # type2 is more specific\n",
    "    ([\"dbo:Artist\"], \"dbo:Agent\", 1/4),  # type2 is more generic\n",
    "    ([\"dbo:NationalSoccerClub\"], \"dbo:Organisation\", 1/8),  # type2 is more generic\n",
    "    ([\"dbo:Artist\"], \"dbo:Broadcaster\", 0),  # different branch\n",
    "    ([\"dbo:DisneyCharacter\"], \"dbo:MythologicalFigure\", 0),  # sibling categories\n",
    "])\n",
    "def test_gain_exponential(gt_types, answer_type_id, gain):\n",
    "    assert gain_exponential(gt_types, answer_type_id) == pytest.approx(gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3dee7",
   "metadata": {
    "id": "K0y3nl7dn03T"
   },
   "source": [
    "## Putting everything together\n",
    "\n",
    "Plug the gain values computed using either linear or exponential into the NDCG computation to get a final evaluation score.\n",
    "\n",
    "The DCG and NDCG computation parts are given. The only part that needs completing is the construction of the ideal ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89d212",
   "metadata": {
    "id": "5uE5c0Rjn03U"
   },
   "outputs": [],
   "source": [
    "def get_ideal_ranking(gt_types: Set[str]) -> List[str]:\n",
    "    \"\"\"Generates an ideal ranking corresponding to a set of ground truth types.\n",
    "    \n",
    "    Args:\n",
    "        gt_types: Set of ground truth types.\n",
    "    \n",
    "    Returns:\n",
    "        A ranked list of types that constitute an ideal ranking gain-wise.\n",
    "    \"\"\"\n",
    "    gains = {}\n",
    "    \n",
    "    for gt_type in gt_types:\n",
    "        # Ground truth type has max gain.\n",
    "        gains[gt_type] = type_taxonomy.max_depth()\n",
    "        \n",
    "        # Traverse upwards to add parent types.\n",
    "        parent_type = type_taxonomy.parent(gt_type)\n",
    "        gain = type_taxonomy.max_depth() - 1\n",
    "        while not type_taxonomy.is_root(parent_type):\n",
    "            gains[parent_type] = max(gains.get(parent_type, 0), gain)\n",
    "            gain -= 1\n",
    "            parent_type = type_taxonomy.parent(parent_type)\n",
    "\n",
    "        # Traverse downwards to add children types.\n",
    "        children_types = type_taxonomy.children(gt_type)\n",
    "        gain = type_taxonomy.max_depth() - 1\n",
    "        while len(children_types) > 0:\n",
    "            grandchildren_types = set()\n",
    "            for t in children_types:\n",
    "                gains[t] = max(gains.get(t, 0), gain)\n",
    "                grandchildren_types.update(type_taxonomy.children(t))\n",
    "            gain -= 1\n",
    "            children_types = grandchildren_types\n",
    "    \n",
    "    # Return types ordered by gain desc.\n",
    "    return [k for k, v in sorted(gains.items(), key=operator.itemgetter(1), reverse=True)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f3a03",
   "metadata": {
    "id": "bfNt9TMgn03U"
   },
   "outputs": [],
   "source": [
    "def dcg(relevances: List[float], k: int) -> float:\n",
    "    \"\"\"Computes DCG@k, given the corresponding relevance levels for a ranked list of types.\n",
    "    \n",
    "    Args:\n",
    "        relevances: List with the relevance levels corresponding to a ranked list of types.\n",
    "        k: Rank cut-off.\n",
    "        \n",
    "    Returns:\n",
    "        DCG@k (float).\n",
    "    \"\"\"\n",
    "    return relevances[0] + sum(\n",
    "        [relevances[i] / math.log(i + 1, 2) \n",
    "         for i in range(1, min(k, len(relevances)))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25aa66",
   "metadata": {
    "id": "CyXsK-i1n03U"
   },
   "outputs": [],
   "source": [
    "def ndcg(system_ranking: List[str], gt_types: Set[str], gain_function: Callable, k:int = 10) -> float:\n",
    "    \"\"\"Computes NDCG@k for a given system ranking.\n",
    "    \n",
    "    Args:\n",
    "        system_ranking: Ranked list of answer type IDs (from most to least relevant).\n",
    "        gt_types: Set of ground truth types.\n",
    "        gain_function: Function for computing the gain of an answer type.\n",
    "        k: Rank cut-off.\n",
    "    \n",
    "    Returns:\n",
    "        NDCG@k (float).\n",
    "    \"\"\"\n",
    "    # Relevance (gain) levels for the ranked docs.\n",
    "    relevances = [gain_function(gt_types, type_id) for type_id in system_ranking]\n",
    "\n",
    "    # Relevance levels (gains) of the idealized ranking.\n",
    "    relevances_ideal = [gain_function(gt_types, type_id) \n",
    "                        for type_id in get_ideal_ranking(gt_types)]\n",
    "    \n",
    "    return dcg(relevances, k) / dcg(relevances_ideal, k)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684cf09d",
   "metadata": {
    "id": "NyXfZYefn03U"
   },
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426a038",
   "metadata": {
    "id": "eKQ15O27n03U"
   },
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_ideal_ranking_single_gt():\n",
    "    ideal_rankings = get_ideal_ranking({\"dbo:Person\"})\n",
    "    # Ideal ranking starts with ground truth type.\n",
    "    assert \"dbo:Person\" in ideal_rankings \n",
    "    assert ideal_rankings.index(\"dbo:Person\") == 0\n",
    "    # Types that are not parent or children types are not present.\n",
    "    assert \"dbo:Organisation\" not in ideal_rankings\n",
    "    # Parent types are present and ranked lower than GT type.\n",
    "    assert \"dbo:Agent\" in ideal_rankings \n",
    "    assert ideal_rankings.index(\"dbo:Agent\") > 0\n",
    "    # Children types are present and ranked lower than GT type.    \n",
    "    assert \"dbo:Politician\" in ideal_rankings \n",
    "    assert ideal_rankings.index(\"dbo:Politician\") > 0\n",
    "    assert \"dbo:President\" in ideal_rankings\n",
    "    # Relative ranking of children types is correct.\n",
    "    assert ideal_rankings.index(\"dbo:President\") > ideal_rankings.index(\"dbo:Politician\")    \n",
    "    # Relative ranking of parent vs. children types is correct.\n",
    "    assert ideal_rankings.index(\"dbo:Agent\") < ideal_rankings.index(\"dbo:President\")    \n",
    "\n",
    "def test_ideal_ranking_multi_gt():\n",
    "    # Ground truth types are subtypes of person but on different branches\n",
    "    # and at different depths:\n",
    "    # Skier -> WinterSportPlayer -> Athlete -> Person\n",
    "    # DisneyCharacter -> FictionalCharacter -> Person\n",
    "    ideal_rankings = get_ideal_ranking({\"dbo:Skier\", \"dbo:DisneyCharacter\"})\n",
    "    # Ground truth type at the top of the ideal ranking in any order\n",
    "    assert set(ideal_rankings[:2]) == {\"dbo:Skier\", \"dbo:DisneyCharacter\"}\n",
    "    # Path tho Skier.\n",
    "    assert \"dbo:WinterSportPlayer\" in ideal_rankings \n",
    "    assert \"dbo:Athlete\" in ideal_rankings \n",
    "    assert \"dbo:Person\" in ideal_rankings \n",
    "    assert \"dbo:Agent\" in ideal_rankings \n",
    "    assert ideal_rankings.index(\"dbo:WinterSportPlayer\") < ideal_rankings.index(\"dbo:Athlete\")\n",
    "    assert ideal_rankings.index(\"dbo:Person\") < ideal_rankings.index(\"dbo:Agent\")\n",
    "    # Path to DisneyCharacter.\n",
    "    assert \"dbo:FictionalCharacter\" in ideal_rankings \n",
    "    assert ideal_rankings.index(\"dbo:FictionalCharacter\") < ideal_rankings.index(\"dbo:Person\")\n",
    "    assert ideal_rankings.index(\"dbo:FictionalCharacter\") < ideal_rankings.index(\"dbo:Athlete\")    \n",
    "    # Sibling types to ground truth types.\n",
    "    assert \"dbo:Ski_jumper\" not in ideal_rankings\n",
    "    assert \"dbo:SoapCharacter\" not in ideal_rankings\n",
    "    # Types on other branches.\n",
    "    assert \"dbo:Gymnast\" not in ideal_rankings\n",
    "    assert \"dbo:ShoppingMall\" not in ideal_rankings\n",
    "\n",
    "\n",
    "def test_ndcg():\n",
    "    # Perfect ranking.\n",
    "    assert ndcg([\"dbo:Person\", \"dbo:Agent\", \"dbo:Politician\"], \n",
    "                {\"dbo:Person\"}, gain_linear, k=3) == 1.0\n",
    "    assert ndcg([\"dbo:Organisation\", \"dbo:Agent\", \"dbo:Politician\"], \n",
    "                {\"dbo:Person\"}, gain_linear, k=3) == pytest.approx(0.583, rel=1e-3)\n",
    "    assert ndcg([\"dbo:Athlete\", \"dbo:Agent\", \"dbo:FictionalCharacter\"], \n",
    "                {\"dbo:Skier\", \"dbo:DisneyCharacter\"}, \n",
    "                gain_linear, k=3) == pytest.approx(0.719, rel=1e-3)\n",
    "    assert ndcg([\"dbo:Athlete\", \"dbo:Agent\", \"dbo:DisneyCharacter\"], \n",
    "                {\"dbo:Skier\", \"dbo:DisneyCharacter\"}, \n",
    "                gain_linear, k=3) == pytest.approx(0.754, rel=1e-3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06293eea",
   "metadata": {
    "id": "8erErqJlrN8V"
   },
   "source": [
    "# Exploring Word2Vec with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6badbc47",
   "metadata": {
    "id": "P9FEOaWsrN8X"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f6fb8",
   "metadata": {
    "id": "km-s_ja2rN8X"
   },
   "source": [
    "Word2Vec is an approach to learning *word embeddings*, vector representations of words that capture semantic and syntactic relationships between words based on their co-occurrences in natural language text. \n",
    "\n",
    "This unsupervised learning approach also reduces the dimensionality of the vectors representing words, which can be helpful for memory and to manage the *curse of dimensionality*, whereby high-dimensional vector spaces lead to a relative data sparsity, e.g., for machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f5efa",
   "metadata": {
    "id": "uDcuhEiBrN8X"
   },
   "source": [
    "In this exercise you will look at the capabilities of Word2Vec as implemented in the module Gensim. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf172883",
   "metadata": {
    "id": "pW5GlR2PrN8Y"
   },
   "source": [
    "## Requirements "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979d46c",
   "metadata": {
    "id": "JHwayhworN8Y"
   },
   "source": [
    "Uncomment the lines below, run the installations once as needed, then comment the code out again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b888855",
   "metadata": {
    "id": "iE3Ev6HLr9kL"
   },
   "outputs": [],
   "source": [
    "pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f52e5",
   "metadata": {
    "id": "f7Pgt9EorN8Y"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade Cython\n",
    "# !pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a1b53",
   "metadata": {
    "id": "-x1hXqJArN8Z"
   },
   "source": [
    "Import all necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aafba56",
   "metadata": {
    "id": "2AnrOe_QrN8Z"
   },
   "outputs": [],
   "source": [
    "# Import modules and set up logging.\n",
    "from typing import List, Generator\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17236828",
   "metadata": {
    "id": "P60Uvf8yrN8Z"
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f6881",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7i26KlerN8a",
    "outputId": "46ef7716-a91f-4a93-f8f6-0068faa6e14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_records': 1701, 'record_format': 'list of str (tokens)', 'file_size': 33182058, 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py', 'license': 'not found', 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.', 'checksum': '68799af40b6bda07dfa47a32612e5364', 'file_name': 'text8.gz', 'read_more': ['http://mattmahoney.net/dc/textdata.html'], 'parts': 1}\n",
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Load the Text8 corpus.\n",
    "print(api.info('text8'))\n",
    "text8_corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2817ecf",
   "metadata": {
    "id": "WHWVODH5rN8a"
   },
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d2013",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWbcFsrUrN8a",
    "outputId": "540fe028-06de-4bb5-e9d2-3529c171a364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68252116\n"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model on the Text8 corpus with default hyperparameters. \n",
    "model = Word2Vec(text8_corpus)  \n",
    "\n",
    "# Perform a sanity check on the trained model.\n",
    "print(model.wv.similarity('tree', 'leaf')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e697036",
   "metadata": {
    "id": "5RS3ddp3rN8a"
   },
   "outputs": [],
   "source": [
    "# Reduce logging level.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7a73a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHDVIKlFrN8a",
    "outputId": "70cd12a3-6ede-4894-b4fa-ff9c96a46c29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('leaf', 0.6825212240219116), ('trees', 0.6816262006759644), ('bark', 0.6624914407730103), ('avl', 0.6079550981521606), ('fruit', 0.6029431819915771), ('flower', 0.6008095741271973), ('cactus', 0.6005957126617432), ('grass', 0.5823603868484497), ('bird', 0.5789984464645386), ('pond', 0.5772603750228882)]\n",
      "[('bark', 0.7890558838844299), ('pigment', 0.7656276822090149), ('colored', 0.7648026347160339), ('reddish', 0.7575005888938904), ('coloured', 0.7569463849067688), ('grass', 0.7538576126098633), ('yellowish', 0.752110481262207), ('beetle', 0.7462701797485352), ('aloe', 0.7384286522865295), ('fleshy', 0.73822021484375)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('tree')) \n",
    "print(model.wv.most_similar('leaf')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61552472",
   "metadata": {
    "id": "4JfXVHjPrN8a"
   },
   "source": [
    "## Relationships\n",
    "\n",
    "Investigate the relationships between words in terms of trained representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3c499",
   "metadata": {
    "id": "fvVb4RZ6rN8a"
   },
   "source": [
    "### Evaluate  analogies\n",
    "With the model you have trained, evaluate the analogy\n",
    "`king-man+woman =~ queen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c7d7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMq0rNp6rN8b",
    "outputId": "69e69bca-9c44-400c-dfb8-953590b4a4d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6803272366523743), ('empress', 0.6377588510513306), ('daughter', 0.6349191665649414), ('emperor', 0.6257573366165161), ('prince', 0.6172590255737305)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be604e",
   "metadata": {
    "id": "jNPZWXZArN8b"
   },
   "source": [
    "Evaluate the analogy `ship-boat+rocket =~ spacecraft`. How similar are the left-hand side of the analogy to the right-hand side? Implement a function that can find the answer for analogies in general. We assume the right-hand side of the analogy will always be a single, positive term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1805f88",
   "metadata": {
    "id": "4Zv5s2AjrN8b"
   },
   "outputs": [],
   "source": [
    "def eval_analogy(model: Word2Vec, lhs_pos: List[str], lhs_neg: List[str], rhs: str)->float:\n",
    "    \"\"\"Returns the similarity between the left-hand and right-hand sides of an anaology.\n",
    "    \n",
    "        Arguments: \n",
    "            model: Trained Gensim word2vec model to use.\n",
    "            lhs_pos: List of terms that are positive on the left-hand side in the analogy. \n",
    "            lhs_neg: List of terms that are negative on the left-hand side in the analogy. \n",
    "            rhs: A single positive term on the right-hand side in the analogy.\n",
    "            \n",
    "        Returns:\n",
    "            Float of the similarity if right-hand side term is found in the top 500 most similar terms.\n",
    "            Otherwise, return None.\"\"\"\n",
    "    # How similar are the left-hand side of the analogy to the right-hand side? \n",
    "    # Implement a function that can find the answer for analogies in general.\n",
    "    # TODO: Complete.\n",
    "    similarities_list = model.most_similar(positive=lhs_pos, negative=lhs_neg, topn=500)\n",
    "    similarities_dict = {}\n",
    "    for term, sim in similarities_list:\n",
    "        similarities_dict[term] = sim\n",
    "    if rhs in similarities_dict:\n",
    "        return similarities_dict[rhs]\n",
    "    else:\n",
    "        print(\"Right-hand side term not found in top 500 most similar terms to the left-hand side analogy.\")\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb0007",
   "metadata": {
    "id": "y47NEZeJrN8b"
   },
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca78b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxbmLvSjrN8c",
    "outputId": "85623ee1-952c-4f1c-a19c-1fa2c4b48a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_eval_analogy():\n",
    "    assert eval_analogy(model.wv, ['ship', 'rocket'], ['boat'], 'spacecraft') == pytest.approx(0.7, abs=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a1867",
   "metadata": {
    "id": "y3T6E2a8rN8c"
   },
   "source": [
    "## Load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5318a10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pknof_arN8c",
    "outputId": "22f8fa38-9fee-446b-cb64-1cc107ed3e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model_loaded = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a2fc8",
   "metadata": {
    "id": "zQFYufuxrN8c"
   },
   "outputs": [],
   "source": [
    "loaded_analogy_eval = -1\n",
    "# Evaluate the analogy 'king'-'man'+'woman' compared to 'queen' using the loaded model \n",
    "# and assign the value to the variable `loaded_analogy_eval`.\n",
    "# TODO: Complete.\n",
    "loaded_analogy_eval = eval_analogy(model_loaded, ['king', 'woman'], ['man'], 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640d0f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1bW9LVSrN8c",
    "outputId": "b6b7d5b6-bd0b-4f10-a44e-c46be9c10e4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%%run_pytest[clean] and %%run_pytest are deprecated in favor of %%ipytest. %%ipytest will clean tests, evaluate the cell and then run pytest. To disable cleaning, configure ipytest with ipytest.config(clean=False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_loaded_analogy_eval():\n",
    "    assert loaded_analogy_eval != -1\n",
    "    assert loaded_analogy_eval == pytest.approx(0.7, abs=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e895fc",
   "metadata": {
    "id": "SrPRdsZdrN8c"
   },
   "source": [
    "## Train Word2Vec on different corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64e70f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hC7wUGL6rN8c",
    "outputId": "80cab086-e335-4811-816e-13b4852798de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-27 18:42:21--  https://raw.githubusercontent.com/gsurma/text_predictor/master/data/kanye/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 330453 (323K) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>] 322.71K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2022-09-27 18:42:22 (9.42 MB/s) - ‘input.txt’ saved [330453/330453]\n",
      "\n",
      "--2022-09-27 18:42:22--  https://raw.githubusercontent.com/gsurma/text_predictor/master/data/shakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4573338 (4.4M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   4.36M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2022-09-27 18:42:23 (58.1 MB/s) - ‘input.txt’ saved [4573338/4573338]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the rap lyrics of Kanye West.\n",
    "! wget https://raw.githubusercontent.com/gsurma/text_predictor/master/data/kanye/input.txt\n",
    "! mv input.txt kanye.txt\n",
    "\n",
    "# Download the complete works of William Shakespeare.\n",
    "! wget https://raw.githubusercontent.com/gsurma/text_predictor/master/data/shakespeare/input.txt\n",
    "! mv input.txt shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511a990",
   "metadata": {
    "id": "_hUkv9ZirN8d"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, data: str) -> None:\n",
    "        self.data = data\n",
    "\n",
    "    def __iter__(self) -> Generator[List[str], None, None]:\n",
    "        corpus_path = datapath(self.data)\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb44e8",
   "metadata": {
    "id": "sOK7dp0vrN8d"
   },
   "source": [
    "Separately train two new models using the two different datasets, and compare how these datasets affect relationships between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a342b5",
   "metadata": {
    "id": "syhlDRCXrN8d"
   },
   "outputs": [],
   "source": [
    "kanye_data = MyCorpus(os.getcwd()+'/kanye.txt')\n",
    "shakespeare_data = MyCorpus(os.getcwd()+'/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2b37f",
   "metadata": {
    "id": "SSZUSqyHrN8d"
   },
   "outputs": [],
   "source": [
    "kanye_model = None\n",
    "# Train a Word2Vec model on the Kanye corpus, and name it `kanye_model`.\n",
    "# TODO: Complete\n",
    "kanye_model = Word2Vec(sentences=kanye_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f675b",
   "metadata": {
    "id": "xN4oiKairN8d"
   },
   "outputs": [],
   "source": [
    "shakespeare_model = None\n",
    "# Train a Word2Vec model on the Shakespeare corpus, and name it `shakespeare_model`.\n",
    "# TODO: Complete\n",
    "shakespeare_model = Word2Vec(sentences=shakespeare_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2301c9",
   "metadata": {
    "id": "kvvBEVm1rN8d"
   },
   "source": [
    "For each of the models, we can easily find words where the two models learn very different similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd808010",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yaa2hWkrN8d",
    "outputId": "acdaa12f-6ac9-4168-ac33-1edac0aa7959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('big', 0.9997344017028809), ('his', 0.9997167587280273), ('by', 0.9996941089630127), ('might', 0.9996739625930786), ('ooh', 0.9996687769889832)]\n",
      "[('prince', 0.8771281838417053), ('duke', 0.7768160104751587), ('fifth', 0.6678986549377441), ('gaunt', 0.664115846157074), ('queen', 0.6587375402450562)]\n"
     ]
    }
   ],
   "source": [
    "# For example, compare:\n",
    "print(kanye_model.wv.most_similar(positive=['king'], topn=5))\n",
    "print(shakespeare_model.wv.most_similar(positive=['king'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b23baf",
   "metadata": {
    "id": "ixw2F4sDrN8d"
   },
   "source": [
    "## Compare Skip-gram and CBOW\n",
    "\n",
    "By using the arguments of the model (training) method in `gensim.models.Word2Vec()` you can select either Skip-gram or CBOW explicitly, as well as modifying other hyperparameters. \n",
    "\n",
    "Train a Skip-gram model on the Text8 corpus and compare with the default CBOW model on the same dataset, with the same context window size, and compare how relationships are expressed in terms of the resulting embedding vectors.\n",
    "\n",
    "**Hint:** Use the keyword argument `sg` in when instantiating the model object to specify Skip-gram, rather than the defaul CBOW setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42e1fc",
   "metadata": {
    "id": "Udul-W-xrN8d"
   },
   "outputs": [],
   "source": [
    "model_sg = None\n",
    "# Train a skip-gram Word2Vec model on `text8_corpus` and name it `model_sg``\n",
    "# TODO: Complete\n",
    "model_sg = Word2Vec(text8_corpus, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84426b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J5nLuj7QrN8d",
    "outputId": "10ded2a8-fe17-4fbd-8f15-7bb9ec41ba63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "loaded_analogy_eval_sg = eval_analogy(model_sg, ['king', 'woman'], ['man'], 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d876d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7bmyAoyrN8e",
    "outputId": "7051dc39-0abe-434c-ed10-398db4c35089"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "loaded_analogy_eval_cbow = eval_analogy(model, ['king', 'woman'], ['man'], 'queen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142849b",
   "metadata": {
    "id": "LqS5gCcmrN8e"
   },
   "source": [
    "**Discuss:** Which of the models produces the highest similarity for the example analogy? Will this always be the case? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b959086",
   "metadata": {
    "id": "1C8JgtObrN8e"
   },
   "source": [
    "For more information about Gensim, see https://radimrehurek.com/gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9a705",
   "metadata": {
    "id": "QRkBMRtgUY4-"
   },
   "source": [
    "# Dense retrieval with ANCE\n",
    "\n",
    "We will use a [PyTerrier plugin for ANCE](https://github.com/terrierteam/pyterrier_ance) for dense passage retrieval. \n",
    "\n",
    "[ANCE](https://github.com/microsoft/ANCE) is a dense retrieval system leveraging single representations to encode documents and queries. ANCE does not require combination with sparse retrieval. ANCE leverages a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances than the negative training instances selected by a sparse retrieval mechanism.\n",
    "\n",
    "The experiments are run on [CORD19 corpus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7251955/) and [TREC Covid test collection](https://ir.nist.gov/covidSubmit/).\n",
    "\n",
    "This is exercise is based on the [example](https://colab.research.google.com/github/terrierteam/pyterrier_ance/blob/master/pyterrier_ance_vaswani.ipynb) provided by the PyTerrier team and [CIKM 2021 Tutorial Notebook](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=451eb743b6a9202f20fde3ac85dbe6ad00103506&device=unknown&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f746572726965722d6f72672f63696b6d323032317475746f7269616c2f343531656237343362366139323032663230666465336163383564626536616430303130333530362f6e6f7465626f6f6b732f6e6f7465626f6f6b345f322e6970796e62&logged_in=false&nwo=terrier-org%2Fcikm2021tutorial&path=notebooks%2Fnotebook4_2.ipynb&platform=android&repository_id=416829271&repository_type=Repository&version=96)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ed77c",
   "metadata": {
    "id": "w25oC9DIUSKw"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q python-terrier\n",
    "!apt install -q --upgrade libomp-dev\n",
    "!pip install -q --upgrade faiss-gpu==1.6.3\n",
    "!pip install -q git+https://github.com/terrierteam/pyterrier_ance.git\n",
    "!pip install -q ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b6872",
   "metadata": {
    "id": "Z-C61wUqUNsF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "  pt.init(tqdm='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324eaf69",
   "metadata": {
    "id": "PpTOtAqFVZRm"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Collecting the topics and qrels of the TREC-COVID19 dataset\n",
    "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "topics = cord19.get_topics(variant='title')\n",
    "qrels = cord19.get_qrels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369d9d5",
   "metadata": {
    "id": "YXFWrHywY2w4"
   },
   "source": [
    "### BM25 inverted index\n",
    "\n",
    "We use a pre-built Terrier inverted index for the TREC-COVID19 collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b143d7a",
   "metadata": {
    "id": "XHwrvZk4YlqO"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "bm25_index = pt.datasets.get_dataset('trec-covid').get_index('terrier_stemmed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812b02a",
   "metadata": {
    "id": "khUX-VzzY6HI"
   },
   "source": [
    "### ANCE dense index\n",
    "\n",
    "\n",
    "We download a pre-built ANCE FAISS index for the TREC-COVID19 collection. The indexing procedure generates a number of FAISS shards, together with some additional files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd6fd7",
   "metadata": {
    "id": "Nlf5veaaY5rg"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "ance_index = pt.datasets.get_dataset('trec-covid').get_index('ance_msmarco_psg')\n",
    "!ls /content/anceindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da383e",
   "metadata": {
    "id": "CiNpY6Z3ZaeG"
   },
   "source": [
    "### Retrieval\n",
    "\n",
    "We create BM25 baseline transformer, and the ANCE retrieve transformer. Since most documents exceed the maximum length supported by ANCE, a sliding window of 150 tokens is used (stride 75, prepending title) to construct passages. As such, passage scores need to be aggregated, e.g., using pt.text.max_passage().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b69cf",
   "metadata": {
    "id": "EiQQcxaHZl23"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pyterrier_ance import ANCERetrieval\n",
    "\n",
    "bm25_retriever = pt.BatchRetrieve(bm25_index, wmodel=\"BM25\")\n",
    "ance_retriever = ANCERetrieval.from_dataset('trec-covid', 'ance_msmarco_psg') >> pt.text.max_passage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e729a6e",
   "metadata": {
    "id": "M2LaXXndZuPM"
   },
   "source": [
    "We retrieve the top 50 ranked documents for the official topics, and compute several effectiveness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780391fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "c758e53a1bb943d6b3133513fc78a483",
      "7de81030f7af475d887ff76088ba04bc",
      "2257887cf2404ccd9bb213262a2740f1",
      "6c57c1028de048d4bc906c7ee930d63b",
      "55fcad639dd44549a6a1cf1df236286b",
      "3f206ce9dea44afe89bae9b9b2ffa7eb",
      "6b8b0bc75bfd4dbea224357d80e8568a",
      "ed7b9c09dabc4c8f9e91cb5addfdc3da",
      "bdfcb7926b774a0cbbbda543e370de0a",
      "54f30e96ac1f409a8607baa01b967456",
      "c62f956ebb28466d90ce63c24c1f36a6"
     ]
    },
    "id": "j51Zj5aaVlHk",
    "outputId": "b42f9265-98b9-41f4-9818-c836751817b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** inference of 50 queries *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in distributed mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 1it [00:09, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** faiss search for 50 queries on 1 shards *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c758e53a1bb943d6b3133513fc78a483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?shard/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d88b2f31-8ba2-436c-b67a-d660056bc2fb\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>recip_rank</th>\n",
       "      <th>P_10</th>\n",
       "      <th>ndcg_cut_3</th>\n",
       "      <th>recall_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.048270</td>\n",
       "      <td>0.807094</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.631732</td>\n",
       "      <td>0.062774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANCE</td>\n",
       "      <td>0.024658</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.479889</td>\n",
       "      <td>0.037527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d88b2f31-8ba2-436c-b67a-d660056bc2fb')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d88b2f31-8ba2-436c-b67a-d660056bc2fb button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d88b2f31-8ba2-436c-b67a-d660056bc2fb');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   name       map  recip_rank   P_10  ndcg_cut_3  recall_100\n",
       "0  BM25  0.048270    0.807094  0.678    0.631732    0.062774\n",
       "1  ANCE  0.024658    0.656000  0.452    0.479889    0.037527"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [bm25_retriever % 50, ance_retriever % 50], \n",
    "    topics,\n",
    "    qrels,\n",
    "    eval_metrics=[\"map\", \"recip_rank\", \"P_10\", \"ndcg_cut_3\", \"recall_100\"],\n",
    "    names=['BM25', 'ANCE'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242cf80",
   "metadata": {
    "id": "dszak0L_di-0"
   },
   "source": [
    "The underperforming results computed our ANCE retriever are due to the lack of fine-tuning of the underlying BERT-based model with COVID19 and medical-related documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1a04f",
   "metadata": {
    "id": "E2a_B6f-V7n0"
   },
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ac16b",
   "metadata": {
    "id": "eF_Byg0eWf4v"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "def show_res_with_text_labels(retriever, qid):\n",
    "    \"\"\"Displays the texts of the retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        retriever: The retriever to be used to retreive documents.\n",
    "        qid: Query ID.\n",
    "        \n",
    "    Returns:\n",
    "        Retrieved documents with the text.    \n",
    "    \"\"\"\n",
    "    def make_doi_url(df):\n",
    "      df[\"doi\"] = df[\"doi\"].apply(lambda doi: \"https://doi.org/\" + doi)\n",
    "      return df\n",
    "    pipe = (retriever % 10) >> pt.text.get_text(cord19, [\"title\", \"doi\"]) >> pt.apply.generic(make_doi_url)\n",
    "    res = pipe.transform(topics[topics.qid == qid])\n",
    "    res = res.merge(qrels, how='left')\n",
    "    def make_clickable(val):\n",
    "        return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val, val)\n",
    "    res = res.sort_values(\"rank\", ascending=True)\n",
    "    res.style.format({'doi': make_clickable})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5755db0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "B9JFfTKgWE5i",
    "outputId": "3c417391-b707-4f67-efe3-3484f1da0db6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c51a2806-bb8b-454b-b589-c942e4b2f5a8\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>label</th>\n",
       "      <th>iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>122553</td>\n",
       "      <td>75773gwg</td>\n",
       "      <td>0</td>\n",
       "      <td>11.558188</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>Zoonotic origins of human coronavirus 2019 (HC...</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>122554</td>\n",
       "      <td>kn2z7lho</td>\n",
       "      <td>1</td>\n",
       "      <td>11.558188</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>Zoonotic origins of human coronavirus 2019 (HC...</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>122555</td>\n",
       "      <td>4fb291hq</td>\n",
       "      <td>2</td>\n",
       "      <td>11.558188</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>Zoonotic origins of human coronavirus 2019 (HC...</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>135022</td>\n",
       "      <td>ne5r4d4b</td>\n",
       "      <td>3</td>\n",
       "      <td>11.558188</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>Origin and evolution of pathogenic coronaviruses</td>\n",
       "      <td>https://doi.org/10.1038/s41579-018-0118-9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>186652</td>\n",
       "      <td>hl967ekh</td>\n",
       "      <td>4</td>\n",
       "      <td>11.558188</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>Zoonotic origins of human coronavirus 2019 (HC...</td>\n",
       "      <td>https://doi.org/10.24272/j.issn.2095-8137.2020...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>120776</td>\n",
       "      <td>kqqantwg</td>\n",
       "      <td>5</td>\n",
       "      <td>11.417061</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>Possible Bat Origin of Severe Acute Respirator...</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>158983</td>\n",
       "      <td>12dcftwt</td>\n",
       "      <td>6</td>\n",
       "      <td>11.417061</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>Possible Bat Origin of Severe Acute Respirator...</td>\n",
       "      <td>https://doi.org/10.3201/eid2607.200092</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>81979</td>\n",
       "      <td>8ccl9aui</td>\n",
       "      <td>7</td>\n",
       "      <td>11.324364</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>Mosaic evolution of the severe acute respirato...</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>68472</td>\n",
       "      <td>4dtk1kyh</td>\n",
       "      <td>8</td>\n",
       "      <td>11.255136</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>Origin of Novel Coronavirus (COVID-19): A Comp...</td>\n",
       "      <td>https://doi.org/10.1101/2020.05.12.091397</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>104060</td>\n",
       "      <td>pl48ev5o</td>\n",
       "      <td>9</td>\n",
       "      <td>11.230726</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>Origin and evolution of the 2019 novel coronav...</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c51a2806-bb8b-454b-b589-c942e4b2f5a8')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c51a2806-bb8b-454b-b589-c942e4b2f5a8 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c51a2806-bb8b-454b-b589-c942e4b2f5a8');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  qid   docid     docno  rank      score               query  \\\n",
       "0   1  122553  75773gwg     0  11.558188  coronavirus origin   \n",
       "1   1  122554  kn2z7lho     1  11.558188  coronavirus origin   \n",
       "2   1  122555  4fb291hq     2  11.558188  coronavirus origin   \n",
       "3   1  135022  ne5r4d4b     3  11.558188  coronavirus origin   \n",
       "4   1  186652  hl967ekh     4  11.558188  coronavirus origin   \n",
       "5   1  120776  kqqantwg     5  11.417061  coronavirus origin   \n",
       "6   1  158983  12dcftwt     6  11.417061  coronavirus origin   \n",
       "7   1   81979  8ccl9aui     7  11.324364  coronavirus origin   \n",
       "8   1   68472  4dtk1kyh     8  11.255136  coronavirus origin   \n",
       "9   1  104060  pl48ev5o     9  11.230726  coronavirus origin   \n",
       "\n",
       "                                               title  \\\n",
       "0  Zoonotic origins of human coronavirus 2019 (HC...   \n",
       "1  Zoonotic origins of human coronavirus 2019 (HC...   \n",
       "2  Zoonotic origins of human coronavirus 2019 (HC...   \n",
       "3   Origin and evolution of pathogenic coronaviruses   \n",
       "4  Zoonotic origins of human coronavirus 2019 (HC...   \n",
       "5  Possible Bat Origin of Severe Acute Respirator...   \n",
       "6  Possible Bat Origin of Severe Acute Respirator...   \n",
       "7  Mosaic evolution of the severe acute respirato...   \n",
       "8  Origin of Novel Coronavirus (COVID-19): A Comp...   \n",
       "9  Origin and evolution of the 2019 novel coronav...   \n",
       "\n",
       "                                                 doi  label iteration  \n",
       "0                                   https://doi.org/      2         5  \n",
       "1                                   https://doi.org/      2         3  \n",
       "2                                   https://doi.org/      1         3  \n",
       "3          https://doi.org/10.1038/s41579-018-0118-9      0       1.5  \n",
       "4  https://doi.org/10.24272/j.issn.2095-8137.2020...      2         3  \n",
       "5                                   https://doi.org/      2         5  \n",
       "6             https://doi.org/10.3201/eid2607.200092      2         5  \n",
       "7                                   https://doi.org/      2         1  \n",
       "8          https://doi.org/10.1101/2020.05.12.091397      2         3  \n",
       "9                                   https://doi.org/      1         4  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_res_with_text_labels(bm25_retriever, \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2abf0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464,
     "referenced_widgets": [
      "a15d78932bd04bd6af3f6cd77b5fcded",
      "40ae372f62ac40ba9287b18045be403d",
      "0ce7581cf4d94ab499858fe60597a26a",
      "47b21bb02ac34479ac0ac730b4b94c1e",
      "2bf52e116eb040bea3692879f2a51109",
      "df4cd25788df40f3b402cd4118d2dfd0",
      "b394584452544fcd811c179141733d31",
      "b9e57ab307344be0985fe08d2f000676",
      "3bf5ac8e90c44be281daca5772add9a7",
      "c98847bc0f584e1aaff7365baaf906a2",
      "4dfa151b7fe64df7a89ad9a6bd7e0fdf"
     ]
    },
    "id": "mbyG9m-eadjH",
    "outputId": "22b0fae9-80a3-4ad9-9b02-ab2a4bd47d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** inference of 1 queries *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in distributed mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 1it [00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** faiss search for 1 queries on 1 shards *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15d78932bd04bd6af3f6cd77b5fcded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?shard/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-040ad336-580b-4152-a73e-2a3eea6e4b7f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>score</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>label</th>\n",
       "      <th>iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>715.294006</td>\n",
       "      <td>j1cdoxqs</td>\n",
       "      <td>0</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>714.861938</td>\n",
       "      <td>be0mr85h</td>\n",
       "      <td>1</td>\n",
       "      <td>Coronavirus.</td>\n",
       "      <td>https://doi.org/10.1177/0025817220933546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>714.552856</td>\n",
       "      <td>9pla28n4</td>\n",
       "      <td>2</td>\n",
       "      <td>Coronaviruses: origin and evolution</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>714.552856</td>\n",
       "      <td>jkejiuf2</td>\n",
       "      <td>3</td>\n",
       "      <td>Coronaviruses: origin and evolution</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>714.378540</td>\n",
       "      <td>bp9xz9wk</td>\n",
       "      <td>4</td>\n",
       "      <td>Coronavirus?</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>714.107727</td>\n",
       "      <td>l0sbncnp</td>\n",
       "      <td>5</td>\n",
       "      <td>Exploration on mechanism of anti-coronavirus o...</td>\n",
       "      <td>https://doi.org/10.7501/j.issn.0253-2670.2020....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>714.046753</td>\n",
       "      <td>hmvo5b0q</td>\n",
       "      <td>6</td>\n",
       "      <td>Understanding Coronavirus</td>\n",
       "      <td>https://doi.org/</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>713.970276</td>\n",
       "      <td>8l411r1w</td>\n",
       "      <td>7</td>\n",
       "      <td>Discovery of a novel coronavirus, China Rattus...</td>\n",
       "      <td>https://doi.org/10.1128/jvi.02420-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>713.963501</td>\n",
       "      <td>utsr0zv7</td>\n",
       "      <td>8</td>\n",
       "      <td>The Human Coronavirus Disease COVID-19: Its Or...</td>\n",
       "      <td>https://doi.org/10.3390/pathogens9050331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>713.778870</td>\n",
       "      <td>7mfedn03</td>\n",
       "      <td>9</td>\n",
       "      <td>Coronavirus Infections</td>\n",
       "      <td>https://doi.org/10.1016/b978-1-4160-2406-4.500...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-040ad336-580b-4152-a73e-2a3eea6e4b7f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-040ad336-580b-4152-a73e-2a3eea6e4b7f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-040ad336-580b-4152-a73e-2a3eea6e4b7f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  qid               query       score     docno  rank  \\\n",
       "6   1  coronavirus origin  715.294006  j1cdoxqs     0   \n",
       "3   1  coronavirus origin  714.861938  be0mr85h     1   \n",
       "2   1  coronavirus origin  714.552856  9pla28n4     2   \n",
       "7   1  coronavirus origin  714.552856  jkejiuf2     3   \n",
       "4   1  coronavirus origin  714.378540  bp9xz9wk     4   \n",
       "8   1  coronavirus origin  714.107727  l0sbncnp     5   \n",
       "5   1  coronavirus origin  714.046753  hmvo5b0q     6   \n",
       "1   1  coronavirus origin  713.970276  8l411r1w     7   \n",
       "9   1  coronavirus origin  713.963501  utsr0zv7     8   \n",
       "0   1  coronavirus origin  713.778870  7mfedn03     9   \n",
       "\n",
       "                                               title  \\\n",
       "6                                        Coronavirus   \n",
       "3                                       Coronavirus.   \n",
       "2                Coronaviruses: origin and evolution   \n",
       "7                Coronaviruses: origin and evolution   \n",
       "4                                       Coronavirus?   \n",
       "8  Exploration on mechanism of anti-coronavirus o...   \n",
       "5                          Understanding Coronavirus   \n",
       "1  Discovery of a novel coronavirus, China Rattus...   \n",
       "9  The Human Coronavirus Disease COVID-19: Its Or...   \n",
       "0                             Coronavirus Infections   \n",
       "\n",
       "                                                 doi  label iteration  \n",
       "6                                   https://doi.org/    NaN       NaN  \n",
       "3           https://doi.org/10.1177/0025817220933546    NaN       NaN  \n",
       "2                                   https://doi.org/    NaN       NaN  \n",
       "7                                   https://doi.org/    2.0         3  \n",
       "4                                   https://doi.org/    NaN       NaN  \n",
       "8  https://doi.org/10.7501/j.issn.0253-2670.2020....    NaN       NaN  \n",
       "5                                   https://doi.org/    1.0         5  \n",
       "1               https://doi.org/10.1128/jvi.02420-14    0.0         1  \n",
       "9           https://doi.org/10.3390/pathogens9050331    1.0         3  \n",
       "0  https://doi.org/10.1016/b978-1-4160-2406-4.500...    NaN       NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_res_with_text_labels(ance_retriever, \"1\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
